{
  "rate_limits": [
    {
      "name": "babbage-002",
      "slug": "babbage-002",
      "current_snapshot": "babbage-002",
      "tagline": "Replacement for the GPT-3 ada and babbage base models",
      "description": "GPT base models can understand and generate natural language or code but are not trained with instruction following. These models are made to be replacements for our original GPT-3 base models and use the legacy Completions API. Most customers should use GPT-3.5 or GPT-4.\n",
      "type": "chat",
      "snapshots": [
        "babbage-002"
      ],
      "compare_prices": [
        "gpt-4o-mini",
        "gpt-4o"
      ],
      "point_to": "gpt-4o",
      "rate_limits": {
        "tier_1": {
          "rpm": 500,
          "rpd": 10000.0,
          "tpm": 10000.0,
          "batch_queue_limit": 100000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 40000.0,
          "batch_queue_limit": 200000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 80000.0,
          "batch_queue_limit": 5000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 300000.0,
          "batch_queue_limit": 30000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 1000000.0,
          "batch_queue_limit": 150000000.0
        }
      }
    },
    {
      "name": "chatgpt-4o-latest",
      "slug": "chatgpt-4o-latest",
      "display_name": "ChatGPT-4o",
      "current_snapshot": "chatgpt-4o-latest",
      "tagline": "GPT-4o model used in ChatGPT",
      "description": "ChatGPT-4o points to the GPT-4o snapshot currently used in ChatGPT. GPT-4o is our versatile, high-intelligence flagship model.\nIt accepts both text and image inputs, and produces text outputs.\nIt is the best model for most tasks, and is our most capable model outside of our o-series models.\n",
      "type": "chat",
      "snapshots": [
        "chatgpt-4o-latest"
      ],
      "compare_prices": [
        "gpt-4o",
        "gpt-4o-mini"
      ],
      "examples": [
        "math_tutor",
        "travel_assistant",
        "clothing_recommendation",
        "recipe_generation"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 500,
          "tpm": 30000.0,
          "batch_queue_limit": 90000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 450000.0,
          "batch_queue_limit": 1350000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 800000.0,
          "batch_queue_limit": 50000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 2000000.0,
          "batch_queue_limit": 200000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 30000000.0,
          "batch_queue_limit": 5000000000.0
        }
      }
    },
    {
      "name": "codex-mini-latest",
      "slug": "codex-mini-latest",
      "display_name": "codex-mini-latest",
      "current_snapshot": "codex-mini-latest",
      "tagline": "Fast reasoning model optimized for the Codex CLI",
      "description": "codex-mini-latest is a fine-tuned version of o4-mini specifically\nfor use in Codex CLI. For direct use in the API, we recommend starting \nwith gpt-4.1.\n",
      "type": "other",
      "snapshots": [
        "codex-mini-latest"
      ],
      "compare_prices": [
        "o4-mini",
        "gpt-4.1"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 1000.0,
          "tpm": 100000.0,
          "batch_queue_limit": 1000000.0
        },
        "tier_2": {
          "rpm": 2000.0,
          "tpm": 200000.0,
          "batch_queue_limit": 2000000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 4000000.0,
          "batch_queue_limit": 40000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 10000000.0,
          "batch_queue_limit": 1000000000.0
        },
        "tier_5": {
          "rpm": 30000.0,
          "tpm": 150000000.0,
          "batch_queue_limit": 15000000000.0
        }
      }
    },
    {
      "name": "computer-use-preview",
      "slug": "computer-use-preview",
      "current_snapshot": "computer-use-preview-2025-03-11",
      "tagline": "Specialized model for computer use tool",
      "description": "The computer-use-preview model is a specialized model for the computer use \ntool. It is trained to understand and execute computer tasks.\nSee the [computer use guide](/docs/guides/tools-computer-use) for more\ninformation. This model is only usable in the \n[Responses API](/docs/api-reference/responses).\n",
      "type": "other",
      "snapshots": [
        "computer-use-preview-2025-03-11"
      ],
      "compare_prices": [
        "o3-mini",
        "o1"
      ],
      "grouped_models": null,
      "rate_limits": {
        "tier_3": {
          "rpm": 3000.0,
          "tpm": 20000000.0,
          "batch_queue_limit": 450000000.0
        },
        "tier_4": {
          "rpm": 3000.0,
          "tpm": 20000000.0,
          "batch_queue_limit": 450000000.0
        },
        "tier_5": {
          "rpm": 3000.0,
          "tpm": 20000000.0,
          "batch_queue_limit": 450000000.0
        }
      }
    },
    {
      "name": "dall-e-2",
      "slug": "dall-e-2",
      "display_name": "DALL\u00b7E 2",
      "current_snapshot": "dall-e-2",
      "tagline": "Our first image generation model",
      "description": "DALL\u00b7E is an AI system that creates realistic images and art from a natural language description. Older than DALL\u00b7E 3, DALL\u00b7E 2 offers more control in prompting and more requests at once.\n",
      "type": "other",
      "snapshots": [
        "dall-e-2"
      ],
      "compare_prices": [
        "dall-e-3"
      ],
      "point_to": "dall-e-3",
      "rate_limits": {
        "tier_free": {
          "rpm": "5 img/min"
        },
        "tier_1": {
          "rpm": "500 img/min"
        },
        "tier_2": {
          "rpm": "2500 img/min"
        },
        "tier_3": {
          "rpm": "5000 img/min"
        },
        "tier_4": {
          "rpm": "7500 img/min"
        },
        "tier_5": {
          "rpm": "10000 img/min"
        }
      }
    },
    {
      "name": "dall-e-3",
      "slug": "dall-e-3",
      "display_name": "DALL\u00b7E 3",
      "current_snapshot": "dall-e-3",
      "tagline": "Previous generation image generation model",
      "description": "DALL\u00b7E is an AI system that creates realistic images and art from a natural language description. DALL\u00b7E 3 currently supports the ability, given a prompt, to create a new image with a specific size.\n",
      "type": "other",
      "snapshots": [
        "dall-e-3"
      ],
      "compare_prices": [
        "dall-e-2"
      ],
      "rate_limits": {
        "tier_free": {
          "rpm": "1 img/min"
        },
        "tier_1": {
          "rpm": "500 img/min"
        },
        "tier_2": {
          "rpm": "2500 img/min"
        },
        "tier_3": {
          "rpm": "5000 img/min"
        },
        "tier_4": {
          "rpm": "7500 img/min"
        },
        "tier_5": {
          "rpm": "10000 img/min"
        }
      }
    },
    {
      "name": "davinci-002",
      "slug": "davinci-002",
      "current_snapshot": "davinci-002",
      "tagline": "Replacement for the GPT-3 curie and davinci base models",
      "description": "GPT base models can understand and generate natural language or code but are not trained with instruction following. These models are made to be replacements for our original GPT-3 base models and use the legacy Completions API. Most customers should use GPT-3.5 or GPT-4.\n",
      "type": "chat",
      "snapshots": [
        "davinci-002"
      ],
      "compare_prices": [
        "gpt-4o-mini",
        "gpt-4o"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 500,
          "rpd": 10000.0,
          "tpm": 10000.0,
          "batch_queue_limit": 100000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 40000.0,
          "batch_queue_limit": 200000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 80000.0,
          "batch_queue_limit": 5000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 300000.0,
          "batch_queue_limit": 30000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 1000000.0,
          "batch_queue_limit": 150000000.0
        }
      }
    },
    {
      "name": "gpt-3.5-turbo-16k-0613",
      "slug": "gpt-3-5-turbo-16k-0613",
      "current_snapshot": "gpt-3.5-turbo-16k-0613",
      "tagline": "Legacy GPT model for cheaper chat and non-chat tasks",
      "description": "GPT-3.5 Turbo models can understand and generate natural language or code and have been optimized for chat using the Chat Completions API but work well for non-chat tasks as well. As of July 2024, use gpt-4o-mini in place of GPT-3.5 Turbo, as it is cheaper, more capable, multimodal, and just as fast. GPT-3.5 Turbo is still available for use in the API.\n",
      "type": "chat",
      "snapshots": [
        "gpt-3.5-turbo-16k-0613"
      ],
      "compare_prices": [
        "gpt-4o-mini",
        "o3-mini"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 3500,
          "rpd": 10000.0,
          "tpm": 200000.0,
          "batch_queue_limit": 2000000.0
        },
        "tier_2": {
          "rpm": 3500,
          "tpm": 2000000.0,
          "batch_queue_limit": 5000000.0
        },
        "tier_3": {
          "rpm": 3500,
          "tpm": 800000.0,
          "batch_queue_limit": 50000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 10000000.0,
          "batch_queue_limit": 1000000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 50000000.0,
          "batch_queue_limit": 10000000000.0
        }
      }
    },
    {
      "name": "gpt-3.5-turbo-instruct",
      "slug": "gpt-3-5-turbo-instruct",
      "current_snapshot": "gpt-3.5-turbo-instruct",
      "tagline": "An older model only compatible with the legacy Completions endpoint",
      "description": "Similar capabilities as GPT-3 era models. Compatible with legacy Completions endpoint and not Chat Completions.\n",
      "type": "chat",
      "snapshots": [
        "gpt-3.5-turbo-instruct"
      ],
      "compare_prices": [
        "gpt-4o-mini",
        "o3-mini"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 3500,
          "rpd": 10000.0,
          "tpm": 200000.0,
          "batch_queue_limit": 2000000.0
        },
        "tier_2": {
          "rpm": 3500,
          "tpm": 2000000.0,
          "batch_queue_limit": 5000000.0
        },
        "tier_3": {
          "rpm": 3500,
          "tpm": 800000.0,
          "batch_queue_limit": 50000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 10000000.0,
          "batch_queue_limit": 1000000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 50000000.0,
          "batch_queue_limit": 10000000000.0
        }
      }
    },
    {
      "name": "gpt-3.5-turbo",
      "slug": "gpt-3-5-turbo",
      "display_name": "GPT-3.5 Turbo",
      "current_snapshot": "gpt-3.5-turbo-0125",
      "tagline": "Legacy GPT model for cheaper chat and non-chat tasks",
      "description": "GPT-3.5 Turbo models can understand and generate natural language or code and have been optimized for chat using the Chat Completions API but work well for non-chat tasks as well. As of July 2024, use gpt-4o-mini in place of GPT-3.5 Turbo, as it is cheaper, more capable, multimodal, and just as fast. GPT-3.5 Turbo is still available for use in the API.\n",
      "type": "chat",
      "snapshots": [
        "gpt-3.5-turbo-0125",
        "gpt-3.5-turbo-1106",
        "gpt-3.5-turbo-instruct"
      ],
      "compare_prices": [
        "gpt-4o-mini",
        "o3-mini"
      ],
      "point_to": "gpt-4o-mini",
      "rate_limits": {
        "tier_1": {
          "rpm": 3500,
          "rpd": 10000.0,
          "tpm": 200000.0,
          "batch_queue_limit": 2000000.0
        },
        "tier_2": {
          "rpm": 3500,
          "tpm": 2000000.0,
          "batch_queue_limit": 5000000.0
        },
        "tier_3": {
          "rpm": 3500,
          "tpm": 800000.0,
          "batch_queue_limit": 50000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 10000000.0,
          "batch_queue_limit": 1000000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 50000000.0,
          "batch_queue_limit": 10000000000.0
        }
      }
    },
    {
      "name": "gpt-4-turbo-preview",
      "slug": "gpt-4-turbo-preview",
      "display_name": "GPT-4 Turbo Preview",
      "current_snapshot": "gpt-4-0125-preview",
      "tagline": "An older fast GPT model",
      "description": "This is a research preview of the GPT-4 Turbo model, an older high-intelligence GPT model.\n",
      "type": "chat",
      "snapshots": [
        "gpt-4-0125-preview",
        "gpt-4-1106-vision-preview"
      ],
      "compare_prices": [
        "gpt-4o-mini",
        "o3-mini"
      ],
      "point_to": "gpt-4o",
      "rate_limits": {
        "tier_1": {
          "rpm": 500,
          "tpm": 30000.0,
          "batch_queue_limit": 90000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 450000.0,
          "batch_queue_limit": 1350000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 600000.0,
          "batch_queue_limit": 40000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 800000.0,
          "batch_queue_limit": 80000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 2000000.0,
          "batch_queue_limit": 300000000.0
        }
      }
    },
    {
      "name": "gpt-4-turbo",
      "slug": "gpt-4-turbo",
      "display_name": "GPT-4 Turbo",
      "current_snapshot": "gpt-4-turbo-2024-04-09",
      "tagline": "An older high-intelligence GPT model",
      "description": "GPT-4 Turbo is the next generation of GPT-4, an older high-intelligence GPT model. It was designed to be a cheaper, better version of GPT-4. Today, we recommend using a newer model like GPT-4o.\n",
      "type": "chat",
      "snapshots": [
        "gpt-4-turbo-2024-04-09"
      ],
      "compare_prices": [
        "gpt-4o-mini",
        "o3-mini"
      ],
      "point_to": "gpt-4o",
      "grouped_models": [
        "gpt-4-turbo-preview"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 500,
          "tpm": 30000.0,
          "batch_queue_limit": 90000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 450000.0,
          "batch_queue_limit": 1350000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 600000.0,
          "batch_queue_limit": 40000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 800000.0,
          "batch_queue_limit": 80000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 2000000.0,
          "batch_queue_limit": 300000000.0
        }
      }
    },
    {
      "name": "gpt-4.1-mini",
      "slug": "gpt-4.1-mini",
      "display_name": "GPT-4.1 mini",
      "current_snapshot": "gpt-4.1-mini-2025-04-14",
      "tagline": "Balanced for intelligence, speed, and cost",
      "description": "GPT-4.1 mini provides a balance between intelligence, speed, and cost that\nmakes it an attractive model for many use cases.\n",
      "type": "chat",
      "snapshots": [
        "gpt-4.1-mini-2025-04-14"
      ],
      "compare_prices": [
        "gpt-4o-mini",
        "gpt-4.1"
      ],
      "supported_tools": [
        "function_calling",
        "web_search",
        "file_search",
        "code_interpreter",
        "mcp"
      ],
      "rate_limits": [
        {
          "name": "Standard",
          "rate_limits": {
            "free": {
              "rpm": 3,
              "rpd": 200,
              "tpm": 40000.0
            },
            "tier_1": {
              "rpm": 500,
              "rpd": 10000.0,
              "tpm": 200000.0,
              "batch_queue_limit": 2000000.0
            },
            "tier_2": {
              "rpm": 5000.0,
              "tpm": 2000000.0,
              "batch_queue_limit": 20000000.0
            },
            "tier_3": {
              "rpm": 5000.0,
              "tpm": 4000000.0,
              "batch_queue_limit": 40000000.0
            },
            "tier_4": {
              "rpm": 10000.0,
              "tpm": 10000000.0,
              "batch_queue_limit": 1000000000.0
            },
            "tier_5": {
              "rpm": 30000.0,
              "tpm": 150000000.0,
              "batch_queue_limit": 15000000000.0
            }
          }
        },
        {
          "name": "Long Context",
          "tooltip": "> 128k input tokens",
          "rate_limits": {
            "tier_1": {
              "rpm": 200,
              "tpm": 400000.0,
              "batch_queue_limit": 5000000.0
            },
            "tier_2": {
              "rpm": 500,
              "tpm": 1000000.0,
              "batch_queue_limit": 40000000.0
            },
            "tier_3": {
              "rpm": 1000.0,
              "tpm": 2000000.0,
              "batch_queue_limit": 80000000.0
            },
            "tier_4": {
              "rpm": 2000.0,
              "tpm": 10000000.0,
              "batch_queue_limit": 200000000.0
            },
            "tier_5": {
              "rpm": 8000.0,
              "tpm": 20000000.0,
              "batch_queue_limit": 2000000000.0
            }
          }
        }
      ]
    },
    {
      "name": "gpt-4.1-nano",
      "slug": "gpt-4.1-nano",
      "display_name": "GPT-4.1 nano",
      "current_snapshot": "gpt-4.1-nano-2025-04-14",
      "tagline": "Fastest, most cost-effective GPT-4.1 model",
      "description": "GPT-4.1 nano is the fastest, most cost-effective GPT-4.1 model.\n",
      "type": "chat",
      "snapshots": [
        "gpt-4.1-nano-2025-04-14"
      ],
      "compare_prices": [
        "gpt-4.1-mini",
        "gpt-4o-mini"
      ],
      "supported_tools": [
        "function_calling",
        "file_search",
        "image_generation",
        "code_interpreter",
        "mcp"
      ],
      "rate_limits": [
        {
          "name": "Standard",
          "rate_limits": {
            "free": {
              "rpm": 3,
              "rpd": 200,
              "tpm": 40000.0
            },
            "tier_1": {
              "rpm": 500,
              "rpd": 10000.0,
              "tpm": 200000.0,
              "batch_queue_limit": 2000000.0
            },
            "tier_2": {
              "rpm": 5000.0,
              "tpm": 2000000.0,
              "batch_queue_limit": 20000000.0
            },
            "tier_3": {
              "rpm": 5000.0,
              "tpm": 4000000.0,
              "batch_queue_limit": 40000000.0
            },
            "tier_4": {
              "rpm": 10000.0,
              "tpm": 10000000.0,
              "batch_queue_limit": 1000000000.0
            },
            "tier_5": {
              "rpm": 30000.0,
              "tpm": 150000000.0,
              "batch_queue_limit": 15000000000.0
            }
          }
        },
        {
          "name": "Long Context",
          "tooltip": "> 128k input tokens",
          "rate_limits": {
            "tier_1": {
              "rpm": 200,
              "tpm": 400000.0,
              "batch_queue_limit": 5000000.0
            },
            "tier_2": {
              "rpm": 500,
              "tpm": 1000000.0,
              "batch_queue_limit": 40000000.0
            },
            "tier_3": {
              "rpm": 1000.0,
              "tpm": 2000000.0,
              "batch_queue_limit": 80000000.0
            },
            "tier_4": {
              "rpm": 2000.0,
              "tpm": 10000000.0,
              "batch_queue_limit": 200000000.0
            },
            "tier_5": {
              "rpm": 8000.0,
              "tpm": 20000000.0,
              "batch_queue_limit": 2000000000.0
            }
          }
        }
      ]
    },
    {
      "name": "gpt-4.1",
      "slug": "gpt-4.1",
      "display_name": "GPT-4.1",
      "current_snapshot": "gpt-4.1-2025-04-14",
      "tagline": "Excels at function calling and instruction following",
      "description": "GPT-4.1 is our flagship model for complex tasks. It is well suited for problem\nsolving across domains.\n",
      "type": "chat",
      "snapshots": [
        "gpt-4.1-2025-04-14"
      ],
      "compare_prices": [
        "gpt-4o",
        "o3-mini"
      ],
      "supported_tools": [
        "function_calling",
        "web_search",
        "file_search",
        "image_generation",
        "code_interpreter",
        "mcp"
      ],
      "rate_limits": [
        {
          "name": "default",
          "rate_limits": {
            "tier_1": {
              "rpm": 500,
              "tpm": 30000.0,
              "batch_queue_limit": 90000.0
            },
            "tier_2": {
              "rpm": 5000.0,
              "tpm": 450000.0,
              "batch_queue_limit": 1350000.0
            },
            "tier_3": {
              "rpm": 5000.0,
              "tpm": 800000.0,
              "batch_queue_limit": 50000000.0
            },
            "tier_4": {
              "rpm": 10000.0,
              "tpm": 2000000.0,
              "batch_queue_limit": 200000000.0
            },
            "tier_5": {
              "rpm": 10000.0,
              "tpm": 30000000.0,
              "batch_queue_limit": 5000000000.0
            }
          }
        },
        {
          "name": "Long Context",
          "tooltip": "> 128k input tokens",
          "rate_limits": {
            "tier_1": {
              "rpm": 100,
              "tpm": 200000.0,
              "batch_queue_limit": 2000000.0
            },
            "tier_2": {
              "rpm": 250,
              "tpm": 500000.0,
              "batch_queue_limit": 20000000.0
            },
            "tier_3": {
              "rpm": 500,
              "tpm": 1000000.0,
              "batch_queue_limit": 40000000.0
            },
            "tier_4": {
              "rpm": 1000.0,
              "tpm": 5000000.0,
              "batch_queue_limit": 100000000.0
            },
            "tier_5": {
              "rpm": 4000.0,
              "tpm": 10000000.0,
              "batch_queue_limit": 1000000000.0
            }
          }
        }
      ]
    },
    {
      "name": "gpt-4.5-preview",
      "slug": "gpt-4-5-preview",
      "display_name": "GPT-4.5 Preview (Deprecated)",
      "current_snapshot": "gpt-4.5-preview-2025-02-27",
      "tagline": "Deprecated large model.",
      "description": "Deprecated - a research preview of GPT-4.5. We recommend using gpt-4.1 or o3 \nmodels instead for most use cases.\n",
      "type": "chat",
      "snapshots": [
        "gpt-4.5-preview-2025-02-27"
      ],
      "compare_prices": [
        "gpt-4.1",
        "o3"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 1000.0,
          "tpm": 125000.0,
          "batch_queue_limit": 50000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 250000.0,
          "batch_queue_limit": 500000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 500000.0,
          "batch_queue_limit": 50000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 1000000.0,
          "batch_queue_limit": 100000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 2000000.0,
          "batch_queue_limit": 5000000000.0
        }
      }
    },
    {
      "name": "gpt-4",
      "slug": "gpt-4",
      "display_name": "GPT-4",
      "current_snapshot": "gpt-4-0613",
      "tagline": "An older high-intelligence GPT model",
      "description": "GPT-4 is an older version of a high-intelligence GPT model, usable in Chat Completions.\n",
      "type": "chat",
      "snapshots": [
        "gpt-4-0613",
        "gpt-4-0314"
      ],
      "compare_prices": [
        "gpt-4o-mini",
        "o3-mini"
      ],
      "point_to": "gpt-4o",
      "rate_limits": {
        "tier_1": {
          "rpm": 500,
          "rpd": 10000.0,
          "tpm": 10000.0,
          "batch_queue_limit": 100000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 40000.0,
          "batch_queue_limit": 200000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 80000.0,
          "batch_queue_limit": 5000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 300000.0,
          "batch_queue_limit": 30000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 1000000.0,
          "batch_queue_limit": 150000000.0
        }
      }
    },
    {
      "name": "gpt-4o-audio-preview",
      "slug": "gpt-4o-audio-preview",
      "display_name": "GPT-4o Audio",
      "current_snapshot": "gpt-4o-audio-preview-2025-06-03",
      "tagline": "GPT-4o models capable of audio inputs and outputs",
      "description": "This is a preview release of the GPT-4o Audio models. These models accept \naudio inputs and outputs, and can be used in the Chat Completions REST API.\n",
      "type": "chat",
      "snapshots": [
        "gpt-4o-audio-preview-2025-06-03",
        "gpt-4o-audio-preview-2024-12-17",
        "gpt-4o-audio-preview-2024-10-01"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 500,
          "tpm": 30000.0,
          "batch_queue_limit": 90000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 450000.0,
          "batch_queue_limit": 1350000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 800000.0,
          "batch_queue_limit": 50000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 2000000.0,
          "batch_queue_limit": 2000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 30000000.0,
          "batch_queue_limit": 5000000000.0
        }
      }
    },
    {
      "name": "gpt-4o-mini-audio-preview",
      "slug": "gpt-4o-mini-audio-preview",
      "display_name": "GPT-4o mini Audio",
      "current_snapshot": "gpt-4o-mini-audio-preview-2024-12-17",
      "tagline": "Smaller model capable of audio inputs and outputs",
      "description": "This is a preview release of the smaller GPT-4o Audio mini model. It's designed to input audio or create audio outputs via the REST API.\n",
      "type": "chat",
      "snapshots": [
        "gpt-4o-mini-audio-preview-2024-12-17"
      ],
      "supported_tools": [
        "web_search",
        "file_search",
        "code_interpreter",
        "mcp"
      ],
      "rate_limits": {
        "free": {
          "rpm": 3,
          "rpd": 200,
          "tpm": 40000.0
        },
        "tier_1": {
          "rpm": 500,
          "rpd": 10000.0,
          "tpm": 200000.0,
          "batch_queue_limit": 2000000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 2000000.0,
          "batch_queue_limit": 20000000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 4000000.0,
          "batch_queue_limit": 40000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 10000000.0,
          "batch_queue_limit": 1000000000.0
        },
        "tier_5": {
          "rpm": 30000.0,
          "tpm": 150000000.0,
          "batch_queue_limit": 15000000000.0
        }
      }
    },
    {
      "name": "gpt-4o-mini-realtime-preview",
      "slug": "gpt-4o-mini-realtime-preview",
      "display_name": "GPT-4o mini Realtime",
      "current_snapshot": "gpt-4o-mini-realtime-preview-2024-12-17",
      "tagline": "Smaller realtime model for text and audio inputs and outputs",
      "description": "This is a preview release of the GPT-4o-mini Realtime model, capable of responding to audio and text inputs in realtime over WebRTC or a WebSocket interface.\n",
      "type": "other",
      "playground_url": "/playground/realtime",
      "snapshots": [
        "gpt-4o-mini-realtime-preview-2024-12-17"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 200,
          "rpd": 1000.0,
          "tpm": 40000.0
        },
        "tier_2": {
          "rpm": 400,
          "tpm": 200000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 800000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 4000000.0
        },
        "tier_5": {
          "rpm": 20000.0,
          "tpm": 15000000.0
        }
      }
    },
    {
      "name": "gpt-4o-mini-search-preview",
      "slug": "gpt-4o-mini-search-preview",
      "display_name": "GPT-4o mini Search Preview",
      "current_snapshot": "gpt-4o-mini-search-preview-2025-03-11",
      "tagline": "Fast, affordable small model for web search",
      "description": "GPT-4o mini Search Preview is a specialized model trained to understand and execute [web search](/docs/guides/tools-web-search?api-mode=chat) queries with the Chat Completions API. In addition to token fees, web search queries have a fee per tool call. Learn more in the [pricing](/docs/pricing) page.\n",
      "type": "other",
      "snapshots": [
        "gpt-4o-mini-search-preview-2025-03-11"
      ],
      "compare_prices": [
        "gpt-4o",
        "gpt-4o-mini"
      ],
      "examples": null,
      "rate_limits": {
        "free": {
          "rpm": 3,
          "rpd": 200,
          "tpm": 40000.0
        },
        "tier_1": {
          "rpm": 500,
          "rpd": 10000.0,
          "tpm": 200000.0,
          "batch_queue_limit": 2000000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 2000000.0,
          "batch_queue_limit": 20000000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 4000000.0,
          "batch_queue_limit": 40000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 10000000.0,
          "batch_queue_limit": 1000000000.0
        },
        "tier_5": {
          "rpm": 30000.0,
          "tpm": 150000000.0,
          "batch_queue_limit": 15000000000.0
        }
      }
    },
    {
      "name": "gpt-4o-mini-transcribe",
      "slug": "gpt-4o-mini-transcribe",
      "display_name": "GPT-4o mini Transcribe",
      "current_snapshot": "gpt-4o-mini-transcribe",
      "tagline": "Speech-to-text model powered by GPT-4o mini",
      "description": "GPT-4o mini Transcribe is a speech-to-text model that uses GPT-4o mini to transcribe audio.\nIt offers improvements to word error rate and better language recognition and accuracy compared to original Whisper models. Use it for more accurate transcripts.\n",
      "type": "other",
      "snapshots": [
        "gpt-4o-mini-transcribe"
      ],
      "compare_prices": [
        "whisper-1",
        "gpt-4o-mini"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 500,
          "tpm": 50000.0
        },
        "tier_2": {
          "rpm": 2000.0,
          "tpm": 150000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 600000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 2000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 8000000.0
        }
      }
    },
    {
      "name": "gpt-4o-mini-tts",
      "slug": "gpt-4o-mini-tts",
      "display_name": "GPT-4o mini TTS",
      "current_snapshot": "gpt-4o-mini-tts",
      "tagline": "Text-to-speech model powered by GPT-4o mini",
      "description": "GPT-4o mini TTS is a text-to-speech model built on GPT-4o mini, a fast and powerful language model. Use it to convert text to natural sounding spoken text. The maximum number of input tokens is 2000.\n",
      "type": "other",
      "playground_url": "/playground/tts",
      "snapshots": [
        "gpt-4o-mini-tts"
      ],
      "compare_prices": [
        "gpt-4o-mini-realtime-preview",
        "gpt-4o-realtime-preview"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 500,
          "tpm": 50000.0
        },
        "tier_2": {
          "rpm": 2000.0,
          "tpm": 150000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 600000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 2000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 8000000.0
        }
      }
    },
    {
      "name": "gpt-4o-mini",
      "slug": "gpt-4o-mini",
      "display_name": "GPT-4o mini",
      "current_snapshot": "gpt-4o-mini-2024-07-18",
      "tagline": "Fast, affordable small model for focused tasks",
      "description": "GPT-4o mini (\u201co\u201d for \u201comni\u201d) is a fast, affordable small model for focused tasks. \nIt accepts both text and image inputs, and produces text outputs (including Structured Outputs). \nIt is ideal for fine-tuning, and model outputs from a larger model like GPT-4o can be distilled to GPT-4o-mini to produce similar results at lower cost and latency.\n",
      "type": "chat",
      "snapshots": [
        "gpt-4o-mini-2024-07-18"
      ],
      "compare_prices": [
        "gpt-4o",
        "o3-mini"
      ],
      "examples": [
        "classification",
        "keywords_search",
        "translation",
        "extract_tags"
      ],
      "supported_tools": [
        "function_calling",
        "web_search",
        "file_search",
        "image_generation",
        "code_interpreter",
        "mcp"
      ],
      "rate_limits": {
        "free": {
          "rpm": 3,
          "rpd": 200,
          "tpm": 40000.0
        },
        "tier_1": {
          "rpm": 500,
          "rpd": 10000.0,
          "tpm": 200000.0,
          "batch_queue_limit": 2000000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 2000000.0,
          "batch_queue_limit": 20000000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 4000000.0,
          "batch_queue_limit": 40000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 10000000.0,
          "batch_queue_limit": 1000000000.0
        },
        "tier_5": {
          "rpm": 30000.0,
          "tpm": 150000000.0,
          "batch_queue_limit": 15000000000.0
        }
      }
    },
    {
      "name": "gpt-4o-realtime-preview",
      "slug": "gpt-4o-realtime-preview",
      "display_name": "GPT-4o Realtime",
      "current_snapshot": "gpt-4o-realtime-preview-2025-06-03",
      "tagline": "Model capable of realtime text and audio inputs and outputs",
      "description": "This is a preview release of the GPT-4o Realtime model, capable of responding to audio and text inputs in realtime over WebRTC or a WebSocket interface.\n",
      "type": "other",
      "playground_url": "/playground/realtime",
      "snapshots": [
        "gpt-4o-realtime-preview-2025-06-03",
        "gpt-4o-realtime-preview-2024-12-17",
        "gpt-4o-realtime-preview-2024-10-01"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 200,
          "rpd": 1000.0,
          "tpm": 40000.0
        },
        "tier_2": {
          "rpm": 400,
          "tpm": 200000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 800000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 4000000.0
        },
        "tier_5": {
          "rpm": 20000.0,
          "tpm": 15000000.0
        }
      }
    },
    {
      "name": "gpt-4o-search-preview",
      "slug": "gpt-4o-search-preview",
      "display_name": "GPT-4o Search Preview",
      "current_snapshot": "gpt-4o-search-preview-2025-03-11",
      "tagline": "GPT model for web search in Chat Completions",
      "description": "GPT-4o Search Preview is a specialized model trained to understand and execute [web search](/docs/guides/tools-web-search?api-mode=chat) queries with the Chat Completions API. In addition to token fees, web search queries have a fee per tool call. Learn more in the [pricing](/docs/pricing) page.\n",
      "type": "other",
      "snapshots": [
        "gpt-4o-search-preview-2025-03-11"
      ],
      "compare_prices": [
        "gpt-4o-mini",
        "gpt-4o"
      ],
      "examples": null,
      "rate_limits": {
        "tier_1": {
          "rpm": 100,
          "tpm": 30000.0,
          "batch_queue_limit": 0
        },
        "tier_2": {
          "rpm": 500,
          "tpm": 45000.0,
          "batch_queue_limit": 0
        },
        "tier_3": {
          "rpm": 500,
          "tpm": 80000.0,
          "batch_queue_limit": 0
        },
        "tier_4": {
          "rpm": 1000.0,
          "tpm": 200000.0,
          "batch_queue_limit": 0
        },
        "tier_5": {
          "rpm": 1000.0,
          "tpm": 3000000.0,
          "batch_queue_limit": 0
        }
      }
    },
    {
      "name": "gpt-4o-transcribe",
      "slug": "gpt-4o-transcribe",
      "display_name": "GPT-4o Transcribe",
      "current_snapshot": "gpt-4o-transcribe",
      "tagline": "Speech-to-text model powered by GPT-4o",
      "description": "GPT-4o Transcribe is a speech-to-text model that uses GPT-4o to transcribe audio.\nIt offers improvements to word error rate and better language recognition and accuracy compared to original Whisper models. Use it for more accurate transcripts.\n",
      "type": "other",
      "snapshots": [
        "gpt-4o-transcribe"
      ],
      "compare_prices": [
        "whisper-1"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 500,
          "tpm": 10000.0
        },
        "tier_2": {
          "rpm": 2000.0,
          "tpm": 100000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 400000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 2000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 6000000.0
        }
      }
    },
    {
      "name": "gpt-4o",
      "slug": "gpt-4o",
      "display_name": "GPT-4o",
      "current_snapshot": "gpt-4o-2024-08-06",
      "tagline": "Fast, intelligent, flexible GPT model",
      "description": "GPT-4o (\u201co\u201d for \u201comni\u201d) is our versatile, high-intelligence flagship model.\nIt accepts both text and image inputs, and produces text outputs (including Structured Outputs).\nIt is the best model for most tasks, and is our most capable model outside of our o-series models.\n",
      "type": "chat",
      "snapshots": [
        "gpt-4o-2024-11-20",
        "gpt-4o-2024-08-06",
        "gpt-4o-2024-05-13"
      ],
      "compare_prices": [
        "gpt-4o-mini",
        "o3-mini"
      ],
      "examples": [
        "math_tutor",
        "travel_assistant",
        "clothing_recommendation",
        "recipe_generation"
      ],
      "supported_tools": [
        "function_calling",
        "web_search",
        "file_search",
        "image_generation",
        "code_interpreter",
        "mcp"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 500,
          "tpm": 30000.0,
          "batch_queue_limit": 90000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 450000.0,
          "batch_queue_limit": 1350000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 800000.0,
          "batch_queue_limit": 50000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 2000000.0,
          "batch_queue_limit": 200000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 30000000.0,
          "batch_queue_limit": 5000000000.0
        }
      }
    },
    {
      "name": "gpt-5-chat-latest",
      "slug": "gpt-5-chat-latest",
      "display_name": "GPT-5 Chat",
      "current_snapshot": "gpt-5-chat-latest",
      "tagline": "GPT-5 model used in ChatGPT",
      "description": "GPT-5 Chat points to the GPT-5 snapshot currently used in ChatGPT. \nGPT-5 is our next-generation, high-intelligence flagship model.\nIt accepts both text and image inputs, and produces text outputs.\n",
      "type": "reasoning",
      "snapshots": [
        "gpt-5-chat-latest"
      ],
      "compare_prices": [
        "gpt-5",
        "gpt-5-mini"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 500,
          "tpm": 30000.0,
          "batch_queue_limit": 50000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 450000.0,
          "batch_queue_limit": 1350000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 800000.0,
          "batch_queue_limit": 100000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 2000000.0,
          "batch_queue_limit": 200000000.0
        },
        "tier_5": {
          "rpm": 15000.0,
          "tpm": 40000000.0,
          "batch_queue_limit": 15000000000.0
        }
      }
    },
    {
      "name": "gpt-5-mini",
      "slug": "gpt-5-mini",
      "display_name": "GPT-5 mini",
      "current_snapshot": "gpt-5-mini-2025-08-07",
      "tagline": "A faster, more cost-efficient version of GPT-5 for well-defined tasks",
      "description": "GPT-5 mini is a faster, more cost-efficient version of GPT-5. It's great for\nwell-defined tasks and precise prompts. Learn more in our \n[GPT-5 usage guide](/docs/guides/gpt-5).\n",
      "type": "reasoning",
      "snapshots": [
        "gpt-5-mini-2025-08-07"
      ],
      "compare_prices": [
        "gpt-5",
        "gpt-5-nano"
      ],
      "supported_tools": [
        "function_calling",
        "web_search",
        "file_search",
        "code_interpreter",
        "mcp"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 500,
          "tpm": 200000.0,
          "batch_queue_limit": 2000000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 2000000.0,
          "batch_queue_limit": 20000000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 4000000.0,
          "batch_queue_limit": 40000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 10000000.0,
          "batch_queue_limit": 1000000000.0
        },
        "tier_5": {
          "rpm": 30000.0,
          "tpm": 180000000.0,
          "batch_queue_limit": 15000000000.0
        }
      }
    },
    {
      "name": "gpt-5-nano",
      "slug": "gpt-5-nano",
      "display_name": "GPT-5 nano",
      "current_snapshot": "gpt-5-nano-2025-08-07",
      "tagline": "Fastest, most cost-efficient version of GPT-5",
      "description": "GPT-5 Nano is our fastest, cheapest version of GPT-5. It's great for summarization and classification tasks.\nLearn more in our [GPT-5 usage guide](/docs/guides/gpt-5).\n",
      "type": "reasoning",
      "snapshots": [
        "gpt-5-nano-2025-08-07"
      ],
      "compare_prices": [
        "gpt-5-mini",
        "gpt-5"
      ],
      "supported_tools": [
        "function_calling",
        "file_search",
        "image_generation",
        "code_interpreter",
        "mcp"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 500,
          "tpm": 200000.0,
          "batch_queue_limit": 2000000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 2000000.0,
          "batch_queue_limit": 20000000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 4000000.0,
          "batch_queue_limit": 40000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 10000000.0,
          "batch_queue_limit": 1000000000.0
        },
        "tier_5": {
          "rpm": 30000.0,
          "tpm": 180000000.0,
          "batch_queue_limit": 15000000000.0
        }
      }
    },
    {
      "name": "gpt-5",
      "slug": "gpt-5",
      "display_name": "GPT-5",
      "current_snapshot": "gpt-5-2025-08-07",
      "tagline": "The best model for coding and agentic tasks across domains",
      "description": "GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains.\nLearn more in our [GPT-5 usage guide](/docs/guides/gpt-5).\n",
      "type": "reasoning",
      "snapshots": [
        "gpt-5-2025-08-07"
      ],
      "compare_prices": [
        "gpt-5-mini",
        "gpt-5-nano"
      ],
      "supported_tools": [
        "function_calling",
        "web_search",
        "file_search",
        "image_generation",
        "code_interpreter",
        "mcp"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 500,
          "tpm": 30000.0,
          "batch_queue_limit": 90000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 450000.0,
          "batch_queue_limit": 1350000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 800000.0,
          "batch_queue_limit": 100000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 2000000.0,
          "batch_queue_limit": 200000000.0
        },
        "tier_5": {
          "rpm": 15000.0,
          "tpm": 40000000.0,
          "batch_queue_limit": 15000000000.0
        }
      }
    },
    {
      "name": "gpt-image-1",
      "slug": "gpt-image-1",
      "display_name": "GPT Image 1",
      "current_snapshot": "gpt-image-1",
      "tagline": "State-of-the-art image generation model",
      "description": "GPT Image 1 is our new state-of-the-art image generation model. It is a natively multimodal language model that accepts both text and image inputs, and produces image outputs.\n",
      "type": "other",
      "snapshots": [
        "gpt-image-1"
      ],
      "playground_url": "/playground/images",
      "rate_limits": {
        "tier_1": {
          "tpm": 100000.0,
          "ipm": 5
        },
        "tier_2": {
          "tpm": 250000.0,
          "ipm": 20
        },
        "tier_3": {
          "tpm": 800000.0,
          "ipm": 50
        },
        "tier_4": {
          "tpm": 3000000.0,
          "ipm": 150
        },
        "tier_5": {
          "tpm": 8000000.0,
          "ipm": 250
        }
      }
    },
    {
      "name": "gpt-oss-120b",
      "slug": "gpt-oss-120b",
      "current_snapshot": "gpt-oss-120b",
      "tagline": "Our most powerful open weight model, which fits into a single H100 GPU",
      "description": "`gpt-oss-120b`is our most powerful open weight model, which fits into a single \nH100 GPU (117B parameters with 5.1B active parameters).\n\n[Download gpt-oss-120b on HuggingFace](https://huggingface.co/openai/gpt-oss-120b).\n\n**Key features**\n\n-   **Permissive Apache 2.0 license:** Build freely without copyleft restrictions or patent risk\u2014ideal for experimentation, customization, and commercial deployment.\n-   **Configurable reasoning effort:** Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs.\n-   **Full chain-of-thought:** Gain complete access to the model's reasoning process, facilitating easier debugging and increased trust in outputs.\n-   **Fine-tunable:** Fully customize models to your specific use case through parameter fine-tuning.\n-   **Agentic capabilities:** Use the models' native capabilities for function calling, web browsing, Python code execution, and structured outputs.\n",
      "type": "reasoning",
      "snapshots": [
        "gpt-oss-120b"
      ],
      "supported_tools": [
        "function_calling",
        "code_interpreter",
        "mcp",
        "web_search"
      ],
      "compare_prices": [
        "o3",
        "gpt-oss-20b"
      ],
      "playground_url": "none",
      "rate_limits": {
        "tier_1": {
          "rpm": 0,
          "tpm": 0,
          "batch_queue_limit": 0
        },
        "tier_2": {
          "rpm": 0,
          "tpm": 0,
          "batch_queue_limit": 0
        },
        "tier_3": {
          "rpm": 0,
          "tpm": 0,
          "batch_queue_limit": 0
        },
        "tier_4": {
          "rpm": 0,
          "tpm": 0,
          "batch_queue_limit": 0
        },
        "tier_5": {
          "rpm": 0,
          "tpm": 0,
          "batch_queue_limit": 0
        }
      }
    },
    {
      "name": "gpt-oss-20b",
      "slug": "gpt-oss-20b",
      "current_snapshot": "gpt-oss-20b",
      "tagline": "Our medium-sized open weight model for low latency",
      "description": "`gpt-oss-20b` isur medium-sized open model for low latency, local, or \nspecialized use-cases (21B parameters with 3.6B active parameters).\n\n[Download gpt-oss-20b on HuggingFace](https://huggingface.co/openai/gpt-oss-20b).\n\n**Key features**\n\n-   **Permissive Apache 2.0 license:** Build freely without copyleft restrictions or patent risk\u2014ideal for experimentation, customization, and commercial deployment.\n-   **Configurable reasoning effort:** Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs.\n-   **Full chain-of-thought:** Gain complete access to the model's reasoning process, facilitating easier debugging and increased trust in outputs.\n-   **Fine-tunable:** Fully customize models to your specific use case through parameter fine-tuning.\n-   **Agentic capabilities:** Use the models' native capabilities for function calling, web browsing, Python code execution, and structured outputs.\n",
      "type": "reasoning",
      "snapshots": [
        "gpt-oss-20b"
      ],
      "supported_tools": [
        "function_calling",
        "code_interpreter",
        "mcp",
        "web_search"
      ],
      "playground_url": "none",
      "compare_prices": [
        "o3",
        "gpt-oss-120b"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 0,
          "tpm": 0,
          "batch_queue_limit": 0
        },
        "tier_2": {
          "rpm": 0,
          "tpm": 0,
          "batch_queue_limit": 0
        },
        "tier_3": {
          "rpm": 0,
          "tpm": 0,
          "batch_queue_limit": 0
        },
        "tier_4": {
          "rpm": 0,
          "tpm": 0,
          "batch_queue_limit": 0
        },
        "tier_5": {
          "rpm": 0,
          "tpm": 0,
          "batch_queue_limit": 0
        }
      }
    },
    {
      "name": "o1-mini",
      "slug": "o1-mini",
      "current_snapshot": "o1-mini-2024-09-12",
      "tagline": "A small model alternative to o1",
      "description": "The o1 reasoning model is designed to solve hard problems across domains. o1-mini is a faster and more affordable reasoning model, but we recommend using the newer o3-mini model that features higher intelligence at the same latency and price as o1-mini.\n",
      "type": "reasoning",
      "snapshots": [
        "o1-mini-2024-09-12"
      ],
      "compare_prices": [
        "o1",
        "o3-mini"
      ],
      "point_to": "o3-mini",
      "deprecated": true,
      "supported_tools": [
        "file_search",
        "code_interpreter",
        "mcp"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 500,
          "tpm": 200000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 2000000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 4000000.0,
          "batch_queue_limit": 40000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 10000000.0,
          "batch_queue_limit": 1000000000.0
        },
        "tier_5": {
          "rpm": 30000.0,
          "tpm": 150000000.0,
          "batch_queue_limit": 15000000000.0
        }
      }
    },
    {
      "name": "o1-preview",
      "slug": "o1-preview",
      "display_name": "o1 Preview",
      "current_snapshot": "o1-preview-2024-09-12",
      "tagline": "Preview of our first o-series reasoning model",
      "description": "Research preview of the o1 series of models, trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user.\n",
      "type": "reasoning",
      "snapshots": [
        "o1-preview-2024-09-12"
      ],
      "compare_prices": [
        "o1",
        "o3-mini"
      ],
      "point_to": "o1",
      "deprecated": true,
      "rate_limits": {
        "tier_1": {
          "rpm": 500,
          "tpm": 30000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 450000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 800000.0,
          "batch_queue_limit": 50000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 2000000.0,
          "batch_queue_limit": 200000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 30000000.0,
          "batch_queue_limit": 5000000000.0
        }
      }
    },
    {
      "name": "o1-pro",
      "slug": "o1-pro",
      "current_snapshot": "o1-pro-2025-03-19",
      "tagline": "Version of o1 with more compute for better responses",
      "description": "The o1 series of models are trained with reinforcement learning to think \nbefore they answer and perform complex reasoning. The o1-pro model uses more \ncompute to think harder and provide consistently better answers.\n\no1-pro is available in the [Responses API only](/docs/api-reference/responses)\nto enable support for multi-turn model interactions before responding to API\nrequests, and other advanced API features in the future.\n",
      "type": "reasoning",
      "snapshots": [
        "o1-pro-2025-03-19"
      ],
      "compare_prices": [
        "o1",
        "o3-mini"
      ],
      "supported_tools": [
        "function_calling",
        "file_search",
        "mcp"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 500,
          "tpm": 30000.0,
          "batch_queue_limit": 90000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 450000.0,
          "batch_queue_limit": 1350000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 800000.0,
          "batch_queue_limit": 50000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 2000000.0,
          "batch_queue_limit": 200000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 30000000.0,
          "batch_queue_limit": 5000000000.0
        }
      }
    },
    {
      "name": "o1",
      "slug": "o1",
      "current_snapshot": "o1-2024-12-17",
      "tagline": "Previous full o-series reasoning model",
      "description": "The o1 series of models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user.\n",
      "type": "reasoning",
      "snapshots": [
        "o1-2024-12-17"
      ],
      "compare_prices": [
        "o1-mini",
        "o3-mini"
      ],
      "grouped_models": [
        "o1-preview"
      ],
      "supported_tools": [
        "function_calling",
        "file_search",
        "mcp"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 500,
          "tpm": 30000.0,
          "batch_queue_limit": 90000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 450000.0,
          "batch_queue_limit": 1350000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 800000.0,
          "batch_queue_limit": 50000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 2000000.0,
          "batch_queue_limit": 200000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 30000000.0,
          "batch_queue_limit": 5000000000.0
        }
      }
    },
    {
      "name": "o3-deep-research",
      "slug": "o3-deep-research",
      "current_snapshot": "o3-deep-research-2025-06-26",
      "tagline": "Our most powerful deep research model",
      "description": "o3-deep-research is our most advanced model for deep research, \ndesigned to tackle complex, multi-step research tasks. It can \nsearch and synthesize information from across the internet as \nwell as from your own data\u2014brought in through MCP connectors.\n\nLearn more about getting started with this model in our\n[deep research](/docs/guides/deep-research) guide.\n",
      "type": "reasoning",
      "snapshots": [
        "o3-deep-research-2025-06-26"
      ],
      "compare_prices": [
        "o1",
        "o4-mini"
      ],
      "supported_tools": [
        "web_search",
        "code_interpreter",
        "mcp"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 500,
          "tpm": 200000.0,
          "batch_queue_limit": 200000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 450000.0,
          "batch_queue_limit": 300000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 800000.0,
          "batch_queue_limit": 500000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 2000000.0,
          "batch_queue_limit": 2000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 30000000.0,
          "batch_queue_limit": 10000000.0
        }
      }
    },
    {
      "name": "o3-mini",
      "slug": "o3-mini",
      "current_snapshot": "o3-mini-2025-01-31",
      "tagline": "A small model alternative to o3",
      "description": "o3-mini is our newest small reasoning model, providing high intelligence at the same cost and latency targets of o1-mini. o3-mini supports key developer features, like Structured Outputs, function calling, and Batch API.\n",
      "type": "reasoning",
      "snapshots": [
        "o3-mini-2025-01-31"
      ],
      "compare_prices": [
        "gpt-4o-mini",
        "o1-mini"
      ],
      "examples": [
        "landing_page_generation",
        "analyze_policy",
        "text_to_sql",
        "graph_entity_extraction"
      ],
      "supported_tools": [
        "function_calling",
        "file_search",
        "code_interpreter",
        "mcp",
        "image_generation"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 1000.0,
          "tpm": 100000.0,
          "batch_queue_limit": 1000000.0
        },
        "tier_2": {
          "rpm": 2000.0,
          "tpm": 200000.0,
          "batch_queue_limit": 2000000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 4000000.0,
          "batch_queue_limit": 40000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 10000000.0,
          "batch_queue_limit": 1000000000.0
        },
        "tier_5": {
          "rpm": 30000.0,
          "tpm": 150000000.0,
          "batch_queue_limit": 15000000000.0
        }
      }
    },
    {
      "name": "o3-pro",
      "slug": "o3-pro",
      "current_snapshot": "o3-pro-2025-06-10",
      "tagline": "Version of o3 with more compute for better responses",
      "description": "The o-series of models are trained with reinforcement learning to think \nbefore they answer and perform complex reasoning. The o3-pro model uses more \ncompute to think harder and provide consistently better answers.\n\no3-pro is available in the [Responses API only](/docs/api-reference/responses)\nto enable support for multi-turn model interactions before responding to API \nrequests, and other advanced API features in the future. Since o3-pro is designed \nto tackle tough problems, some requests may take several minutes to finish. \nTo avoid timeouts, try using [background mode](/docs/guides/background).\n",
      "type": "reasoning",
      "snapshots": [
        "o3-pro-2025-06-10"
      ],
      "compare_prices": [
        "o3",
        "o3-mini"
      ],
      "supported_tools": [
        "function_calling",
        "file_search",
        "image_generation",
        "mcp",
        "web_search"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 500,
          "tpm": 30000.0,
          "batch_queue_limit": 90000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 450000.0,
          "batch_queue_limit": 1350000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 800000.0,
          "batch_queue_limit": 50000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 2000000.0,
          "batch_queue_limit": 200000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 30000000.0,
          "batch_queue_limit": 5000000000.0
        }
      }
    },
    {
      "name": "o3",
      "slug": "o3",
      "current_snapshot": "o3-2025-04-16",
      "tagline": "Our most powerful reasoning model",
      "description": "o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images. \n\nLearn more about how to use our reasoning models in our [reasoning](/docs/guides/reasoning?api-mode=responses) guide.\n",
      "type": "reasoning",
      "snapshots": [
        "o3-2025-04-16"
      ],
      "compare_prices": [
        "o1",
        "o4-mini"
      ],
      "supported_tools": [
        "function_calling",
        "file_search",
        "image_generation",
        "code_interpreter",
        "mcp",
        "web_search"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 500,
          "tpm": 30000.0,
          "batch_queue_limit": 90000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 450000.0,
          "batch_queue_limit": 1350000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 800000.0,
          "batch_queue_limit": 50000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 2000000.0,
          "batch_queue_limit": 200000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 30000000.0,
          "batch_queue_limit": 5000000000.0
        }
      }
    },
    {
      "name": "o4-mini-deep-research",
      "slug": "o4-mini-deep-research",
      "current_snapshot": "o4-mini-deep-research-2025-06-26",
      "tagline": "Faster, more affordable deep research model",
      "description": "o4-mini-deep-research is our faster, more affordable deep \nresearch model\u2014ideal for tackling complex, multi-step research \ntasks. It can search and synthesize information from across the \ninternet as well as from your own data, brought in through \nMCP connectors.\n\nLearn more about how to use this model in our\n[deep research](/docs/guides/deep-research) guide.\n",
      "type": "reasoning",
      "snapshots": [
        "o4-mini-deep-research-2025-06-26"
      ],
      "compare_prices": [
        "o3",
        "o3-mini"
      ],
      "supported_tools": [
        "web_search",
        "code_interpreter",
        "mcp"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 1000.0,
          "tpm": 200000.0,
          "batch_queue_limit": 200000.0
        },
        "tier_2": {
          "rpm": 2000.0,
          "tpm": 2000000.0,
          "batch_queue_limit": 300000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 4000000.0,
          "batch_queue_limit": 500000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 10000000.0,
          "batch_queue_limit": 2000000.0
        },
        "tier_5": {
          "rpm": 30000.0,
          "tpm": 150000000.0,
          "batch_queue_limit": 10000000.0
        }
      }
    },
    {
      "name": "o4-mini",
      "slug": "o4-mini",
      "current_snapshot": "o4-mini-2025-04-16",
      "tagline": "Faster, more affordable reasoning model",
      "description": "o4-mini is our latest small o-series model. It's optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. \n\nLearn more about how to use our reasoning models in our [reasoning](/docs/guides/reasoning?api-mode=responses) guide.\n",
      "type": "reasoning",
      "snapshots": [
        "o4-mini-2025-04-16"
      ],
      "compare_prices": [
        "o3",
        "o3-mini"
      ],
      "supported_tools": [
        "function_calling",
        "file_search",
        "code_interpreter",
        "mcp",
        "web_search"
      ],
      "rate_limits": {
        "tier_1": {
          "rpm": 1000.0,
          "tpm": 100000.0,
          "batch_queue_limit": 1000000.0
        },
        "tier_2": {
          "rpm": 2000.0,
          "tpm": 2000000.0,
          "batch_queue_limit": 2000000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 4000000.0,
          "batch_queue_limit": 40000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 10000000.0,
          "batch_queue_limit": 1000000000.0
        },
        "tier_5": {
          "rpm": 30000.0,
          "tpm": 150000000.0,
          "batch_queue_limit": 15000000000.0
        }
      }
    },
    {
      "name": "omni-moderation-latest",
      "display_name": "omni-moderation",
      "slug": "omni-moderation-latest",
      "current_snapshot": "omni-moderation-2024-09-26",
      "tagline": "Identify potentially harmful content in text and images",
      "description": "Moderation models are free models designed to detect harmful content.\nThis model is our most capable moderation model, accepting images as input as well.\n",
      "type": "other",
      "snapshots": [
        "omni-moderation-2024-09-26"
      ],
      "rate_limits": {
        "free": {
          "rpm": 250,
          "rpd": 5000.0,
          "tpm": 10000.0
        },
        "tier_1": {
          "rpm": 500,
          "rpd": 10000.0,
          "tpm": 10000.0
        },
        "tier_2": {
          "rpm": 500,
          "tpm": 20000.0
        },
        "tier_3": {
          "rpm": 1000.0,
          "tpm": 50000.0
        },
        "tier_4": {
          "rpm": 2000.0,
          "tpm": 250000.0
        },
        "tier_5": {
          "rpm": 5000.0,
          "tpm": 500000.0
        }
      }
    },
    {
      "name": "text-embedding-3-large",
      "slug": "text-embedding-3-large",
      "current_snapshot": "text-embedding-3-large",
      "tagline": "Most capable embedding model",
      "description": "text-embedding-3-large is our most capable embedding model for both english and non-english tasks.\nEmbeddings are a numerical representation of text that can be used to measure the relatedness between two pieces of text.\nEmbeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks.\n",
      "type": "other",
      "snapshots": [
        "text-embedding-3-large"
      ],
      "compare_prices": [
        "text-embedding-3-small"
      ],
      "rate_limits": {
        "free": {
          "rpm": 100,
          "rpd": 2000.0,
          "tpm": 40000.0
        },
        "tier_1": {
          "rpm": 3000.0,
          "tpm": 1000000.0,
          "batch_queue_limit": 3000000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 1000000.0,
          "batch_queue_limit": 20000000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 5000000.0,
          "batch_queue_limit": 100000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 5000000.0,
          "batch_queue_limit": 500000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 10000000.0,
          "batch_queue_limit": 4000000000.0
        }
      }
    },
    {
      "name": "text-embedding-3-small",
      "slug": "text-embedding-3-small",
      "current_snapshot": "text-embedding-3-small",
      "tagline": "Small embedding model",
      "description": "text-embedding-3-small is our improved, more performant version of our ada embedding model.\nEmbeddings are a numerical representation of text that can be used to measure the relatedness between two pieces of text.\nEmbeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks.\n",
      "type": "other",
      "snapshots": [
        "text-embedding-3-small"
      ],
      "compare_prices": [
        "text-embedding-3-large"
      ],
      "rate_limits": {
        "free": {
          "rpm": 100,
          "rpd": 2000.0,
          "tpm": 40000.0
        },
        "tier_1": {
          "rpm": 3000.0,
          "tpm": 1000000.0,
          "batch_queue_limit": 3000000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 1000000.0,
          "batch_queue_limit": 20000000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 5000000.0,
          "batch_queue_limit": 100000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 5000000.0,
          "batch_queue_limit": 500000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 10000000.0,
          "batch_queue_limit": 4000000000.0
        }
      }
    },
    {
      "name": "text-embedding-ada-002",
      "slug": "text-embedding-ada-002",
      "current_snapshot": "text-embedding-ada-002",
      "tagline": "Older embedding model",
      "description": "text-embedding-ada-002 is our improved, more performant version of our ada embedding model.\nEmbeddings are a numerical representation of text that can be used to measure the relatedness between two pieces of text.\nEmbeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks.\n",
      "type": "other",
      "snapshots": [
        "text-embedding-ada-002"
      ],
      "compare_prices": [
        "text-embedding-3-small"
      ],
      "rate_limits": {
        "free": {
          "rpm": 100,
          "rpd": 2000.0,
          "tpm": 40000.0
        },
        "tier_1": {
          "rpm": 3000.0,
          "tpm": 1000000.0,
          "batch_queue_limit": 3000000.0
        },
        "tier_2": {
          "rpm": 5000.0,
          "tpm": 1000000.0,
          "batch_queue_limit": 20000000.0
        },
        "tier_3": {
          "rpm": 5000.0,
          "tpm": 5000000.0,
          "batch_queue_limit": 100000000.0
        },
        "tier_4": {
          "rpm": 10000.0,
          "tpm": 5000000.0,
          "batch_queue_limit": 500000000.0
        },
        "tier_5": {
          "rpm": 10000.0,
          "tpm": 10000000.0,
          "batch_queue_limit": 4000000000.0
        }
      }
    },
    {
      "name": "tts-1-hd",
      "slug": "tts-1-hd",
      "display_name": "TTS-1 HD",
      "current_snapshot": "tts-1-hd",
      "tagline": "Text-to-speech model optimized for quality",
      "description": "TTS is a model that converts text to natural sounding spoken text. The tts-1-hd model is optimized for high quality text-to-speech use cases. Use it with the Speech endpoint in the Audio API.\n",
      "type": "other",
      "playground_url": "/playground/tts",
      "snapshots": [
        "tts-1-hd"
      ],
      "compare_prices": [
        "tts-1",
        "gpt-4o-mini-realtime-preview"
      ],
      "rate_limits": {
        "free": null,
        "tier_1": {
          "rpm": 500
        },
        "tier_2": {
          "rpm": 2500
        },
        "tier_3": {
          "rpm": 5000.0
        },
        "tier_4": {
          "rpm": 7500
        },
        "tier_5": {
          "rpm": 10000.0
        }
      }
    },
    {
      "name": "tts-1",
      "slug": "tts-1",
      "display_name": "TTS-1",
      "current_snapshot": "tts-1",
      "tagline": "Text-to-speech model optimized for speed",
      "description": "TTS is a model that converts text to natural sounding spoken text. The tts-1 model is optimized for realtime text-to-speech use cases. Use it with the Speech endpoint in the Audio API.\n",
      "type": "other",
      "playground_url": "/playground/tts",
      "snapshots": [
        "tts-1"
      ],
      "compare_prices": [
        "tts-1-hd",
        "gpt-4o-mini-realtime-preview"
      ],
      "rate_limits": {
        "free": {
          "rpm": 3,
          "rpd": 200
        },
        "tier_1": {
          "rpm": 500
        },
        "tier_2": {
          "rpm": 2500
        },
        "tier_3": {
          "rpm": 5000.0
        },
        "tier_4": {
          "rpm": 7500
        },
        "tier_5": {
          "rpm": 10000.0
        }
      }
    },
    {
      "name": "whisper-1",
      "slug": "whisper-1",
      "display_name": "Whisper",
      "current_snapshot": "whisper-1",
      "tagline": "General-purpose speech recognition model",
      "description": "Whisper is a general-purpose speech recognition model, trained on a large dataset of diverse audio. You can also use it as a multitask model to perform multilingual speech recognition as well as speech translation and language identification.\n",
      "type": "other",
      "snapshots": [
        "whisper-1"
      ],
      "rate_limits": {
        "free": {
          "rpm": 3,
          "rpd": 200
        },
        "tier_1": {
          "rpm": 500
        },
        "tier_2": {
          "rpm": 2500
        },
        "tier_3": {
          "rpm": 5000.0
        },
        "tier_4": {
          "rpm": 7500
        },
        "tier_5": {
          "rpm": 10000.0
        }
      }
    }
  ],
  "modalities": [
    {
      "name": "babbage-002",
      "slug": "babbage-002",
      "performance": 1,
      "latency": 3,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "max_output_tokens": 16384,
      "knowledge_cutoff": 1630454400000.0,
      "supported_features": [
        "fine_tuning"
      ],
      "supported_endpoints": [
        "completions"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "chatgpt-4o-latest",
      "slug": "chatgpt-4o-latest",
      "performance": 3,
      "latency": 3,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 128000.0,
      "max_output_tokens": 16384,
      "knowledge_cutoff": 1696118400000.0,
      "supported_features": [
        "streaming",
        "predicted_outputs",
        "image_input"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses"
      ],
      "reasoning_tokens": false
    },
    {
      "name": "codex-mini-latest",
      "slug": "codex-mini-latest",
      "performance": 4,
      "latency": 3,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 200000.0,
      "max_output_tokens": 100000.0,
      "knowledge_cutoff": 1717200000000.0,
      "supported_features": [
        "streaming",
        "structured_outputs",
        "function_calling",
        "image_input",
        "prompt_caching",
        "evals",
        "stored_completions"
      ],
      "supported_endpoints": [
        "responses"
      ],
      "reasoning_tokens": true
    },
    {
      "name": "computer-use-preview-2025-03-11",
      "slug": "computer-use-preview-2025-03-11",
      "performance": 2,
      "latency": 2,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 8192,
      "max_output_tokens": 1024,
      "knowledge_cutoff": 1696118400000.0,
      "supported_features": [
        "function_calling"
      ],
      "supported_endpoints": [
        "responses",
        "batch"
      ],
      "reasoning_tokens": true,
      "price_data": {
        "main": {
          "input": 3,
          "output": 12
        },
        "batch": {
          "input": 1.5,
          "output": 6
        }
      }
    },
    {
      "name": "dall-e-2",
      "slug": "dall-e-2",
      "performance": 1,
      "latency": 2,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "image"
        ]
      },
      "supported_endpoints": [
        "image_generation",
        "image_edit"
      ],
      "supported_features": [
        "inpainting"
      ],
      "reasoning_tokens": false
    },
    {
      "name": "dall-e-3",
      "slug": "dall-e-3",
      "performance": 3,
      "latency": 2,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "image"
        ]
      },
      "supported_endpoints": [
        "image_generation"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "davinci-002",
      "slug": "davinci-002",
      "performance": 1,
      "latency": 3,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "max_output_tokens": 16384,
      "knowledge_cutoff": 1630454400000.0,
      "supported_features": [
        "fine_tuning"
      ],
      "supported_endpoints": [
        "completions"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "gpt-3.5-0301",
      "slug": "gpt-3-5-0301",
      "performance": 1,
      "latency": 2,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 16385,
      "max_output_tokens": 4096,
      "knowledge_cutoff": 1630454400000.0,
      "supported_features": [
        "fine_tuning"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses"
      ],
      "reasoning_tokens": false
    },
    {
      "name": "gpt-3.5-turbo-0125",
      "slug": "gpt-3-5-turbo-0125",
      "performance": 1,
      "latency": 2,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 16385,
      "max_output_tokens": 4096,
      "knowledge_cutoff": 1630454400000.0,
      "supported_features": [
        "fine_tuning"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "batch",
        "fine_tuning"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "gpt-3.5-turbo-0613",
      "slug": "gpt-3-5-turbo-0613",
      "performance": 1,
      "latency": 2,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 16385,
      "max_output_tokens": 4096,
      "knowledge_cutoff": 1630454400000.0,
      "supported_features": [
        "fine_tuning"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "batch"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "gpt-3.5-turbo-1106",
      "slug": "gpt-3-5-turbo-1106",
      "performance": 1,
      "latency": 2,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 16385,
      "max_output_tokens": 4096,
      "knowledge_cutoff": 1630454400000.0,
      "supported_features": [
        "fine_tuning"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "batch",
        "fine_tuning"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "gpt-3.5-turbo-16k-0613",
      "slug": "gpt-3-5-turbo-16k-0613",
      "performance": 1,
      "latency": 2,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 16385,
      "max_output_tokens": 4096,
      "knowledge_cutoff": 1630454400000.0,
      "supported_features": [
        "fine_tuning"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "batch"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "gpt-3.5-turbo-instruct",
      "slug": "gpt-3-5-turbo-instruct",
      "performance": 1,
      "latency": 2,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 4096,
      "max_output_tokens": 4096,
      "knowledge_cutoff": 1630454400000.0,
      "supported_features": [
        "fine_tuning"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "gpt-4-0125-preview",
      "slug": "gpt-4-0125-preview",
      "performance": 2,
      "latency": 3,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 128000.0,
      "max_output_tokens": 4096,
      "knowledge_cutoff": 1701388800000.0,
      "supported_features": [
        "fine_tuning"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "assistants"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "gpt-4-0314",
      "slug": "gpt-4-0314",
      "performance": 2,
      "latency": 3,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 8192,
      "max_output_tokens": 8192,
      "knowledge_cutoff": 1701388800000.0,
      "supported_features": [
        "fine_tuning",
        "streaming"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "assistants"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 30,
          "output": 60
        },
        "batch": {
          "input": 15,
          "output": 30
        }
      }
    },
    {
      "name": "gpt-4-0613",
      "slug": "gpt-4-0613",
      "performance": 2,
      "latency": 3,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 8192,
      "max_output_tokens": 8192,
      "knowledge_cutoff": 1701388800000.0,
      "supported_features": [
        "fine_tuning",
        "streaming"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "assistants",
        "batch",
        "fine_tuning"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "gpt-4-1106-vision-preview",
      "slug": "gpt-4-1106-vision-preview",
      "performance": 2,
      "latency": 3,
      "deprecated": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 128000.0,
      "max_output_tokens": 4096,
      "knowledge_cutoff": 1701388800000.0,
      "supported_features": [
        "fine_tuning",
        "streaming"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "assistants"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "gpt-4-turbo-2024-04-09",
      "slug": "gpt-4-turbo-2024-04-09",
      "performance": 2,
      "latency": 3,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 128000.0,
      "max_output_tokens": 4096,
      "knowledge_cutoff": 1701388800000.0,
      "supported_features": [
        "streaming",
        "function_calling",
        "image_input"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "assistants",
        "batch"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "gpt-4.1-2025-04-14",
      "slug": "gpt-4.1-2025-04-14",
      "performance": 4,
      "latency": 3,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 1047576,
      "max_output_tokens": 32768,
      "knowledge_cutoff": 1717200000000.0,
      "supported_features": [
        "streaming",
        "structured_outputs",
        "predicted_outputs",
        "distillation",
        "function_calling",
        "file_search",
        "file_uploads",
        "image_input",
        "web_search",
        "fine_tuning",
        "prompt_caching"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "assistants",
        "batch",
        "fine_tuning"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "gpt-4.1-mini-2025-04-14",
      "slug": "gpt-4.1-mini-2025-04-14",
      "performance": 3,
      "latency": 4,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 1047576,
      "max_output_tokens": 32768,
      "knowledge_cutoff": 1717200000000.0,
      "supported_features": [
        "predicted_outputs",
        "streaming",
        "function_calling",
        "fine_tuning",
        "file_search",
        "file_uploads",
        "web_search",
        "structured_outputs",
        "image_input"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "assistants",
        "batch",
        "fine_tuning"
      ],
      "reasoning_tokens": false
    },
    {
      "name": "gpt-4.1-nano-2025-04-14",
      "slug": "gpt-4.1-nano-2025-04-14",
      "performance": 2,
      "latency": 5,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 1047576,
      "max_output_tokens": 32768,
      "knowledge_cutoff": 1717200000000.0,
      "supported_features": [
        "predicted_outputs",
        "streaming",
        "function_calling",
        "file_search",
        "file_uploads",
        "structured_outputs",
        "image_input",
        "prompt_caching",
        "fine_tuning"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "assistants",
        "batch",
        "fine_tuning"
      ],
      "reasoning_tokens": false
    },
    {
      "name": "gpt-4.5-preview-2025-02-27",
      "slug": "gpt-4.5-preview-2025-02-27",
      "performance": 4,
      "latency": 3,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 128000.0,
      "max_output_tokens": 16384,
      "knowledge_cutoff": 1696118400000.0,
      "supported_features": [
        "function_calling",
        "structured_outputs",
        "streaming",
        "system_messages",
        "evals",
        "prompt_caching",
        "image_input"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "assistants",
        "batch"
      ],
      "reasoning_tokens": false
    },
    {
      "name": "gpt-4o-2024-05-13",
      "slug": "gpt-4o-2024-05-13",
      "performance": 3,
      "latency": 3,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 128000.0,
      "max_output_tokens": 4096,
      "knowledge_cutoff": 1696118400000.0,
      "supported_features": [
        "streaming",
        "function_calling",
        "fine_tuning",
        "file_search",
        "file_uploads",
        "image_input",
        "web_search",
        "predicted_outputs"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "assistants",
        "batch"
      ],
      "reasoning_tokens": false
    },
    {
      "name": "gpt-4o-2024-08-06",
      "slug": "gpt-4o-2024-08-06",
      "performance": 3,
      "latency": 3,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 128000.0,
      "max_output_tokens": 16384,
      "knowledge_cutoff": 1696118400000.0,
      "supported_features": [
        "streaming",
        "structured_outputs",
        "predicted_outputs",
        "distillation",
        "file_search",
        "file_uploads",
        "fine_tuning",
        "function_calling",
        "image_input",
        "web_search"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "assistants",
        "batch",
        "fine_tuning"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "gpt-4o-2024-11-20",
      "slug": "gpt-4o-2024-11-20",
      "performance": 3,
      "latency": 3,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 128000.0,
      "max_output_tokens": 16384,
      "knowledge_cutoff": 1696118400000.0,
      "supported_features": [
        "streaming",
        "structured_outputs",
        "predicted_outputs",
        "distillation",
        "function_calling",
        "file_search",
        "file_uploads",
        "image_input",
        "web_search"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "assistants",
        "batch"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "gpt-4o-audio-preview-2024-10-01",
      "slug": "gpt-4o-audio-preview-2024-10-01",
      "performance": 3,
      "latency": 3,
      "modalities": {
        "input": [
          "text",
          "audio"
        ],
        "output": [
          "text",
          "audio"
        ]
      },
      "context_window": 128000.0,
      "max_output_tokens": 16384,
      "knowledge_cutoff": 1696118400000.0,
      "supported_features": [
        "streaming",
        "function_calling"
      ],
      "supported_endpoints": [
        "chat_completions"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "gpt-4o-audio-preview-2024-12-17",
      "slug": "gpt-4o-audio-preview-2024-12-17",
      "performance": 3,
      "latency": 3,
      "modalities": {
        "input": [
          "text",
          "audio"
        ],
        "output": [
          "text",
          "audio"
        ]
      },
      "context_window": 128000.0,
      "max_output_tokens": 16384,
      "knowledge_cutoff": 1696118400000.0,
      "supported_features": [
        "streaming",
        "function_calling"
      ],
      "supported_endpoints": [
        "chat_completions"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "gpt-4o-audio-preview-2025-06-03",
      "slug": "gpt-4o-audio-preview-2025-06-03",
      "performance": 3,
      "latency": 3,
      "modalities": {
        "input": [
          "text",
          "audio"
        ],
        "output": [
          "text",
          "audio"
        ]
      },
      "context_window": 128000.0,
      "max_output_tokens": 16384,
      "knowledge_cutoff": 1696118400000.0,
      "supported_features": [
        "streaming",
        "function_calling"
      ],
      "supported_endpoints": [
        "chat_completions"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "gpt-4o-mini-2024-07-18",
      "slug": "gpt-4o-mini-2024-07-18",
      "performance": 2,
      "latency": 4,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 128000.0,
      "max_output_tokens": 16384,
      "knowledge_cutoff": 1696118400000.0,
      "supported_features": [
        "predicted_outputs",
        "streaming",
        "function_calling",
        "fine_tuning",
        "file_search",
        "file_uploads",
        "web_search",
        "structured_outputs",
        "image_input"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "assistants",
        "batch",
        "fine_tuning"
      ],
      "reasoning_tokens": false
    },
    {
      "name": "gpt-4o-mini-audio-preview-2024-12-17",
      "slug": "gpt-4o-mini-audio-preview-2024-12-17",
      "performance": 2,
      "latency": 4,
      "modalities": {
        "input": [
          "text",
          "audio"
        ],
        "output": [
          "text",
          "audio"
        ]
      },
      "context_window": 128000.0,
      "max_output_tokens": 16384,
      "knowledge_cutoff": 1696118400000.0,
      "supported_features": [
        "streaming",
        "function_calling"
      ],
      "supported_endpoints": [
        "chat_completions"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "gpt-4o-mini-realtime-preview-2024-12-17",
      "slug": "gpt-4o-mini-realtime-preview-2024-12-17",
      "performance": 2,
      "latency": 5,
      "modalities": {
        "input": [
          "text",
          "audio"
        ],
        "output": [
          "text",
          "audio"
        ]
      },
      "context_window": 128000.0,
      "max_output_tokens": 4096,
      "knowledge_cutoff": 1696118400000.0,
      "supported_features": [
        "function_calling",
        "prompt_caching"
      ],
      "supported_endpoints": [
        "realtime"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "gpt-4o-mini-search-preview-2025-03-11",
      "slug": "gpt-4o-mini-search-preview-2025-03-11",
      "performance": 2,
      "latency": 4,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 128000.0,
      "max_output_tokens": 16384,
      "knowledge_cutoff": 1696118400000.0,
      "supported_features": [
        "streaming",
        "structured_outputs",
        "image_input"
      ],
      "supported_endpoints": [
        "chat_completions"
      ],
      "reasoning_tokens": false
    },
    {
      "name": "gpt-4o-mini-transcribe",
      "slug": "gpt-4o-mini-transcribe",
      "performance": 3,
      "latency": 4,
      "modalities": {
        "input": [
          "audio",
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 16000.0,
      "max_output_tokens": 2000.0,
      "knowledge_cutoff": 1717200000000.0,
      "supported_endpoints": [
        "transcription",
        "realtime"
      ],
      "reasoning_tokens": false
    },
    {
      "name": "gpt-4o-mini-tts",
      "slug": "gpt-4o-mini-tts",
      "performance": 4,
      "latency": 4,
      "current_snapshot": "gpt-4o-mini-tts",
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "audio"
        ]
      },
      "supported_endpoints": [
        "speech_generation"
      ],
      "reasoning_tokens": false
    },
    {
      "name": "gpt-4o-realtime-preview-2024-10-01",
      "slug": "gpt-4o-realtime-preview-2024-10-01",
      "performance": 2,
      "latency": 4,
      "modalities": {
        "input": [
          "text",
          "audio"
        ],
        "output": [
          "text",
          "audio"
        ]
      },
      "context_window": 16000.0,
      "max_output_tokens": 4096,
      "knowledge_cutoff": 1696118400000.0,
      "supported_features": [
        "function_calling",
        "prompt_caching"
      ],
      "supported_endpoints": [
        "realtime"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "gpt-4o-realtime-preview-2024-12-17",
      "slug": "gpt-4o-realtime-preview-2024-12-17",
      "performance": 3,
      "latency": 4,
      "modalities": {
        "input": [
          "text",
          "audio"
        ],
        "output": [
          "text",
          "audio"
        ]
      },
      "context_window": 16000.0,
      "max_output_tokens": 4096,
      "knowledge_cutoff": 1696118400000.0,
      "supported_features": [
        "function_calling",
        "prompt_caching"
      ],
      "supported_endpoints": [
        "realtime"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "gpt-4o-realtime-preview-2025-06-03",
      "slug": "gpt-4o-realtime-preview-2025-06-03",
      "performance": 3,
      "latency": 4,
      "modalities": {
        "input": [
          "text",
          "audio"
        ],
        "output": [
          "text",
          "audio"
        ]
      },
      "context_window": 32000.0,
      "max_output_tokens": 4096,
      "knowledge_cutoff": 1696118400000.0,
      "supported_features": [
        "function_calling",
        "prompt_caching"
      ],
      "supported_endpoints": [
        "realtime"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "gpt-4o-search-preview-2025-03-11",
      "slug": "gpt-4o-search-preview-2025-03-11",
      "performance": 3,
      "latency": 3,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 128000.0,
      "max_output_tokens": 16384,
      "knowledge_cutoff": 1696118400000.0,
      "supported_features": [
        "streaming",
        "structured_outputs",
        "image_input"
      ],
      "supported_endpoints": [
        "chat_completions"
      ],
      "reasoning_tokens": false,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "gpt-4o-transcribe",
      "slug": "gpt-4o-transcribe",
      "performance": 4,
      "latency": 3,
      "modalities": {
        "input": [
          "audio",
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 16000.0,
      "max_output_tokens": 2000.0,
      "knowledge_cutoff": 1717200000000.0,
      "supported_endpoints": [
        "transcription",
        "realtime"
      ],
      "reasoning_tokens": false
    },
    {
      "name": "gpt-5-2025-08-07",
      "slug": "gpt-5-2025-08-07",
      "performance": 4,
      "latency": 3,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 400000.0,
      "max_output_tokens": 128000.0,
      "max_input_tokens": 272000.0,
      "knowledge_cutoff": 1727740800000.0,
      "supported_features": [
        "streaming",
        "structured_outputs",
        "predicted_outputs",
        "distillation",
        "function_calling",
        "file_search",
        "file_uploads",
        "image_input",
        "web_search",
        "fine_tuning",
        "prompt_caching"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "assistants",
        "batch"
      ],
      "reasoning_tokens": true,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "gpt-5-chat-latest",
      "slug": "gpt-5-chat-latest",
      "performance": 3,
      "latency": 3,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 400000.0,
      "max_output_tokens": 128000.0,
      "max_input_tokens": 272000.0,
      "knowledge_cutoff": 1727654400000.0,
      "supported_features": [
        "streaming",
        "predicted_outputs",
        "image_input"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses"
      ],
      "reasoning_tokens": true
    },
    {
      "name": "gpt-5-mini-2025-08-07",
      "slug": "gpt-5-mini-2025-08-07",
      "performance": 3,
      "latency": 4,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 400000.0,
      "max_output_tokens": 128000.0,
      "max_input_tokens": 272000.0,
      "knowledge_cutoff": 1717113600000.0,
      "supported_features": [
        "streaming",
        "function_calling",
        "fine_tuning",
        "file_search",
        "file_uploads",
        "web_search",
        "structured_outputs",
        "image_input"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "assistants",
        "batch"
      ],
      "reasoning_tokens": true
    },
    {
      "name": "gpt-5-nano-2025-08-07",
      "slug": "gpt-5-nano-2025-08-07",
      "performance": 2,
      "latency": 5,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 400000.0,
      "max_output_tokens": 128000.0,
      "max_input_tokens": 272000.0,
      "knowledge_cutoff": 1717113600000.0,
      "supported_features": [
        "streaming",
        "function_calling",
        "file_search",
        "file_uploads",
        "structured_outputs",
        "image_input",
        "prompt_caching",
        "fine_tuning"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "assistants",
        "batch"
      ],
      "reasoning_tokens": true
    },
    {
      "name": "gpt-image-1",
      "slug": "gpt-image-1",
      "performance": 4,
      "latency": 1,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "image"
        ]
      },
      "supported_endpoints": [
        "image_generation",
        "image_edit"
      ],
      "supported_features": [
        "inpainting"
      ],
      "reasoning_tokens": false
    },
    {
      "name": "gpt-oss-120b",
      "slug": "gpt-oss-120b",
      "performance": 4,
      "latency": 3,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 131072,
      "max_output_tokens": 131072,
      "knowledge_cutoff": 1717200000000.0,
      "supported_features": [
        "streaming",
        "structured_outputs",
        "function_calling"
      ],
      "supported_endpoints": [
        "responses",
        "batch"
      ],
      "reasoning_tokens": true,
      "price_data": {
        "main": {
          "input": 0.0005,
          "cached_output": 0.0001,
          "output": 0.0015
        },
        "batch": {
          "input": 0.0004,
          "output": 0.0012
        }
      }
    },
    {
      "name": "gpt-oss-20b",
      "slug": "gpt-oss-20b",
      "performance": 4,
      "latency": 3,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 131072,
      "max_output_tokens": 131072,
      "knowledge_cutoff": 1717200000000.0,
      "supported_features": [
        "streaming",
        "structured_outputs",
        "function_calling"
      ],
      "supported_endpoints": [
        "responses",
        "batch"
      ],
      "reasoning_tokens": true,
      "price_data": {
        "main": {
          "input": 0.0002,
          "cached_output": 5e-05,
          "output": 0.0006
        },
        "batch": {
          "input": 0.00015,
          "output": 0.0005
        }
      }
    },
    {
      "name": "o1-2024-12-17",
      "slug": "o1-2024-12-17",
      "performance": 4,
      "latency": 1,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 200000.0,
      "max_output_tokens": 100000.0,
      "knowledge_cutoff": 1696118400000.0,
      "supported_features": [
        "streaming",
        "structured_outputs",
        "file_search",
        "function_calling",
        "file_uploads",
        "image_input"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "assistants",
        "batch"
      ],
      "reasoning_tokens": true,
      "price_data": {
        "main": {
          "input": 15,
          "cached_output": 7.5,
          "output": 60
        },
        "batch": {
          "input": 7.5,
          "output": 30
        }
      }
    },
    {
      "name": "o1-mini-2024-09-12",
      "slug": "o1-mini-2024-09-12",
      "performance": 3,
      "latency": 2,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 128000.0,
      "max_output_tokens": 65536,
      "knowledge_cutoff": 1696118400000.0,
      "supported_features": [
        "streaming",
        "file_search",
        "file_uploads"
      ],
      "supported_endpoints": [
        "chat_completions",
        "assistants"
      ],
      "reasoning_tokens": true,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "o1-preview-2024-09-12",
      "slug": "o1-preview-2024-09-12",
      "performance": 3,
      "latency": 1,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 128000.0,
      "max_output_tokens": 32768,
      "knowledge_cutoff": 1696118400000.0,
      "supported_features": [
        "streaming",
        "structured_outputs",
        "file_search",
        "function_calling",
        "file_uploads"
      ],
      "supported_endpoints": [
        "chat_completions",
        "assistants"
      ],
      "reasoning_tokens": true,
      "price_data": {
        "main": {
          "input": 10,
          "output": 30
        },
        "batch": {
          "input": 5,
          "output": 15
        }
      }
    },
    {
      "name": "o1-pro-2025-03-19",
      "slug": "o1-pro-2025-03-19",
      "performance": 4,
      "latency": 1,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 200000.0,
      "max_output_tokens": 100000.0,
      "knowledge_cutoff": 1696118400000.0,
      "supported_features": [
        "structured_outputs",
        "function_calling",
        "image_input"
      ],
      "supported_endpoints": [
        "responses",
        "batch"
      ],
      "reasoning_tokens": true,
      "price_data": {
        "main": {
          "input": 150,
          "output": 600
        },
        "batch": {
          "input": 75,
          "output": 300
        }
      }
    },
    {
      "name": "o3-2025-04-16",
      "slug": "o3-2025-04-16",
      "performance": 5,
      "latency": 1,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 200000.0,
      "max_output_tokens": 100000.0,
      "knowledge_cutoff": 1717200000000.0,
      "supported_features": [
        "streaming",
        "structured_outputs",
        "file_search",
        "function_calling",
        "file_uploads",
        "image_input",
        "prompt_caching",
        "evals",
        "stored_completions"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "batch"
      ],
      "reasoning_tokens": true,
      "price_data": {
        "main": {
          "input": 15,
          "cached_output": 7.5,
          "output": 60
        },
        "batch": {
          "input": 7.5,
          "output": 30
        }
      }
    },
    {
      "name": "o3-deep-research-2025-06-26",
      "slug": "o3-deep-research-2025-06-26",
      "performance": 5,
      "latency": 1,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 200000.0,
      "max_output_tokens": 100000.0,
      "knowledge_cutoff": 1717200000000.0,
      "supported_features": [
        "streaming",
        "file_uploads",
        "image_input",
        "prompt_caching",
        "evals",
        "stored_completions"
      ],
      "supported_endpoints": [
        "responses",
        "batch"
      ],
      "reasoning_tokens": true,
      "price_data": {
        "main": {
          "input": 15,
          "cached_output": 7.5,
          "output": 60
        },
        "batch": {
          "input": 7.5,
          "output": 30
        }
      }
    },
    {
      "name": "o3-mini-2025-01-31",
      "slug": "o3-mini-2025-01-31",
      "performance": 4,
      "latency": 3,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 200000.0,
      "max_output_tokens": 100000.0,
      "knowledge_cutoff": 1696118400000.0,
      "supported_features": [
        "streaming",
        "structured_outputs",
        "function_calling",
        "file_search",
        "file_uploads"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "assistants",
        "batch"
      ],
      "reasoning_tokens": true
    },
    {
      "name": "o3-pro-2025-06-10",
      "slug": "o3-pro-2025-06-10",
      "performance": 5,
      "latency": 1,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 200000.0,
      "max_output_tokens": 100000.0,
      "knowledge_cutoff": 1717200000000.0,
      "supported_features": [
        "structured_outputs",
        "function_calling",
        "image_input"
      ],
      "supported_endpoints": [
        "responses",
        "batch"
      ],
      "reasoning_tokens": true,
      "price_data": {
        "main": {
          "input": 20,
          "output": 80
        },
        "batch": {
          "input": 10,
          "output": 40
        }
      }
    },
    {
      "name": "o4-mini-2025-04-16",
      "slug": "o4-mini-2025-04-16",
      "performance": 4,
      "latency": 3,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 200000.0,
      "max_output_tokens": 100000.0,
      "knowledge_cutoff": 1717200000000.0,
      "supported_features": [
        "streaming",
        "structured_outputs",
        "function_calling",
        "file_search",
        "file_uploads",
        "image_input",
        "prompt_caching",
        "evals",
        "stored_completions",
        "fine_tuning"
      ],
      "supported_endpoints": [
        "chat_completions",
        "responses",
        "batch",
        "fine_tuning"
      ],
      "reasoning_tokens": true
    },
    {
      "name": "o4-mini-deep-research-2025-06-26",
      "slug": "o4-mini-deep-research-2025-06-26",
      "performance": 4,
      "latency": 3,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "context_window": 200000.0,
      "max_output_tokens": 100000.0,
      "knowledge_cutoff": 1717200000000.0,
      "supported_features": [
        "streaming",
        "file_uploads",
        "image_input",
        "prompt_caching",
        "evals",
        "stored_completions"
      ],
      "supported_endpoints": [
        "responses",
        "batch"
      ],
      "reasoning_tokens": true,
      "price_data": {
        "main": {
          "input": 8.25,
          "cached_output": 2.0625,
          "output": 33
        },
        "batch": {
          "input": 4.125,
          "output": 16.5
        }
      }
    },
    {
      "name": "omni-moderation-2024-09-26",
      "slug": "omni-moderation-2024-09-26",
      "performance": 3,
      "latency": 3,
      "modalities": {
        "input": [
          "text",
          "image"
        ],
        "output": [
          "text"
        ]
      },
      "supported_endpoints": [
        "moderation"
      ],
      "reasoning_tokens": false,
      "supported_features": [
        "image_input"
      ]
    },
    {
      "name": "text-embedding-3-large",
      "slug": "text-embedding-3-large",
      "performance": 3,
      "latency": 2,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "supported_endpoints": [
        "embeddings",
        "batch"
      ]
    },
    {
      "name": "text-embedding-3-small",
      "slug": "text-embedding-3-small",
      "performance": 2,
      "latency": 3,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "supported_endpoints": [
        "embeddings",
        "batch"
      ]
    },
    {
      "name": "text-embedding-ada-002",
      "slug": "text-embedding-ada-002",
      "performance": 1,
      "latency": 2,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "supported_endpoints": [
        "embeddings",
        "batch"
      ]
    },
    {
      "name": "text-moderation-007",
      "slug": "text-moderation-007",
      "performance": 2,
      "latency": 3,
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "text"
        ]
      },
      "max_output_tokens": 32768,
      "knowledge_cutoff": 1630454400000.0,
      "supported_endpoints": [
        "moderation"
      ],
      "reasoning_tokens": false
    },
    {
      "name": "tts-1-hd",
      "slug": "tts-1-hd",
      "performance": 3,
      "latency": 3,
      "current_snapshot": "tts-1-hd",
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "audio"
        ]
      },
      "supported_endpoints": [
        "speech_generation"
      ],
      "reasoning_tokens": false
    },
    {
      "name": "tts-1",
      "slug": "tts-1",
      "performance": 2,
      "latency": 4,
      "current_snapshot": "tts-1",
      "modalities": {
        "input": [
          "text"
        ],
        "output": [
          "audio"
        ]
      },
      "supported_endpoints": [
        "speech_generation"
      ],
      "reasoning_tokens": false
    },
    {
      "name": "whisper-1",
      "slug": "whisper-1",
      "performance": 2,
      "latency": 3,
      "modalities": {
        "input": [
          "audio"
        ],
        "output": [
          "text"
        ]
      },
      "supported_endpoints": [
        "transcription",
        "translation"
      ],
      "reasoning_tokens": false
    }
  ],
  "pricing": [
    {
      "name": "gpt-4.1",
      "current_snapshot": "gpt-4.1-2025-04-14",
      "description": "Flagship GPT model for complex tasks",
      "values": {
        "main": {
          "input": 2,
          "cached_input": 0.5,
          "output": 8
        },
        "batch": {
          "input": 1,
          "output": 4
        }
      },
      "snapshots": [
        {
          "name": "gpt-4.1-2025-04-14",
          "values": {
            "main": {
              "input": 2,
              "cached_input": 0.5,
              "output": 8
            },
            "batch": {
              "input": 1,
              "output": 4
            }
          }
        }
      ]
    },
    {
      "name": "gpt-4.1-mini",
      "current_snapshot": "gpt-4.1-mini-2025-04-14",
      "description": "Balanced for intelligence, speed, and cost",
      "values": {
        "main": {
          "input": 0.4,
          "cached_input": 0.1,
          "output": 1.6
        },
        "batch": {
          "input": 0.2,
          "output": 0.8
        }
      },
      "snapshots": [
        {
          "name": "gpt-4.1-mini-2025-04-14",
          "values": {
            "main": {
              "input": 0.4,
              "cached_input": 0.1,
              "output": 1.6
            },
            "batch": {
              "input": 0.2,
              "output": 0.8
            }
          }
        }
      ]
    },
    {
      "name": "gpt-4.1-nano",
      "current_snapshot": "gpt-4.1-nano-2025-04-14",
      "description": "Fastest, most cost-effective GPT-4.1 model",
      "values": {
        "main": {
          "input": 0.1,
          "cached_input": 0.025,
          "output": 0.4
        },
        "batch": {
          "input": 0.05,
          "output": 0.2
        }
      },
      "snapshots": [
        {
          "name": "gpt-4.1-nano-2025-04-14",
          "values": {
            "main": {
              "input": 0.1,
              "cached_input": 0.025,
              "output": 0.4
            },
            "batch": {
              "input": 0.05,
              "output": 0.2
            }
          }
        }
      ]
    },
    {
      "name": "gpt-4.5-preview",
      "current_snapshot": "gpt-4.5-preview-2025-02-27",
      "description": "Our most generally capable model, excelling at creative tasks",
      "values": {
        "main": {
          "input": 75,
          "cached_input": 37.5,
          "output": 150
        },
        "batch": {
          "input": 37.5,
          "output": 75
        }
      },
      "snapshots": [
        {
          "name": "gpt-4.5-preview-2025-02-27",
          "values": {
            "main": {
              "input": 75,
              "cached_input": 37.5,
              "output": 150
            },
            "batch": {
              "input": 37.5,
              "output": 75
            }
          }
        }
      ]
    },
    {
      "name": "gpt-4o",
      "current_snapshot": "gpt-4o-2024-08-06",
      "description": "High-intelligence model for complex tasks",
      "values": {
        "main": {
          "input": 2.5,
          "cached_input": 1.25,
          "output": 10
        },
        "batch": {
          "input": 1.25,
          "output": 5
        }
      },
      "snapshots": [
        {
          "name": "gpt-4o-2024-11-20",
          "values": {
            "main": {
              "input": 2.5,
              "cached_input": 1.25,
              "output": 10
            },
            "batch": {
              "input": 1.25,
              "output": 5
            }
          }
        },
        {
          "name": "gpt-4o-2024-08-06",
          "values": {
            "main": {
              "input": 2.5,
              "cached_input": 1.25,
              "output": 10
            },
            "batch": {
              "input": 1.25,
              "output": 5
            }
          }
        },
        {
          "name": "gpt-4o-2024-05-13",
          "values": {
            "main": {
              "input": 5,
              "output": 15
            },
            "batch": {
              "input": 2.5,
              "output": 7.5
            }
          }
        }
      ]
    },
    {
      "name": "gpt-4o-audio-preview",
      "current_snapshot": "gpt-4o-audio-preview-2025-06-03",
      "values": {
        "main": {
          "input": 2.5,
          "output": 10
        }
      },
      "snapshots": [
        {
          "name": "gpt-4o-audio-preview-2025-06-03",
          "values": {
            "main": {
              "input": 2.5,
              "output": 10
            }
          }
        },
        {
          "name": "gpt-4o-audio-preview-2024-12-17",
          "values": {
            "main": {
              "input": 2.5,
              "output": 10
            }
          }
        },
        {
          "name": "gpt-4o-audio-preview-2024-10-01",
          "values": {
            "main": {
              "input": 2.5,
              "output": 10
            }
          }
        }
      ]
    },
    {
      "name": "gpt-4o-realtime-preview",
      "current_snapshot": "gpt-4o-realtime-preview-2025-06-03",
      "values": {
        "main": {
          "input": 5,
          "cached_input": 2.5,
          "output": 20
        }
      },
      "snapshots": [
        {
          "name": "gpt-4o-realtime-preview-2025-06-03",
          "values": {
            "main": {
              "input": 5,
              "cached_input": 2.5,
              "output": 20
            }
          }
        },
        {
          "name": "gpt-4o-realtime-preview-2024-12-17",
          "values": {
            "main": {
              "input": 5,
              "cached_input": 2.5,
              "output": 20
            }
          }
        },
        {
          "name": "gpt-4o-realtime-preview-2024-10-01",
          "values": {
            "main": {
              "input": 5,
              "cached_input": 2.5,
              "output": 20
            }
          }
        }
      ]
    },
    {
      "name": "gpt-4o-mini",
      "current_snapshot": "gpt-4o-mini-2024-07-18",
      "description": "Fast and affordable model for simple tasks",
      "values": {
        "main": {
          "input": 0.15,
          "cached_input": 0.075,
          "output": 0.6
        },
        "batch": {
          "input": 0.075,
          "output": 0.3
        }
      },
      "snapshots": [
        {
          "name": "gpt-4o-mini-2024-07-18",
          "values": {
            "main": {
              "input": 0.15,
              "cached_input": 0.075,
              "output": 0.6
            },
            "batch": {
              "input": 0.075,
              "output": 0.3
            }
          }
        }
      ]
    },
    {
      "name": "gpt-4o-mini-audio-preview",
      "current_snapshot": "gpt-4o-mini-audio-preview-2024-12-17",
      "values": {
        "main": {
          "input": 0.15,
          "output": 0.6
        }
      },
      "snapshots": [
        {
          "name": "gpt-4o-mini-audio-preview-2024-12-17",
          "values": {
            "main": {
              "input": 0.15,
              "output": 0.6
            }
          }
        }
      ]
    },
    {
      "name": "gpt-4o-mini-realtime-preview",
      "current_snapshot": "gpt-4o-mini-realtime-preview-2024-12-17",
      "values": {
        "main": {
          "input": 0.6,
          "cached_input": 0.3,
          "output": 2.4
        }
      },
      "snapshots": [
        {
          "name": "gpt-4o-mini-realtime-preview-2024-12-17",
          "values": {
            "main": {
              "input": 0.6,
              "cached_input": 0.3,
              "output": 2.4
            }
          }
        }
      ]
    },
    {
      "name": "o1",
      "current_snapshot": "o1-2024-12-17",
      "description": "Advanced reasoning model",
      "values": {
        "main": {
          "input": 15,
          "cached_input": 7.5,
          "output": 60
        },
        "batch": {
          "input": 7.5,
          "output": 30
        }
      },
      "snapshots": [
        {
          "name": "o1-2024-12-17",
          "values": {
            "main": {
              "input": 15,
              "cached_input": 7.5,
              "output": 60
            },
            "batch": {
              "input": 7.5,
              "output": 30
            }
          }
        },
        {
          "name": "o1-preview-2024-09-12",
          "values": {
            "main": {
              "input": 15,
              "cached_input": 7.5,
              "output": 60
            },
            "batch": {
              "input": 7.5,
              "output": 30
            }
          }
        }
      ]
    },
    {
      "name": "o1-pro",
      "current_snapshot": "o1-pro-2025-03-19",
      "description": "Advanced reasoning model",
      "values": {
        "main": {
          "input": 150,
          "output": 600
        },
        "batch": {
          "input": 75,
          "output": 300
        }
      },
      "snapshots": [
        {
          "name": "o1-pro-2025-03-19",
          "values": {
            "main": {
              "input": 150,
              "output": 600
            },
            "batch": {
              "input": 75,
              "output": 300
            }
          }
        }
      ]
    },
    {
      "name": "o3-pro",
      "current_snapshot": "o3-pro-2025-06-10",
      "description": "Advanced reasoning model",
      "values": {
        "main": {
          "input": 20,
          "output": 80
        },
        "batch": {
          "input": 10,
          "output": 40
        }
      },
      "snapshots": [
        {
          "name": "o3-pro-2025-06-10",
          "values": {
            "main": {
              "input": 20,
              "output": 80
            },
            "batch": {
              "input": 10,
              "output": 40
            }
          }
        }
      ]
    },
    {
      "name": "o3",
      "current_snapshot": "o3-2025-04-16",
      "description": "Our most powerful reasoning model",
      "values": {
        "main": {
          "input": 2,
          "cached_input": 0.5,
          "output": 8
        },
        "batch": {
          "input": 1,
          "output": 4
        }
      },
      "snapshots": [
        {
          "name": "o3-2025-04-16",
          "values": {
            "main": {
              "input": 2,
              "cached_input": 0.5,
              "output": 8
            },
            "batch": {
              "input": 1,
              "output": 4
            }
          }
        }
      ]
    },
    {
      "name": "o3-deep-research",
      "current_snapshot": "o3-deep-research-2025-06-26",
      "description": "Our most powerful reasoning model",
      "values": {
        "main": {
          "input": 10,
          "cached_input": 2.5,
          "output": 40
        },
        "batch": {
          "input": 5,
          "output": 20
        }
      },
      "snapshots": [
        {
          "name": "o3-deep-research-2025-06-26",
          "values": {
            "main": {
              "input": 10,
              "cached_input": 2.5,
              "output": 40
            },
            "batch": {
              "input": 5,
              "output": 20
            }
          }
        }
      ]
    },
    {
      "name": "o4-mini",
      "current_snapshot": "o4-mini-2025-04-16",
      "description": "Small reasoning model for math, science, coding, image parsing, and writing",
      "values": {
        "main": {
          "input": 1.1,
          "cached_input": 0.275,
          "output": 4.4
        },
        "batch": {
          "input": 0.55,
          "output": 2.2
        }
      },
      "snapshots": [
        {
          "name": "o4-mini-2025-04-16",
          "values": {
            "main": {
              "input": 1.1,
              "cached_input": 0.275,
              "output": 4.4
            },
            "batch": {
              "input": 0.55,
              "output": 2.2
            }
          }
        }
      ]
    },
    {
      "name": "o4-mini-deep-research",
      "current_snapshot": "o4-mini-deep-research-2025-06-26",
      "description": "Our small, fast reasoning model optimized for deep analysis",
      "values": {
        "main": {
          "input": 2,
          "cached_input": 0.5,
          "output": 8
        },
        "batch": {
          "input": 1,
          "output": 4
        }
      },
      "snapshots": [
        {
          "name": "o4-mini-deep-research-2025-06-26",
          "values": {
            "main": {
              "input": 2,
              "cached_input": 0.5,
              "output": 8
            },
            "batch": {
              "input": 1,
              "output": 4
            }
          }
        }
      ]
    },
    {
      "name": "o3-mini",
      "current_snapshot": "o3-mini-2025-01-31",
      "description": "Small reasoning model for math, science and coding",
      "values": {
        "main": {
          "input": 1.1,
          "cached_input": 0.55,
          "output": 4.4
        },
        "batch": {
          "input": 0.55,
          "output": 2.2
        }
      },
      "snapshots": [
        {
          "name": "o3-mini-2025-01-31",
          "values": {
            "main": {
              "input": 1.1,
              "cached_input": 0.55,
              "output": 4.4
            },
            "batch": {
              "input": 0.55,
              "output": 2.2
            }
          }
        }
      ]
    },
    {
      "name": "o1-mini",
      "current_snapshot": "o1-mini-2024-09-12",
      "description": "Small reasoning model for math, science and coding",
      "values": {
        "main": {
          "input": 1.1,
          "cached_input": 0.55,
          "output": 4.4
        },
        "batch": {
          "input": 0.55,
          "output": 2.2
        }
      },
      "snapshots": [
        {
          "name": "o1-mini-2024-09-12",
          "values": {
            "main": {
              "input": 1.1,
              "cached_input": 0.55,
              "output": 4.4
            },
            "batch": {
              "input": 0.55,
              "output": 2.2
            }
          }
        }
      ]
    },
    {
      "name": "codex-mini-latest",
      "current_snapshot": "codex-mini-latest",
      "description": "Customized version of o4-mini for the Codex CLI",
      "values": {
        "main": {
          "input": 1.5,
          "cached_input": 0.375,
          "output": 6
        }
      },
      "snapshots": [
        {
          "name": "codex-mini-latest",
          "values": {
            "main": {
              "input": 1.5,
              "cached_input": 0.375,
              "output": 6
            }
          }
        }
      ]
    },
    {
      "name": "gpt-4o-mini-search-preview",
      "current_snapshot": "gpt-4o-mini-search-preview-2025-03-11",
      "description": "Specialized model for search",
      "values": {
        "main": {
          "input": 0.15,
          "output": 0.6
        }
      },
      "snapshots": [
        {
          "name": "gpt-4o-mini-search-preview-2025-03-11",
          "values": {
            "main": {
              "input": 0.15,
              "output": 0.6
            }
          }
        }
      ]
    },
    {
      "name": "gpt-4o-search-preview",
      "current_snapshot": "gpt-4o-search-preview-2025-03-11",
      "description": "Specialized model for search",
      "values": {
        "main": {
          "input": 2.5,
          "output": 10
        }
      },
      "snapshots": [
        {
          "name": "gpt-4o-search-preview-2025-03-11",
          "values": {
            "main": {
              "input": 2.5,
              "output": 10
            }
          }
        }
      ]
    },
    {
      "name": "computer-use-preview",
      "current_snapshot": "computer-use-preview-2025-03-11",
      "description": "Specialized model for computer use",
      "values": {
        "main": {
          "input": 3,
          "output": 12
        },
        "batch": {
          "input": 1.5,
          "output": 6
        }
      },
      "snapshots": [
        {
          "name": "computer-use-preview-2025-03-11",
          "values": {
            "main": {
              "input": 3,
              "output": 12
            },
            "batch": {
              "input": 1.5,
              "output": 6
            }
          }
        }
      ]
    },
    {
      "name": "gpt-image-1",
      "current_snapshot": "gpt-image-1",
      "description": "Image generation model",
      "values": {
        "main": {
          "input": 5,
          "cached_input": 1.25
        }
      }
    },
    {
      "name": "gpt-5",
      "current_snapshot": "gpt-5-2025-08-07",
      "description": "Flagship GPT-5 model for complex tasks",
      "values": {
        "main": {
          "input": 1.25,
          "cached_input": 0.125,
          "output": 10
        },
        "batch": {
          "input": 0.625,
          "cached_input": 0.0625,
          "output": 5
        },
        "flex": {
          "input": 0.625,
          "cached_input": 0.0625,
          "output": 5
        },
        "priority": {
          "input": 2.5,
          "cached_input": 0.25,
          "output": 20
        }
      },
      "snapshots": [
        {
          "name": "gpt-5-2025-08-07",
          "values": {
            "main": {
              "input": 1.25,
              "cached_input": 0.125,
              "output": 10
            },
            "batch": {
              "input": 0.625,
              "cached_input": 0.0625,
              "output": 5
            },
            "flex": {
              "input": 0.625,
              "cached_input": 0.0625,
              "output": 5
            },
            "priority": {
              "input": 2.5,
              "cached_input": 0.25,
              "output": 20
            }
          }
        }
      ]
    },
    {
      "name": "gpt-5-latest",
      "current_snapshot": "gpt-5-chat-latest",
      "description": "Latest GPT-5 chat model",
      "values": {
        "main": {
          "input": 1.25,
          "cached_input": 0.125,
          "output": 10
        }
      }
    },
    {
      "name": "gpt-5-mini",
      "current_snapshot": "gpt-5-mini-2025-08-07",
      "description": "Smaller, cost-effective GPT-5 model",
      "values": {
        "main": {
          "input": 0.25,
          "cached_input": 0.025,
          "output": 2
        },
        "batch": {
          "input": 0.125,
          "cached_input": 0.0125,
          "output": 1
        },
        "flex": {
          "input": 0.125,
          "cached_input": 0.0125,
          "output": 1
        },
        "priority": {
          "input": 0.45,
          "cached_input": 0.05,
          "output": 3.6
        }
      },
      "snapshots": [
        {
          "name": "gpt-5-mini-2025-08-07",
          "values": {
            "main": {
              "input": 0.25,
              "cached_input": 0.025,
              "output": 2
            },
            "batch": {
              "input": 0.125,
              "cached_input": 0.0125,
              "output": 1
            },
            "flex": {
              "input": 0.125,
              "cached_input": 0.0125,
              "output": 1
            },
            "priority": {
              "input": 0.45,
              "cached_input": 0.05,
              "output": 3.6
            }
          }
        }
      ]
    },
    {
      "name": "gpt-5-nano",
      "current_snapshot": "gpt-5-nano-2025-08-07",
      "description": "Fastest, most cost-effective GPT-5 model",
      "values": {
        "main": {
          "input": 0.05,
          "cached_input": 0.005,
          "output": 0.4
        },
        "batch": {
          "input": 0.025,
          "cached_input": 0.0025,
          "output": 0.2
        },
        "flex": {
          "input": 0.025,
          "cached_input": 0.0025,
          "output": 0.2
        }
      },
      "snapshots": [
        {
          "name": "gpt-5-nano-2025-08-07",
          "values": {
            "main": {
              "input": 0.05,
              "cached_input": 0.005,
              "output": 0.4
            },
            "batch": {
              "input": 0.025,
              "cached_input": 0.0025,
              "output": 0.2
            },
            "flex": {
              "input": 0.025,
              "cached_input": 0.0025,
              "output": 0.2
            }
          }
        }
      ]
    }
  ]
}