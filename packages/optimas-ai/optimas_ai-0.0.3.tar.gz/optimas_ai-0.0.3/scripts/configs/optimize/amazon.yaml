# Dataset and pipeline
dataset: amazon
system: amazon_system
wandb_run_name: iter100-valsize20-trainsize20
val_size: 20
per_iteration_new_data_size: 10
per_component_search_size: 10
per_component_train_size: 20
per_iteration_rm_train_size: 128

# Paths
output_dir: outputs/optim
state_dict_path: outputs/reward_model/amazon_system/full-best/state_dict.pth # an example path
preference_dataset: snap-stanford/amazon_system

# LoRA
lora_r: 32
lora_alpha: 16
lora_dropout: 0.0
add_margin: true

# Iteration & optimization
num_iterations: 100
num_prompt_candidates: 3

# Replay buffer
use_replay_buffer: true
replay_buffer_size: 100
max_sample_workers: 4
num_repeat: 1

# PPO
weight_optimizer: ppo
ppo_epochs: 3
ppo_batch_size: 8
ppo_mini_batch_size: 4
ppo_gradient_accumulation_steps: 2
ppo_learning_rate: 0.00001
ppo_save_steps: 1
ppo_base_model_name: Qwen/Qwen2.5-1.5B-Instruct

# Environment
dotenv_path: .env

# Reward model
do_train: true
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 0.000001
num_train_epochs: 1
max_length: 2048
logging_steps: 10
eval_steps: 8
save_steps: 50
eval_strategy: "steps"
metric_for_best_model: eval_loss
load_best_model_at_end: false
use_score_scaling: false
use_score_norm: false
ddp_find_unused_parameters: false
