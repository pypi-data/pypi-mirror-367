import litellm
import json

def get_llm_output(message, 
                   model="gpt-4o", 
                   max_new_tokens=4096, 
                   temperature=1, 
                   json_object=False,
                   system_prompt=None,
                   **generation_kwargs):
    """
    Use litellm to complete a prompt using the specified model.

    Args:
        message (str or list): The input message or a list of message dicts.
        model (str): The model to use for completion.
        max_new_tokens (int): Maximum number of tokens to generate.
        temperature (float): Sampling temperature.
        json_object (bool): Whether to output in JSON format.
        system_prompt (str): Optional system prompt.

    Returns:
        str: The completed text generated by the model.
    """
    # Convert message to litellm-compatible format if needed
    if isinstance(message, str):
        messages = [{"role": "user", "content": message}]
        if system_prompt:
            messages.insert(0, {"role": "system", "content": system_prompt})
    else:
        messages = message

    kwargs = {
        "model": model,
        "messages": messages,
        "max_tokens": max_new_tokens,
        "temperature": temperature,
    }
    kwargs.update(generation_kwargs)

    if json_object:
        kwargs["response_format"] = {"type": "json_object"}

    # Call litellm
    response = litellm.completion(**kwargs)

    # litellm returns an object similar to OpenAI
    content = response.choices[0].message.content
    if json_object:
        return json.loads(content)
    else:
        return content
