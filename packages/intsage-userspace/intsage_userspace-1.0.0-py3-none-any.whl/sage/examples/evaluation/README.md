# è¯„ä¼°ç¤ºä¾‹ (Evaluation Examples)

è¿™ä¸ªç›®å½•åŒ…å«æ¨¡å‹å’Œç³»ç»Ÿæ€§èƒ½è¯„ä¼°çš„å„ç§æ–¹æ³•ç¤ºä¾‹ï¼Œå¸®åŠ©ç”¨æˆ·è¯„ä¼°RAGç³»ç»Ÿã€é—®ç­”æ¨¡å‹å’Œå…¶ä»–AIç»„ä»¶çš„æ•ˆæœã€‚

## ğŸ“ æ–‡ä»¶åˆ—è¡¨

### `qa_evaluate.py`
é—®ç­”ç³»ç»Ÿç»¼åˆè¯„ä¼°ç¤ºä¾‹ï¼ŒåŒ…å«ï¼š
- **å‡†ç¡®æ€§è¯„ä¼°** - F1-Score, ç²¾ç¡®ç‡, å¬å›ç‡
- **è¯­ä¹‰è¯„ä¼°** - BERTè¯„åˆ†, è¯­ä¹‰ç›¸ä¼¼åº¦
- **ç”Ÿæˆè´¨é‡** - ROUGE, BLEUè¯„åˆ†
- **æ•ˆç‡è¯„ä¼°** - å“åº”æ—¶é—´, ååé‡
- **æ£€ç´¢è´¨é‡** - æ£€ç´¢å‡†ç¡®ç‡, ä¸Šä¸‹æ–‡ç›¸å…³æ€§

#### è¯„ä¼°ç»´åº¦ï¼š
1. **Answer Quality** - ç­”æ¡ˆè´¨é‡è¯„ä¼°
2. **Retrieval Effectiveness** - æ£€ç´¢æ•ˆæœè¯„ä¼°
3. **System Performance** - ç³»ç»Ÿæ€§èƒ½è¯„ä¼°
4. **Cost Analysis** - æˆæœ¬åˆ†æ

## ğŸ” è¯„ä¼°æŒ‡æ ‡

### æ–‡æœ¬ç›¸ä¼¼åº¦æŒ‡æ ‡
- **F1 Score** - ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡
- **ROUGE-L** - æœ€é•¿å…¬å…±å­åºåˆ—è¯„åˆ†
- **BERT Score** - åŸºäºBERTçš„è¯­ä¹‰ç›¸ä¼¼åº¦

### æ£€ç´¢ç›¸å…³æŒ‡æ ‡
- **Context Recall** - ä¸Šä¸‹æ–‡å¬å›ç‡
- **Compression Rate** - ä¿¡æ¯å‹ç¼©ç‡
- **Retrieval Precision** - æ£€ç´¢ç²¾ç¡®åº¦

### ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
- **Latency** - å“åº”å»¶è¿Ÿ
- **Throughput** - å¤„ç†ååé‡
- **Token Count** - ç”Ÿæˆçš„tokenæ•°é‡

## ğŸš€ è¿è¡Œæ–¹å¼

### åŸºç¡€è¯„ä¼°
```bash
python qa_evaluate.py
```

### è‡ªå®šä¹‰è¯„ä¼°
```python
from sage.lib.rag.evaluate import F1Evaluate, BertRecallEvaluate

# é…ç½®è¯„ä¼°å™¨
evaluators = [
    F1Evaluate(config["f1_evaluate"]),
    BertRecallEvaluate(config["bert_evaluate"])
]

# è¿è¡Œè¯„ä¼°
for evaluator in evaluators:
    results = evaluator.evaluate(predictions, ground_truth)
    print(f"{evaluator.__class__.__name__}: {results}")
```

## âš™ï¸ é…ç½®æ–‡ä»¶

ä½¿ç”¨ `../../config/config_evaluate.yaml` é…ç½®ï¼š

```yaml
evaluation:
  metrics:
    - f1_score
    - rouge_l
    - bert_score
    - context_recall
  
  datasets:
    test_set: "data/test_qa.json"
    ground_truth: "data/ground_truth.json"
  
  output:
    report_path: "evaluation_report.json"
    detailed_results: true
```

## ğŸ“Š è¯„ä¼°æŠ¥å‘Š

è¯„ä¼°å®Œæˆåä¼šç”Ÿæˆè¯¦ç»†æŠ¥å‘Šï¼š

```json
{
  "overall_scores": {
    "f1_score": 0.85,
    "rouge_l": 0.78,
    "bert_score": 0.82
  },
  "detailed_results": [
    {
      "question_id": 1,
      "question": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
      "predicted_answer": "...",
      "ground_truth": "...",
      "scores": {
        "f1": 0.9,
        "rouge_l": 0.85
      }
    }
  ],
  "performance_metrics": {
    "avg_latency": 1.2,
    "throughput": 50
  }
}
```

## ğŸ¯ è¯„ä¼°æœ€ä½³å®è·µ

### æ•°æ®é›†å‡†å¤‡
1. **è´¨é‡æ§åˆ¶** - ç¡®ä¿æ ‡æ³¨æ•°æ®çš„å‡†ç¡®æ€§
2. **å¤šæ ·æ€§** - è¦†ç›–ä¸åŒç±»å‹çš„é—®é¢˜
3. **è§„æ¨¡é€‚å½“** - è¶³å¤Ÿçš„è¯„ä¼°æ ·æœ¬

### è¯„ä¼°ç­–ç•¥
1. **å¤šç»´åº¦è¯„ä¼°** - ç»“åˆå¤šç§è¯„ä¼°æŒ‡æ ‡
2. **A/Bæµ‹è¯•** - æ¯”è¾ƒä¸åŒæ¨¡å‹ç‰ˆæœ¬
3. **æŒç»­è¯„ä¼°** - å®šæœŸè¯„ä¼°ç³»ç»Ÿæ€§èƒ½

### ç»“æœåˆ†æ
1. **é”™è¯¯åˆ†æ** - åˆ†æå¤±è´¥æ¡ˆä¾‹
2. **æ€§èƒ½ç“¶é¢ˆ** - è¯†åˆ«ç³»ç»Ÿç“¶é¢ˆ
3. **æ”¹è¿›å»ºè®®** - æå‡ºä¼˜åŒ–æ–¹æ¡ˆ

## ğŸ”§ æ‰©å±•è¯„ä¼°

### æ·»åŠ è‡ªå®šä¹‰è¯„ä¼°å™¨
```python
from sage.lib.rag.evaluate import BaseEvaluate

class CustomEvaluate(BaseEvaluate):
    def evaluate(self, predicted, ground_truth):
        # å®ç°è‡ªå®šä¹‰è¯„ä¼°é€»è¾‘
        score = calculate_custom_score(predicted, ground_truth)
        return {"custom_score": score}
```

### æ‰¹é‡è¯„ä¼°
```python
# æ‰¹é‡è¯„ä¼°å¤šä¸ªæ¨¡å‹
models = ["model_v1", "model_v2", "model_v3"]
results = {}

for model in models:
    results[model] = run_evaluation(model, test_data)
    
compare_models(results)
```

## ğŸ”— ç›¸å…³èµ„æº

- [è¯„ä¼°æ¨¡å—æ–‡æ¡£](../../packages/sage-userspace/src/sage/lib/rag/evaluate.py)
- [åŸºå‡†æµ‹è¯•æ•°æ®é›†](../../data/sample/)
- [è¯„ä¼°å·¥å…·é›†](../../packages/sage-userspace/src/sage/lib/rag/)
