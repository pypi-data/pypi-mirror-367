# Agent Zero Lite Configuration
# Copy this file to .env and configure your settings

# ================================================================================
# API KEYS - Configure at least one LLM provider
# ================================================================================

# OpenAI (GPT models)
OPENAI_API_KEY=sk-...

# Anthropic (Claude models)  
ANTHROPIC_API_KEY=sk-ant-...

# Google (Gemini models)
GOOGLE_API_KEY=...

# Azure OpenAI
AZURE_API_KEY=...
AZURE_API_BASE=https://your-resource.openai.azure.com/
AZURE_API_VERSION=2024-02-15-preview

# Groq (Fast inference)
GROQ_API_KEY=gsk_...

# Together AI
TOGETHER_API_KEY=...

# Cohere
COHERE_API_KEY=...

# Mistral AI
MISTRAL_API_KEY=...

# Perplexity
PERPLEXITYAI_API_KEY=pplx-...

# Hugging Face
HUGGINGFACE_API_KEY=hf_...

# Ollama (Local models)
OLLAMA_API_BASE=http://localhost:11434

# ================================================================================
# MODEL CONFIGURATION
# ================================================================================

# Chat Model (primary model for conversations)
CHAT_MODEL_PROVIDER=openai
CHAT_MODEL_NAME=gpt-4o-mini
CHAT_MODEL_API_BASE=
CHAT_MODEL_CTX_LENGTH=128000
CHAT_MODEL_VISION=true
CHAT_MODEL_RL_REQUESTS=60
CHAT_MODEL_RL_INPUT=100000
CHAT_MODEL_RL_OUTPUT=10000
CHAT_MODEL_KWARGS={}

# Utility Model (for internal operations)
UTIL_MODEL_PROVIDER=openai
UTIL_MODEL_NAME=gpt-4o-mini
UTIL_MODEL_API_BASE=
UTIL_MODEL_CTX_LENGTH=128000
UTIL_MODEL_CTX_INPUT=0.5
UTIL_MODEL_RL_REQUESTS=60
UTIL_MODEL_RL_INPUT=100000
UTIL_MODEL_RL_OUTPUT=10000
UTIL_MODEL_KWARGS={}

# Browser Model (for API compatibility)
BROWSER_MODEL_PROVIDER=openai
BROWSER_MODEL_NAME=gpt-4o-mini
BROWSER_MODEL_API_BASE=
BROWSER_MODEL_CTX_LENGTH=128000
BROWSER_MODEL_VISION=true
BROWSER_MODEL_RL_REQUESTS=60
BROWSER_MODEL_RL_INPUT=100000
BROWSER_MODEL_RL_OUTPUT=10000
BROWSER_MODEL_KWARGS={}

# Embedding Model (for vector operations)
EMBED_MODEL_PROVIDER=openai
EMBED_MODEL_NAME=text-embedding-3-small
EMBED_MODEL_API_BASE=
EMBED_MODEL_RL_REQUESTS=1000
EMBED_MODEL_RL_INPUT=1000000
EMBED_MODEL_KWARGS={}

# ================================================================================
# WEB UI CONFIGURATION
# ================================================================================

# Optional API key for Web UI access (leave empty for no authentication)
API_KEY=

# Flask secret key (auto-generated if not set)
FLASK_SECRET_KEY=

# Server port
PORT=50001

# ================================================================================
# MEMORY & KNOWLEDGE
# ================================================================================

# Memory subdirectory (optional)
MEMORY_SUBDIR=

# Knowledge subdirectories
KNOWLEDGE_SUBDIRS=["default", "custom"]

# ================================================================================
# MCP (Model Context Protocol) SERVERS
# ================================================================================

# MCP servers configuration (JSON format)
# Example with filesystem and git MCP servers:
MCP_SERVERS={}

# Example MCP configuration:
# MCP_SERVERS={"filesystem": {"command": "npx", "args": ["-y", "@modelcontextprotocol/server-filesystem", "/path/to/allowed/files"]}, "git": {"command": "npx", "args": ["-y", "@modelcontextprotocol/server-git", "--repository", "/path/to/git/repo"]}}

# ================================================================================
# EXAMPLE CONFIGURATIONS FOR DIFFERENT PROVIDERS
# ================================================================================

# --- OpenAI GPT-4 ---
# CHAT_MODEL_PROVIDER=openai
# CHAT_MODEL_NAME=gpt-4o

# --- Anthropic Claude ---
# CHAT_MODEL_PROVIDER=anthropic  
# CHAT_MODEL_NAME=claude-3-5-sonnet-20241022

# --- Google Gemini ---
# CHAT_MODEL_PROVIDER=gemini
# CHAT_MODEL_NAME=gemini-1.5-pro

# --- Azure OpenAI ---
# CHAT_MODEL_PROVIDER=azure
# CHAT_MODEL_NAME=gpt-35-turbo
# CHAT_MODEL_API_BASE=https://your-resource.openai.azure.com/

# --- Groq (Fast) ---
# CHAT_MODEL_PROVIDER=groq
# CHAT_MODEL_NAME=llama-3.1-70b-versatile

# --- Local Ollama ---
# CHAT_MODEL_PROVIDER=ollama
# CHAT_MODEL_NAME=llama3.1:8b
# CHAT_MODEL_API_BASE=http://localhost:11434

# --- Mistral ---
# CHAT_MODEL_PROVIDER=mistral
# CHAT_MODEL_NAME=mistral-large-latest

# --- Together AI ---
# CHAT_MODEL_PROVIDER=together_ai
# CHAT_MODEL_NAME=meta-llama/Llama-3.1-70B-Instruct-Turbo

# ================================================================================
# ADVANCED CONFIGURATION
# ================================================================================

# Disable LiteLLM logging (recommended)
LITELLM_LOG=ERROR

# Timezone
TZ=UTC

# Disable tokenizer parallelism warnings
TOKENIZERS_PARALLELISM=false