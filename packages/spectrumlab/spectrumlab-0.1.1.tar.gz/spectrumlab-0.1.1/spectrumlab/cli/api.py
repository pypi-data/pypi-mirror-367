from typing import List, Dict, Optional, Any
from ..benchmark import get_benchmark_group
from ..evaluator import get_evaluator


def run_evaluation(
    model,
    level: str,
    subcategories: Optional[List[str]] = None,
    data_path: str = "./data",
    save_path: str = "./results",
    max_out_len: int = 512,
) -> Dict[str, Any]:
    print("ğŸš€ Starting evaluation")
    print(f"ğŸ“Š Model: {model.__class__.__name__}")
    print(f"ğŸ“ Level: {level}")
    print(f"ğŸ“‚ Data path: {data_path}")
    print(f"ğŸ’¾ Save path: {save_path}")

    print("\nğŸ“¥ Loading benchmark data...")
    benchmark = get_benchmark_group(level, data_path)

    if subcategories:
        data = benchmark.get_data_by_subcategories(subcategories)
        print(f"ğŸ“‹ Subcategories: {subcategories}")
    else:
        data = benchmark.get_data_by_subcategories("all")
        print("ğŸ“‹ Subcategories: all")

    print(f"ğŸ“Š Total data items: {len(data)}")

    print("\nâš™ï¸ Getting evaluator...")
    evaluator = get_evaluator(level)

    print("\nğŸ”„ Running evaluation...")
    results = evaluator.evaluate(
        data_items=data,
        model=model,
        max_out_len=max_out_len,
        save_path=save_path,
    )

    return results
