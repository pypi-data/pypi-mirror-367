import gradio as gr
import pandas as pd
import json
from pathlib import Path
from typing import Dict, Optional


class SpectralLeaderboard:
    def __init__(self, data_file: str = "../leaderboard_v_1.0.json"):
        # è·å–å½“å‰è„šæœ¬çš„ç›®å½•
        current_dir = Path(__file__).parent
        # æ„å»ºæ­£ç¡®çš„æ•°æ®æ–‡ä»¶è·¯å¾„
        if data_file.startswith("../"):
            self.data_file = current_dir.parent / data_file[3:]
        else:
            self.data_file = Path(data_file)

        print(f"ğŸ” Looking for data file at: {self.data_file}")
        print(f"ğŸ“‚ Current working directory: {Path.cwd()}")
        print(f"ğŸ“„ Script location: {Path(__file__).parent}")
        print(f"âœ… Data file exists: {self.data_file.exists()}")

        self.data = self._load_data()

    def _load_data(self) -> Dict:
        """åŠ è½½æ’è¡Œæ¦œæ•°æ®"""
        try:
            with open(self.data_file, "r", encoding="utf-8") as f:
                data = json.load(f)
                print(
                    f"âœ… Successfully loaded {data['leaderboard_info']['total_models']} models from {self.data_file}"
                )
                return data
        except FileNotFoundError:
            print(
                f"âŒ Data file {self.data_file} not found. Creating empty leaderboard."
            )
            return {"leaderboard_info": {"total_models": 0}, "models": []}
        except Exception as e:
            print(f"âŒ Error loading data: {e}")
            return {"leaderboard_info": {"total_models": 0}, "models": []}

    def _format_accuracy(self, accuracy: Optional[float]) -> str:
        """æ ¼å¼åŒ–å‡†ç¡®ç‡æ˜¾ç¤º"""
        if accuracy is None:
            return "-"
        return f"{accuracy:.1f}"

    def _calculate_average(self, results: Dict) -> Optional[float]:
        """è®¡ç®—å¹³å‡å‡†ç¡®ç‡ï¼Œä½¿ç”¨overall_accuracyå­—æ®µ"""
        return results.get("overall_accuracy")

    def _get_model_type_icon(self, model_type: str) -> str:
        """è·å–æ¨¡å‹ç±»å‹å›¾æ ‡"""
        icons = {"open_source": "ğŸ”“", "proprietary": "ğŸ”’", "baseline": "ğŸ“Š"}
        return icons.get(model_type, "â“")

    def _get_multimodal_icon(self, is_multimodal: bool) -> str:
        """è·å–å¤šæ¨¡æ€å›¾æ ‡"""
        return "ğŸ‘ï¸" if is_multimodal else "ğŸ“"

    def _get_rank_display(self, rank: int) -> str:
        """è·å–æ’åæ˜¾ç¤ºï¼Œå‰ä¸‰åæ˜¾ç¤ºå¥–ç‰Œ"""
        medals = {1: "ğŸ¥‡", 2: "ğŸ¥ˆ", 3: "ğŸ¥‰"}
        return medals.get(rank, str(rank))

    def _create_link(self, text: str, url: str) -> str:
        """åˆ›å»ºHTMLé“¾æ¥"""
        if url and url.strip():
            return f'<a href="{url}" target="_blank" style="text-decoration: none; color: inherit;">{text}</a>'
        return text

    def get_leaderboard_df(
        self,
        model_type_filter: str = "All",
        multimodal_filter: str = "All",
        sort_by: str = "Overall",
        ascending: bool = False,
    ) -> pd.DataFrame:
        """ç”Ÿæˆæ’è¡Œæ¦œDataFrame"""

        models = self.data.get("models", [])
        print(f"ğŸ“Š Processing {len(models)} models")

        # ç­›é€‰æ¨¡å‹
        filtered_models = []
        for model in models:
            # æ¨¡å‹ç±»å‹ç­›é€‰
            if (
                model_type_filter != "All"
                and model.get("model_type", "") != model_type_filter
            ):
                continue

            # å¤šæ¨¡æ€ç­›é€‰
            if multimodal_filter == "Multimodal Only" and not model.get(
                "is_multimodal", False
            ):
                continue
            elif multimodal_filter == "Text Only" and model.get("is_multimodal", False):
                continue

            filtered_models.append(model)

        print(f"ğŸ” After filtering: {len(filtered_models)} models")

        # æ„å»ºDataFrameæ•°æ®
        data = []
        for model in filtered_models:
            try:
                results = model.get("results", {})

                # è·å–å„é¡¹å‡†ç¡®ç‡
                overall_accuracy = self._calculate_average(results)
                signal_acc = results.get("Signal", {}).get("accuracy")
                perception_acc = results.get("Perception", {}).get("accuracy")
                semantic_acc = results.get("Semantic", {}).get("accuracy")
                generation_acc = results.get("Generation", {}).get("accuracy")

                # åˆ›å»ºå¸¦é“¾æ¥çš„æ¨¡å‹åå’Œæäº¤è€…
                model_name_display = self._create_link(
                    model.get("name", "Unknown"), model.get("name_link", "")
                )
                submitter_display = self._create_link(
                    model.get("submitter", "Unknown"), model.get("submitter_link", "")
                )

                row = {
                    "Type": self._get_model_type_icon(
                        model.get("model_type", "unknown")
                    ),
                    "Model": model_name_display,
                    "Size": model.get("model_size", "Unknown"),
                    "MM": self._get_multimodal_icon(model.get("is_multimodal", False)),
                    "Overall": self._format_accuracy(overall_accuracy),
                    "Signal": self._format_accuracy(signal_acc),
                    "Perception": self._format_accuracy(perception_acc),
                    "Semantic": self._format_accuracy(semantic_acc),
                    "Generation": self._format_accuracy(generation_acc),
                    "Submitter": submitter_display,
                    "Date": (
                        model.get("submission_time", "")[:10]
                        if model.get("submission_time")
                        else "-"
                    ),
                    # ç”¨äºæ’åºçš„æ•°å€¼åˆ—
                    "overall_val": overall_accuracy or 0,
                    "signal_val": signal_acc or 0,
                    "perception_val": perception_acc or 0,
                    "semantic_val": semantic_acc or 0,
                    "generation_val": generation_acc or 0,
                }
                data.append(row)
            except Exception as e:
                print(f"âš ï¸ Error processing model {model.get('name', 'Unknown')}: {e}")
                continue

        df = pd.DataFrame(data)
        print(f"ğŸ“‹ Created DataFrame with {len(df)} rows")

        if len(df) == 0:
            print("ğŸ“‹ Empty DataFrame, returning empty table")
            return pd.DataFrame(
                columns=[
                    "Rank",
                    "Type",
                    "Model",
                    "Size",
                    "MM",
                    "Overall",
                    "Signal",
                    "Perception",
                    "Semantic",
                    "Generation",
                    "Submitter",
                    "Date",
                ]
            )

        # æ’åº
        sort_mapping = {
            "Overall": "overall_val",
            "Signal": "signal_val",
            "Perception": "perception_val",
            "Semantic": "semantic_val",
            "Generation": "generation_val",
            "Model": "Model",
            "Date": "Date",
        }

        sort_col = sort_mapping.get(sort_by, "overall_val")
        df = df.sort_values(by=sort_col, ascending=ascending)

        # æ·»åŠ å¸¦å¥–ç‰Œçš„æ’å
        ranks = []
        for i in range(len(df)):
            rank_num = i + 1
            ranks.append(self._get_rank_display(rank_num))

        df.insert(0, "Rank", ranks)

        # ç§»é™¤ç”¨äºæ’åºçš„è¾…åŠ©åˆ—
        display_columns = [
            "Rank",
            "Type",
            "Model",
            "Size",
            "MM",
            "Overall",
            "Signal",
            "Perception",
            "Semantic",
            "Generation",
            "Submitter",
            "Date",
        ]
        result_df = df[display_columns]
        print(f"âœ… Returning DataFrame with {len(result_df)} rows")
        return result_df

    def get_subcategory_details(self, model_name: str) -> pd.DataFrame:
        """è·å–æ¨¡å‹çš„å­ç±»åˆ«è¯¦ç»†ç»“æœ"""
        # ç§»é™¤HTMLæ ‡ç­¾è¿›è¡ŒåŒ¹é…
        clean_model_name = model_name
        if "<a href=" in model_name:
            # æå–é“¾æ¥ä¸­çš„æ–‡æœ¬
            import re

            match = re.search(r">([^<]+)<", model_name)
            if match:
                clean_model_name = match.group(1)

        for model in self.data.get("models", []):
            if model.get("name") == clean_model_name:
                data = []
                results = model.get("results", {})
                for level, level_data in results.items():
                    if level == "overall_accuracy":  # è·³è¿‡æ€»ä½“å‡†ç¡®ç‡å­—æ®µ
                        continue

                    subcategories = level_data.get("subcategories", {})
                    for subcat, subcat_data in subcategories.items():
                        data.append(
                            {
                                "Level": level,
                                "Subcategory": subcat,
                                "Accuracy": self._format_accuracy(
                                    subcat_data.get("accuracy")
                                ),
                            }
                        )
                return pd.DataFrame(data)
        return pd.DataFrame()


def create_leaderboard():
    """åˆ›å»ºæ’è¡Œæ¦œGradioç•Œé¢"""

    leaderboard = SpectralLeaderboard()

    with gr.Blocks(
        title="ğŸ”¬ SpectrumLab Leaderboard",
        theme=gr.themes.Default(),
        css="""
        .gradio-container {
            max-width: 1400px !important;
        }
        .dataframe table {
            border-collapse: collapse !important;
        }
        .dataframe td, .dataframe th {
            padding: 8px 12px !important;
            border: 1px solid #e1e5e9 !important;
        }
        .dataframe th {
            background-color: #f8f9fa !important;
            font-weight: 600 !important;
        }
        .dataframe tr:nth-child(even) {
            background-color: #f8f9fa !important;
        }
        .dataframe tr:hover {
            background-color: #e8f4f8 !important;
        }
        """,
    ) as demo:
        gr.Markdown(
            """
            # ğŸ† SpectrumLab Leaderboard
            
            A comprehensive benchmark for evaluating large language models on **spectroscopic analysis tasks**.
            
            ğŸ“Š **Evaluation Levels**: Signal Processing, Perception, Semantic Understanding, Generation  
            ğŸ”¬ **Domains**: IR, NMR, UV-Vis, Mass Spectrometry and more  
            ğŸŒŸ **Multimodal**: Support for both text-only and vision-language models
            """
        )

        with gr.Row():
            info = leaderboard.data.get("leaderboard_info", {"total_models": 0})
            gr.Markdown(
                f"""
                **ğŸ“ˆ Stats**: {info["total_models"]} models evaluated  
                **ğŸ… Rankings**: ğŸ¥‡ğŸ¥ˆğŸ¥‰ medals for top performers  
                **ğŸ”— Submit**: Send evaluation results to contribute your model!
                """
            )

        with gr.Row():
            with gr.Column(scale=2):
                model_type_filter = gr.Dropdown(
                    choices=["All", "open_source", "proprietary", "baseline"],
                    value="All",
                    label="ğŸ·ï¸ Model Type",
                )

            with gr.Column(scale=2):
                multimodal_filter = gr.Dropdown(
                    choices=["All", "Multimodal Only", "Text Only"],
                    value="All",
                    label="ğŸ‘ï¸ Modality",
                )

            with gr.Column(scale=2):
                sort_by = gr.Dropdown(
                    choices=[
                        "Overall",
                        "Signal",
                        "Perception",
                        "Semantic",
                        "Generation",
                        "Model",
                        "Date",
                    ],
                    value="Overall",
                    label="ğŸ“Š Sort By",
                )

            with gr.Column(scale=1):
                ascending = gr.Checkbox(value=False, label="â¬†ï¸ Ascending")

            with gr.Column(scale=1):
                refresh_btn = gr.Button("ğŸ”„ Refresh", variant="secondary")

        # ä¸»æ’è¡Œæ¦œè¡¨æ ¼
        initial_df = leaderboard.get_leaderboard_df()
        leaderboard_table = gr.Dataframe(
            value=initial_df,
            interactive=False,
            wrap=True,
            datatype=["html"] * len(initial_df.columns)
            if len(initial_df.columns) > 0
            else ["html"] * 12,
            column_widths=(
                [
                    "6%",
                    "5%",
                    "18%",
                    "8%",
                    "5%",
                    "10%",
                    "10%",
                    "10%",
                    "10%",
                    "10%",
                    "16%",
                    "10%",
                ]
                if len(initial_df.columns) > 0
                else None
            ),
            label="ğŸ† Model Rankings",
        )

        # æ¨¡å‹è¯¦ç»†ä¿¡æ¯
        with gr.Accordion("ğŸ“‹ Model Details", open=False):
            model_choices = [
                model.get("name", "Unknown")
                for model in leaderboard.data.get("models", [])
            ]
            model_select = gr.Dropdown(
                choices=model_choices,
                label="Select Model for Details",
            )

            with gr.Row():
                with gr.Column():
                    subcategory_table = gr.Dataframe(label="ğŸ“Š Subcategory Results")

                with gr.Column():
                    model_info = gr.Markdown(label="â„¹ï¸ Model Information")

        # å›¾ä¾‹è¯´æ˜
        with gr.Accordion("ğŸ“– Legend & Info", open=False):
            gr.Markdown(
                """
                ### ğŸ” Column Explanations
                
                - **Rank**: ğŸ¥‡ 1st place, ğŸ¥ˆ 2nd place, ğŸ¥‰ 3rd place, then numbers
                - **Type**: ğŸ”“ Open Source, ğŸ”’ Proprietary, ğŸ“Š Baseline
                - **MM**: ğŸ‘ï¸ Multimodal, ğŸ“ Text-only  
                - **Overall**: Average accuracy across all evaluated levels
                - **Signal**: Low-level signal processing tasks
                - **Perception**: Mid-level feature extraction tasks
                - **Semantic**: High-level understanding tasks
                - **Generation**: Spectrum generation tasks
                
                ### ğŸ“ Notes
                - "-" indicates the model was not evaluated on that benchmark
                - Rankings are based on overall performance across all evaluated tasks
                - Multimodal models can process both text and spectroscopic images
                - Click on model names and submitters to visit their pages
                
                ### ğŸ“Š Task Categories
                
                **Signal Level:**
                - Spectrum Type Classification (TC)
                - Spectrum Quality Assessment (QE)
                - Basic Feature Extraction (FE)
                - Impurity Peak Detection (ID)
                
                **Perception Level:**
                - Functional Group Recognition (GR)
                - Elemental Compositional Prediction (EP)
                - Peak Assignment (PA)
                - Basic Property Prediction (PP)
                
                **Semantic Level:**
                - Molecular Structure Elucidation (SE)
                - Fusing Spectroscopic Modalities (FM)
                - Multimodal Molecular Reasoning (MR)
                
                **Generation Level:**
                - Forward Problems (FP)
                - Inverse Problems (IP)
                - De Novo Generation (DnG)
                """
            )

        def update_leaderboard(model_type, multimodal, sort_by_val, asc):
            """æ›´æ–°æ’è¡Œæ¦œ"""
            print(
                f"ğŸ”„ Updating leaderboard with filters: {model_type}, {multimodal}, {sort_by_val}, {asc}"
            )
            return leaderboard.get_leaderboard_df(
                model_type_filter=model_type,
                multimodal_filter=multimodal,
                sort_by=sort_by_val,
                ascending=asc,
            )

        def update_model_details(model_name):
            """æ›´æ–°æ¨¡å‹è¯¦ç»†ä¿¡æ¯"""
            if not model_name:
                return pd.DataFrame(), ""

            # è·å–å­ç±»åˆ«è¯¦æƒ…
            subcategory_df = leaderboard.get_subcategory_details(model_name)

            # è·å–æ¨¡å‹åŸºæœ¬ä¿¡æ¯
            for model in leaderboard.data.get("models", []):
                if model.get("name") == model_name:
                    # å¤„ç†é“¾æ¥æ˜¾ç¤º
                    def format_link(name, url):
                        if url and url.strip():
                            return f"[{name}]({url})"
                        return "Not provided"

                    model_info_dict = model.get("model_info", {})
                    results = model.get("results", {})

                    info_md = f"""
                    ### {model.get("name", "Unknown")}
                    
                    **ğŸ‘¤ Submitter**: {model.get("submitter", "Unknown")}  
                    **ğŸ“… Submission**: {model.get("submission_time", "")[:10] if model.get("submission_time") else "Unknown"}  
                    **ğŸ·ï¸ Type**: {model.get("model_type", "Unknown")}  
                    **ğŸ“ Size**: {model.get("model_size", "Unknown")}  
                    **ğŸ‘ï¸ Multimodal**: {"Yes" if model.get("is_multimodal", False) else "No"}  
                    
                    **ğŸ“ Description**: {model_info_dict.get("description", "") or "No description provided"}
                    
                    **ğŸ”— Links**:  
                    - **Homepage**: {format_link("Visit", model_info_dict.get("homepage", ""))}
                    - **Paper**: {format_link("Read", model_info_dict.get("paper", ""))}  
                    - **Code**: {format_link("View", model_info_dict.get("code", ""))}
                    
                    **ğŸ“Š Performance Summary**:
                    - **Overall**: {leaderboard._format_accuracy(results.get("overall_accuracy"))}%
                    - **Signal**: {leaderboard._format_accuracy(results.get("Signal", {}).get("accuracy"))}%
                    - **Perception**: {leaderboard._format_accuracy(results.get("Perception", {}).get("accuracy"))}%
                    - **Semantic**: {leaderboard._format_accuracy(results.get("Semantic", {}).get("accuracy"))}%
                    - **Generation**: {leaderboard._format_accuracy(results.get("Generation", {}).get("accuracy"))}%
                    """
                    return subcategory_df, info_md

            return pd.DataFrame(), ""

        # äº‹ä»¶ç»‘å®š
        for component in [model_type_filter, multimodal_filter, sort_by, ascending]:
            component.change(
                fn=update_leaderboard,
                inputs=[model_type_filter, multimodal_filter, sort_by, ascending],
                outputs=[leaderboard_table],
            )

        refresh_btn.click(
            fn=update_leaderboard,
            inputs=[model_type_filter, multimodal_filter, sort_by, ascending],
            outputs=[leaderboard_table],
        )

        model_select.change(
            fn=update_model_details,
            inputs=[model_select],
            outputs=[subcategory_table, model_info],
        )

    return demo


if __name__ == "__main__":
    app = create_leaderboard()
    print("ğŸš€ Starting SpectrumLab Leaderboard...")
    app.launch(
        server_name="0.0.0.0",
        share=True,
        show_api=False,
        inbrowser=True,
    )
