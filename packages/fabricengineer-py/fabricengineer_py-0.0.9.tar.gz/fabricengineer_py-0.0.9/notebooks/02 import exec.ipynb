{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43180ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from fabricengineer.import_module.import_module import import_module\n",
    "\n",
    "VERSION = \"0.0.8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6dbac0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/04 19:36:58 WARN Utils: Your hostname, MacBook-Air-von-Enrico.local resolves to a loopback address: 127.0.0.1; using 192.168.0.7 instead (on interface en0)\n",
      "25/08/04 19:36:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/04 19:36:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "class NotebookUtilsFSMock:\n",
    "    def _get_path(self, file: str) -> str:\n",
    "        return os.path.join(os.getcwd(), file)\n",
    "\n",
    "    def exists(self, path: str) -> bool:\n",
    "        return os.path.exists(self._get_path(path))\n",
    "\n",
    "    def put(\n",
    "        self,\n",
    "        file: str,\n",
    "        content: str,\n",
    "        overwrite: bool = False\n",
    "    ) -> None:\n",
    "        path = self._get_path(file)\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "        if os.path.exists(path) and not overwrite:\n",
    "            raise FileExistsError(f\"File {path} already exists and overwrite is set to False.\")\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(content)\n",
    "\n",
    "\n",
    "class NotebookUtilsMock:\n",
    "    def __init__(self):\n",
    "        self.fs = NotebookUtilsFSMock()\n",
    "\n",
    "global spark\n",
    "spark: SparkSession = SparkSession.builder.appName(\"PlaygroundSparkSession\").getOrCreate()\n",
    "\n",
    "global notebookutils\n",
    "notebookutils = NotebookUtilsMock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3602f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "\n",
      "from datetime import datetime\n",
      "from typing import Callable\n",
      "from abc import ABC, abstractmethod\n",
      "from dataclasses import dataclass\n",
      "from uuid import uuid4\n",
      "\n",
      "from pyspark.sql import (\n",
      "    SparkSession,\n",
      "    DataFrame,\n",
      "    functions as F,\n",
      "    types as T,\n",
      "    Window\n",
      ")\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class LakehouseTable:\n",
      "    lakehouse: str\n",
      "    schema: str\n",
      "    table: str\n",
      "\n",
      "    @property\n",
      "    def table_path(self) -> str:\n",
      "        return f\"{self.lakehouse}.{self.schema}.{self.table}\"\n",
      "\n",
      "\n",
      "@F.udf(returnType=T.StringType())\n",
      "def generate_uuid():\n",
      "    \"\"\"Generiert eine UUID4\"\"\"\n",
      "    return str(uuid4())\n",
      "\n",
      "\n",
      "@dataclass(frozen=True)\n",
      "class ConstantColumn:\n",
      "    \"\"\"Class for adding a column with constant value to etl\"\"\"\n",
      "    name: str\n",
      "    value: str\n",
      "    part_of_nk: bool = False\n",
      "\n",
      "    def __post_init__(self):\n",
      "        \"\"\"\n",
      "        Nach initialisierung wird der name in UPPERCASE umgewandelt.\n",
      "        \"\"\"\n",
      "        object.__setattr__(self, \"name\", self.name.upper())\n",
      "\n",
      "\n",
      "def get_mock_table_path(table: LakehouseTable) -> str:\n",
      "    if table is None:\n",
      "        raise ValueError(\"Table is not initialized.\")\n",
      "    table_path = table.table_path.replace(\".\", \"/\")\n",
      "    full_table_path = f\"tmp/lakehouse/{table_path}\"\n",
      "    return full_table_path\n",
      "\n",
      "\n",
      "class BaseSilverIngestionService(ABC):\n",
      "    @abstractmethod\n",
      "    def init(self, **kwargs): pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def ingest(self, **kwargs): pass\n",
      "\n",
      "\n",
      "class SilverIngestionInsertOnlyService(BaseSilverIngestionService):\n",
      "    _is_initialized: bool = False\n",
      "\n",
      "    def init(\n",
      "        self,\n",
      "        *,\n",
      "        spark_: SparkSession,\n",
      "        source_table: LakehouseTable,\n",
      "        destination_table: LakehouseTable,\n",
      "        nk_columns: list[str],\n",
      "        constant_columns: list[ConstantColumn],\n",
      "        is_delta_load: bool,\n",
      "        delta_load_use_broadcast: bool,\n",
      "        transformations: dict,\n",
      "        exclude_comparing_columns: list[str] | None = None,\n",
      "        include_comparing_columns: list[str] | None = None,\n",
      "        historize: bool = True,\n",
      "        partition_by_columns: list[str] = None,\n",
      "        df_bronze: DataFrame = None,\n",
      "        create_historized_mlv: bool = True,\n",
      "        mlv_suffix: str = \"_h\",\n",
      "\n",
      "        pk_column_name: str = \"PK\",\n",
      "        nk_column_name: str = \"NK\",\n",
      "        nk_column_concate_str: str = \"_\",\n",
      "        row_is_current_column: str = \"ROW_IS_CURRENT\",\n",
      "        row_hist_number_column: str = \"ROW_HIST_NUMBER\",\n",
      "        row_update_dts_column: str = \"ROW_UPDATE_DTS\",\n",
      "        row_delete_dts_column: str = \"ROW_DELETE_DTS\",\n",
      "        row_load_dts_column: str = \"ROW_LOAD_DTS\",\n",
      "\n",
      "        is_testing_mock: bool = False\n",
      "    ) -> None:\n",
      "        self._mlv_code = None\n",
      "\n",
      "        self._is_testing_mock = is_testing_mock\n",
      "        self._spark = spark_\n",
      "        self._df_bronze = df_bronze\n",
      "        self._historize = historize\n",
      "        self._is_create_hist_mlv = create_historized_mlv\n",
      "        self._mlv_suffix = mlv_suffix\n",
      "        self._is_delta_load = is_delta_load\n",
      "        self._delta_load_use_broadcast = delta_load_use_broadcast\n",
      "        self._src_table = source_table\n",
      "        self._dest_table = destination_table\n",
      "        self._nk_columns = nk_columns\n",
      "        self._include_comparing_columns = include_comparing_columns\n",
      "\n",
      "        self._exclude_comparing_columns = exclude_comparing_columns or []\n",
      "        self._transformations: dict[str, Callable] = transformations or {}\n",
      "        self._constant_columns: list[ConstantColumn] = constant_columns or []\n",
      "        self._partition_by: list[str] = partition_by_columns or []\n",
      "\n",
      "        self._pk_column_name = pk_column_name\n",
      "        self._nk_column_name = nk_column_name\n",
      "        self._nk_column_concate_str = nk_column_concate_str\n",
      "        self._row_hist_number_column = row_hist_number_column\n",
      "        self._row_is_current_column = row_is_current_column\n",
      "        self._row_update_dts_column = row_update_dts_column\n",
      "        self._row_delete_dts_column = row_delete_dts_column\n",
      "        self._row_load_dts_column = row_load_dts_column\n",
      "\n",
      "        self._validate_parameters()\n",
      "        self._set_spark_config()\n",
      "\n",
      "        self._dw_columns = [\n",
      "            self._pk_column_name,\n",
      "            self._nk_column_name,\n",
      "            self._row_delete_dts_column,\n",
      "            self._row_load_dts_column\n",
      "        ]\n",
      "\n",
      "        self._exclude_comparing_columns = set(\n",
      "            [self._pk_column_name]\n",
      "            + self._nk_columns\n",
      "            + self._dw_columns\n",
      "            + self._exclude_comparing_columns\n",
      "            + [column.name for column in self._constant_columns]\n",
      "        )\n",
      "\n",
      "        self._spark.catalog.clearCache()\n",
      "        self._is_initialized = True\n",
      "\n",
      "    @property\n",
      "    def mlv_name(self) -> str:\n",
      "        return f\"{self._dest_table.table_path}{self._mlv_suffix}\"\n",
      "\n",
      "    @property\n",
      "    def mlv_code(self) -> str:\n",
      "        return self._mlv_code\n",
      "\n",
      "    def __str__(self) -> str:\n",
      "        if not self._is_initialized:\n",
      "            return super.__str__(self)\n",
      "\n",
      "        return str({\n",
      "            \"historize\": self._historize,\n",
      "            \"is_delta_load\": self._is_delta_load,\n",
      "            \"delta_load_use_broadcast\": self._delta_load_use_broadcast,\n",
      "            \"src_table_path\": self._src_table.table_path,\n",
      "            \"dist_table_path\": self._dest_table.table_path,\n",
      "            \"nk_columns\": self._nk_columns,\n",
      "            \"include_comparing_columns\": self._include_comparing_columns,\n",
      "            \"exclude_comparing_columns\": self._exclude_comparing_columns,\n",
      "            \"transformations\": self._transformations,\n",
      "            \"constant_columns\": self._constant_columns,\n",
      "            \"partition_by\": self._partition_by,\n",
      "            \"pk_column\": self._pk_column_name,\n",
      "            \"nk_column\": self._nk_column_name,\n",
      "            \"nk_column_concate_str\": self._nk_column_concate_str,\n",
      "            \"row_update_dts_column\": self._row_update_dts_column,\n",
      "            \"row_delete_dts_column\": self._row_delete_dts_column,\n",
      "            \"ldts_column\": self._row_load_dts_column,\n",
      "            \"dw_columns\": self._dw_columns\n",
      "        })\n",
      "\n",
      "    def _validate_parameters(self) -> None:\n",
      "        \"\"\"Validates the in constructor setted parameters, so the etl can run.\n",
      "\n",
      "        Raises:\n",
      "            ValueError: when a valueerror occurs\n",
      "            TypeError: when a typerror occurs\n",
      "            Exception: generic exception\n",
      "        \"\"\"\n",
      "\n",
      "        if self._df_bronze is not None:\n",
      "            self._validate_param_isinstance(self._df_bronze, \"df_bronze\", DataFrame)\n",
      "\n",
      "        self._validate_param_isinstance(self._spark, \"spark\", SparkSession)\n",
      "        self._validate_param_isinstance(self._historize, \"historize\", bool)\n",
      "        self._validate_param_isinstance(self._is_create_hist_mlv, \"create_historized_mlv\", bool)\n",
      "        self._validate_param_isinstance(self._is_delta_load, \"is_delta_load\", bool)\n",
      "        self._validate_param_isinstance(self._delta_load_use_broadcast, \"delta_load_use_broadcast\", bool)\n",
      "        self._validate_param_isinstance(self._transformations, \"transformations\", dict)\n",
      "        self._validate_param_isinstance(self._src_table, \"src_table\", LakehouseTable)\n",
      "        self._validate_param_isinstance(self._dest_table, \"dest_table\", LakehouseTable)\n",
      "        self._validate_param_isinstance(self._include_comparing_columns, \"include_columns_from_comparing\", list)\n",
      "        self._validate_param_isinstance(self._exclude_comparing_columns, \"exclude_columns_from_comparing\", list)\n",
      "        self._validate_param_isinstance(self._partition_by, \"partition_by_columns\", list)\n",
      "        self._validate_param_isinstance(self._pk_column_name, \"pk_column\", str)\n",
      "        self._validate_param_isinstance(self._nk_column_name, \"nk_column\", str)\n",
      "        self._validate_param_isinstance(self._nk_columns, \"nk_columns\", list)\n",
      "        self._validate_param_isinstance(self._nk_column_concate_str, \"nk_column_concate_str\", str)\n",
      "        self._validate_param_isinstance(self._mlv_suffix, \"mlv_suffix\", str)\n",
      "        self._validate_param_isinstance(self._constant_columns, \"constant_columns\", list)\n",
      "        self._validate_param_isinstance(self._row_load_dts_column, \"row_load_dts_column\", str)\n",
      "        self._validate_param_isinstance(self._row_hist_number_column, \"row_hist_number_column\", str)\n",
      "        self._validate_param_isinstance(self._row_is_current_column, \"row_is_current_column\", str)\n",
      "        self._validate_param_isinstance(self._row_update_dts_column, \"row_update_dts_column\", str)\n",
      "        self._validate_param_isinstance(self._row_delete_dts_column, \"row_delete_dts_column\", str)\n",
      "\n",
      "        self._validate_min_length(self._pk_column_name, \"pk_column\", 2)\n",
      "        self._validate_min_length(self._nk_column_name, \"nk_column\", 2)\n",
      "        self._validate_min_length(self._src_table.lakehouse, \"src_lakehouse\", 3)\n",
      "        self._validate_min_length(self._src_table.schema, \"src_schema\", 1)\n",
      "        self._validate_min_length(self._src_table.table, \"src_tablename\", 3)\n",
      "        self._validate_min_length(self._dest_table.lakehouse, \"dest_lakehouse\", 3)\n",
      "        self._validate_min_length(self._dest_table.schema, \"dest_schema\", 1)\n",
      "        self._validate_min_length(self._dest_table.table, \"dest_tablename\", 3)\n",
      "        self._validate_min_length(self._nk_columns, \"nk_columns\", 1)\n",
      "        self._validate_min_length(self._nk_column_concate_str, \"nk_column_concate_str\", 1)\n",
      "        self._validate_min_length(self._mlv_suffix, \"mlv_suffix\", 1)\n",
      "        self._validate_min_length(self._row_load_dts_column, \"row_load_dts_column\", 3)\n",
      "        self._validate_min_length(self._row_hist_number_column, \"row_hist_number_column\", 3)\n",
      "        self._validate_min_length(self._row_is_current_column, \"row_is_current_column\", 3)\n",
      "        self._validate_min_length(self._row_update_dts_column, \"row_update_dts_column\", 3)\n",
      "        self._validate_min_length(self._row_delete_dts_column, \"row_delete_dts_column\", 3)\n",
      "\n",
      "        for key, fn in self._transformations.items():\n",
      "            print(\"Transformation function:\", fn)\n",
      "            if not callable(fn):\n",
      "                err_msg = f\"The transformation function for key '{key}' is not callable.\"\n",
      "                raise TypeError(err_msg)\n",
      "\n",
      "        for constant_column in self._constant_columns:\n",
      "            self._validate_param_isinstance(constant_column, \"constant_column\", ConstantColumn)\n",
      "\n",
      "    def _validate_param_isinstance(self, param, param_name: str, obj_class) -> None:\n",
      "        \"\"\"Validates a parameter to be the expected class instance\n",
      "\n",
      "        Args:\n",
      "            param (any): parameter\n",
      "            param_name (str): parametername\n",
      "            obj_class (_type_): class\n",
      "\n",
      "        Raises:\n",
      "            TypeError: when actual type is different from expected type\n",
      "        \"\"\"\n",
      "        if not isinstance(param, obj_class):\n",
      "            err_msg = f\"The param '{param_name}' should be type of {obj_class.__name__}, but was {str(param.__class__)}\"\n",
      "            raise TypeError(err_msg)\n",
      "\n",
      "    def _validate_min_length(self, param, param_name: str, min_length: int) -> None:\n",
      "        \"\"\"Validates a string or list to be not none and has a minimum length\n",
      "\n",
      "        Args:\n",
      "            param (_type_): parameter\n",
      "            param_name (str): parametername\n",
      "            min_length (int): minimum lenght\n",
      "\n",
      "        Raises:\n",
      "            TypeError: when actual type is different from expected type\n",
      "            ValueError: when parametervalue is to short\n",
      "        \"\"\"\n",
      "        if not isinstance(param, str) and not isinstance(param, list):\n",
      "            err_msg = f\"The param '{param_name}' should be type of string or list, but was {str(param.__class__)}\"\n",
      "            raise TypeError(err_msg)\n",
      "\n",
      "        param_length = len(param)\n",
      "        if param_length < min_length:\n",
      "            err_msg = f\"Param length to short. The minimum length of the param '{param_name}' is {min_length} but was {param_length}\"\n",
      "            raise ValueError(err_msg)\n",
      "\n",
      "    def _validate_constant_columns(self) -> None:\n",
      "        \"\"\"Validates the given constant columns to be an instance of ConstantColumns and\n",
      "        list contains only one part_of_nk=True, because of the following filtering of the dataframe.\n",
      "\n",
      "        It should have just one part_of_nk=True, because the dataframe will filtered later by the\n",
      "        constant_column.name, if part_of_nk=True.\n",
      "        If part_of_nk=True should be supported more then once, then we need to implement\n",
      "        an \"and\" filtering.\n",
      "\n",
      "        Raises:\n",
      "            TypeError: when an item of the list is not an instance of ConstantColumn\n",
      "            ValueError: when list contains more then one ConstantColumn with part_of_nk=True\n",
      "        \"\"\"\n",
      "        nk_count = 0\n",
      "        for column in self._constant_columns:\n",
      "            if column.part_of_nk:\n",
      "                nk_count += 1\n",
      "\n",
      "            if not isinstance(column, ConstantColumn):\n",
      "                err_msg = \"Invalid items in constant_columns found. All items should be instance of ConstantColumn\"\n",
      "                raise TypeError(err_msg)\n",
      "\n",
      "            if nk_count > 1:\n",
      "                err_msg = \"In constant_columns are more then one part_of_nk=True, what is not supported!\"\n",
      "                raise ValueError(err_msg)\n",
      "\n",
      "    def _validate_nk_columns_in_df(self, df: DataFrame) -> None:\n",
      "        \"\"\"Validates the given dataframe. The given dataframe should contain all natural key columns,\n",
      "        because all natural key columns will selected and used for concatitation.\n",
      "\n",
      "        Args:\n",
      "            df (DataFrame): dataframe to validate\n",
      "\n",
      "        Raises:\n",
      "            ValueError: when dataframe does not contain all natural key columns\n",
      "        \"\"\"\n",
      "        df_columns = set(df.columns)\n",
      "        for column in self._nk_columns:\n",
      "            if column in df_columns:\n",
      "                continue\n",
      "\n",
      "            err_msg = f\"The NK Column '{column}' does not exist in df columns: {df_columns}\"\n",
      "            raise ValueError(err_msg)\n",
      "\n",
      "    def _validate_include_comparing_columns(self, df: DataFrame) -> None:\n",
      "        self._validate_param_isinstance(self._include_comparing_columns, \"include_comparing_columns\", list)\n",
      "\n",
      "        if len(self._include_comparing_columns) == 0:\n",
      "            err_msg = \"The param 'include_comparing_columns' is present, but don't contains any columns.\"\n",
      "            raise ValueError(err_msg)\n",
      "\n",
      "        for include_column in self._include_comparing_columns:\n",
      "            if include_column in df.columns:\n",
      "                continue\n",
      "\n",
      "            err_msg = f\"The column '{include_column}' should be compared, but is not given in df.\"\n",
      "            raise ValueError(err_msg)\n",
      "\n",
      "    def _validate_partition_by_columns(self, df: DataFrame) -> None:\n",
      "        self._validate_param_isinstance(self._partition_by, \"partition_by\", list)\n",
      "\n",
      "        for partition_column in self._partition_by:\n",
      "            if partition_column in df.columns:\n",
      "                continue\n",
      "\n",
      "            err_msg = f\"The column '{partition_column}' should be partitioned, but is not given in df.\"\n",
      "            raise ValueError(err_msg)\n",
      "\n",
      "    def _set_spark_config(self) -> None:\n",
      "        \"\"\"Sets additional spark configurations\n",
      "\n",
      "        spark.sql.parquet.vorder.enabled: Setting \"spark.sql.parquet.vorder.enabled\" to \"true\" in PySpark config enables a feature called vectorized parquet decoding.\n",
      "                                                  This optimizes the performance of reading Parquet files by leveraging vectorized instructions and processing multiple values at once, enhancing overall processing speed.\n",
      "\n",
      "        Setting \"spark.sql.parquet.int96RebaseModeInRead\" and \"spark.sql.legacy.parquet.int96RebaseModeInWrite\" to \"CORRECTED\" ensures that Int96 values (a specific timestamp representation used in Parquet files) are correctly rebased during both reading and writing operations.\n",
      "        This is crucial for maintaining consistency and accuracy, especially when dealing with timestamp data across different systems or time zones.\n",
      "        Similarly, configuring \"spark.sql.parquet.datetimeRebaseModeInRead\" and \"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\" to \"CORRECTED\" ensures correct handling of datetime values during Parquet file operations.\n",
      "        By specifying this rebasing mode, potential discrepancies or errors related to datetime representations are mitigated, resulting in more reliable data processing and analysis workflows.\n",
      "        \"\"\"\n",
      "        self._spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
      "\n",
      "        self._spark.conf.set(\"spark.sql.parquet.int96RebaseModeInRead\", \"CORRECTED\")\n",
      "        self._spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
      "        self._spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n",
      "        self._spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
      "\n",
      "    def ingest(self) -> DataFrame:\n",
      "        if not self._is_initialized:\n",
      "            raise RuntimeError(\"The SilverIngestionInsertOnlyService is not initialized. Call the init method first.\")\n",
      "\n",
      "        self._current_timestamp = datetime.now()\n",
      "\n",
      "        # 1.\n",
      "        df_bronze, df_silver, has_schema_changed = self._generate_dataframes()\n",
      "\n",
      "        target_columns_ordered = self._get_columns_ordered(df_bronze)\n",
      "\n",
      "        do_overwrite = (\n",
      "            df_silver is None or\n",
      "            (\n",
      "                not self._historize and\n",
      "                not self._is_delta_load\n",
      "                # If we are not historizing but performing a delta load,\n",
      "                # we need to update the silver-layer data.\n",
      "                # We should not overwrite the silver-layer data,\n",
      "                # because the delta load (bronze layer) do not contain all the data!\n",
      "            )\n",
      "        )\n",
      "        if do_overwrite:\n",
      "            df_inital_load = df_bronze.select(target_columns_ordered)\n",
      "            self._write_df(df_inital_load, \"overwrite\")\n",
      "            self._create_or_replace_historized_mlv(has_schema_changed, target_columns_ordered)\n",
      "            return df_inital_load\n",
      "\n",
      "        # 2.\n",
      "        columns_to_compare = self._get_columns_to_compare(df_bronze)\n",
      "\n",
      "        join_condition = (df_bronze[self._nk_column_name] == df_silver[self._nk_column_name])\n",
      "        df_joined = df_bronze.join(df_silver, join_condition, \"outer\")\n",
      "\n",
      "        # 3.\n",
      "        _, neq_condition = self._get_compare_condition(df_bronze, df_silver, columns_to_compare)\n",
      "        updated_filter_condition = self._get_updated_filter(df_bronze, df_silver, neq_condition)\n",
      "\n",
      "        df_new_records = self._filter_new_records(df_joined, df_bronze, df_silver)\n",
      "        df_updated_records = self._filter_updated_records(df_joined, df_bronze, updated_filter_condition)\n",
      "\n",
      "        # 4.\n",
      "        df_data_to_insert = df_new_records.unionByName(df_updated_records).select(target_columns_ordered).dropDuplicates([\"PK\"])\n",
      "\n",
      "        # 6.\n",
      "        if self._is_delta_load:\n",
      "            self._write_df(df_data_to_insert, \"append\")\n",
      "            self._create_or_replace_historized_mlv(has_schema_changed, target_columns_ordered)\n",
      "            return df_data_to_insert\n",
      "\n",
      "        df_deleted_records = self._filter_deleted_records(df_joined, df_bronze, df_silver).select(target_columns_ordered)\n",
      "        df_data_to_insert = df_data_to_insert.unionByName(df_deleted_records)\n",
      "        self._write_df(df_data_to_insert, \"append\")\n",
      "        self._create_or_replace_historized_mlv(has_schema_changed, target_columns_ordered)\n",
      "        return df_data_to_insert\n",
      "\n",
      "    def read_silver_df(self) -> DataFrame:\n",
      "        if self._is_testing_mock:\n",
      "            if not os.path.exists(get_mock_table_path(self._dest_table)):\n",
      "                return None\n",
      "        elif not self._spark.catalog.tableExists(self._dest_table.table_path):\n",
      "            return None\n",
      "\n",
      "        sql_select_destination = f\"SELECT * FROM {self._dest_table.table_path}\"\n",
      "        df = self._spark.sql(sql_select_destination) if not self._is_testing_mock else self._spark.read.format(\"delta\").load(get_mock_table_path(self._dest_table))\n",
      "        return df\n",
      "\n",
      "    def _generate_dataframes(self) -> tuple[DataFrame, DataFrame, bool]:\n",
      "        df_bronze = self._create_bronze_df()\n",
      "        df_bronze = self._apply_transformations(df_bronze)\n",
      "\n",
      "        df_silver = self._create_silver_df()\n",
      "\n",
      "        has_schema_changed = self._has_schema_change(df_bronze, df_silver)\n",
      "\n",
      "        if df_silver is None:\n",
      "            return df_bronze, df_silver, has_schema_changed\n",
      "\n",
      "        df_bronze = self._add_missing_columns(df_bronze, df_silver)\n",
      "        df_silver = self._add_missing_columns(df_silver, df_bronze)\n",
      "\n",
      "        if self._is_delta_load and self._delta_load_use_broadcast:\n",
      "            df_bronze = F.broadcast(df_bronze)\n",
      "\n",
      "        return df_bronze, df_silver, has_schema_changed\n",
      "\n",
      "    def _get_compare_condition(self, df_bronze: DataFrame, df_silver: DataFrame, columns_to_compare: list[str]):\n",
      "        eq_condition = (\n",
      "            (df_bronze[columns_to_compare[0]] == df_silver[columns_to_compare[0]]) |\n",
      "            (df_bronze[columns_to_compare[0]].isNull() & df_silver[columns_to_compare[0]].isNull())\n",
      "        )\n",
      "\n",
      "        if len(columns_to_compare) == 1:\n",
      "            return eq_condition, ~eq_condition\n",
      "\n",
      "        for compare_column in columns_to_compare[1:]:\n",
      "            eq_condition &= (\n",
      "                (df_bronze[compare_column] == df_silver[compare_column]) |\n",
      "                (df_bronze[compare_column].isNull() & df_silver[compare_column].isNull())\n",
      "            )\n",
      "\n",
      "        return eq_condition, ~eq_condition\n",
      "\n",
      "    def _get_updated_filter(self, df_bronze: DataFrame, df_silver: DataFrame, neq_condition):\n",
      "        updated_filter = (\n",
      "            (df_bronze[self._nk_column_name].isNotNull()) &\n",
      "            (df_silver[self._nk_column_name].isNotNull()) &\n",
      "            (neq_condition)\n",
      "        )\n",
      "\n",
      "        return updated_filter\n",
      "\n",
      "    def _filter_new_records(self, df_joined: DataFrame, df_bronze: DataFrame, df_silver: DataFrame) -> DataFrame:\n",
      "        new_records_filter = (df_silver[self._nk_column_name].isNull())\n",
      "        df_new_records = df_joined.filter(new_records_filter) \\\n",
      "                                  .select(df_bronze[\"*\"])\n",
      "\n",
      "        return df_new_records\n",
      "\n",
      "    def _filter_updated_records(self, df_joined: DataFrame, df_bronze: DataFrame, updated_filter) -> DataFrame:\n",
      "        # Select not matching bronze columns\n",
      "        df_updated_records = df_joined.filter(updated_filter) \\\n",
      "                                      .select(df_bronze[\"*\"])\n",
      "\n",
      "        return df_updated_records\n",
      "\n",
      "    def _filter_expired_records(self, df_joined: DataFrame, df_silver: DataFrame, updated_filter) -> DataFrame:\n",
      "        # Select not matching silver columns\n",
      "        df_expired_records = df_joined.filter(updated_filter) \\\n",
      "                                      .select(df_silver[\"*\"]) \\\n",
      "                                      .withColumn(self._row_delete_dts_column, F.lit(None).cast(\"timestamp\"))\n",
      "\n",
      "        return df_expired_records\n",
      "\n",
      "    def _filter_deleted_records(self, df_joined: DataFrame, df_bronze: DataFrame, df_silver: DataFrame) -> DataFrame:\n",
      "        df_deleted_records = df_joined.filter((df_bronze[self._nk_column_name].isNull()) & (df_silver[self._row_delete_dts_column].isNull())) \\\n",
      "                                      .select(df_silver[\"*\"]) \\\n",
      "                                      .withColumn(self._pk_column_name, generate_uuid()) \\\n",
      "                                      .withColumn(self._row_delete_dts_column, F.lit(self._current_timestamp)) \\\n",
      "                                      .withColumn(self._row_load_dts_column, F.lit(self._current_timestamp))\n",
      "\n",
      "        return df_deleted_records\n",
      "\n",
      "    def _create_bronze_df(self) -> DataFrame:\n",
      "        sql_select_source = f\"SELECT * FROM {self._src_table.table_path}\"\n",
      "        if isinstance(self._df_bronze, DataFrame):\n",
      "            df = self._df_bronze\n",
      "        elif not self._is_testing_mock:\n",
      "            df = self._spark.sql(sql_select_source)\n",
      "        else:\n",
      "            df = self._spark.read.format(\"parquet\").load(get_mock_table_path(self._src_table))\n",
      "\n",
      "        self._validate_nk_columns_in_df(df)\n",
      "\n",
      "        for constant_column in self._constant_columns:\n",
      "            if constant_column.name not in df.columns:\n",
      "                df = df.withColumn(constant_column.name, F.lit(constant_column.value))\n",
      "\n",
      "        df = df.withColumn(self._pk_column_name, generate_uuid())  \\\n",
      "               .withColumn(self._nk_column_name, F.concat_ws(self._nk_column_concate_str, *self._nk_columns)) \\\n",
      "               .withColumn(self._row_delete_dts_column, F.lit(None).cast(\"timestamp\")) \\\n",
      "               .withColumn(self._row_load_dts_column, F.lit(self._current_timestamp))\n",
      "\n",
      "        return df\n",
      "\n",
      "    def _create_silver_df(self) -> DataFrame:\n",
      "        if self._is_testing_mock:\n",
      "            if not os.path.exists(get_mock_table_path(self._dest_table)):\n",
      "                return None\n",
      "        elif not self._spark.catalog.tableExists(self._dest_table.table_path):\n",
      "            return None\n",
      "\n",
      "        sql_select_destination = f\"SELECT * FROM {self._dest_table.table_path}\"\n",
      "        df = self._spark.sql(sql_select_destination) if not self._is_testing_mock else self._spark.read.format(\"parquet\").load(get_mock_table_path(self._dest_table))\n",
      "\n",
      "        self._validate_nk_columns_in_df(df)\n",
      "\n",
      "        for constant_column in self._constant_columns:\n",
      "            if constant_column.name not in df.columns:\n",
      "                df = df.withColumn(constant_column.name, F.lit(None))\n",
      "\n",
      "            if constant_column.part_of_nk:\n",
      "                df = df.filter(F.col(constant_column.name) == constant_column.value)\n",
      "\n",
      "        df = df.withColumn(self._nk_column_name, F.concat_ws(self._nk_column_concate_str, *self._nk_columns))\n",
      "\n",
      "        return_columns = df.columns\n",
      "\n",
      "        window_spec = Window.partitionBy(self._nk_columns).orderBy(df[self._row_load_dts_column].desc())\n",
      "        df_with_rownum = df.withColumn(\"ROW_NUMBER\", F.row_number().over(window_spec))\n",
      "        df = df_with_rownum.filter(df_with_rownum[\"ROW_NUMBER\"] == 1).select(return_columns)\n",
      "\n",
      "        return df\n",
      "\n",
      "    def _add_missing_columns(self, df_target: DataFrame, df_source: DataFrame) -> DataFrame:\n",
      "        missing_columns = [missing_column for missing_column in df_source.columns if missing_column not in df_target.columns]\n",
      "\n",
      "        for missing_column in missing_columns:\n",
      "            df_target = df_target.withColumn(missing_column, F.lit(None))\n",
      "\n",
      "        return df_target\n",
      "\n",
      "    def _get_columns_to_compare(self, df: DataFrame) -> list[str]:\n",
      "        if isinstance(self._include_comparing_columns, list) and len(self._include_comparing_columns) >= 1:\n",
      "            self._validate_include_comparing_columns(df)\n",
      "            return self._include_comparing_columns\n",
      "\n",
      "        comparison_columns = [column for column in df.columns if column not in self._exclude_comparing_columns]\n",
      "\n",
      "        return comparison_columns\n",
      "\n",
      "    def _get_columns_ordered(self, df: DataFrame) -> list[str]:\n",
      "        all_columns = [column for column in df.columns if column not in self._dw_columns]\n",
      "\n",
      "        return [self._pk_column_name, self._nk_column_name] + all_columns + [\n",
      "            self._row_load_dts_column,\n",
      "            self._row_delete_dts_column\n",
      "        ]\n",
      "\n",
      "    def _apply_transformations(self, df: DataFrame) -> DataFrame:\n",
      "        transform_fn: Callable = self._transformations.get(self._src_table.table)\n",
      "        transform_fn_all: Callable = self._transformations.get(\"*\")\n",
      "\n",
      "        if transform_fn_all is not None:\n",
      "            df = transform_fn_all(df, self)\n",
      "\n",
      "        if transform_fn is None:\n",
      "            return df\n",
      "\n",
      "        return transform_fn(df, self)\n",
      "\n",
      "    def _create_or_replace_historized_mlv(\n",
      "            self,\n",
      "            has_schema_changed: bool,\n",
      "            target_columns_ordered: list[str]\n",
      "    ) -> None:\n",
      "        if not has_schema_changed:\n",
      "            print(\"MLV: No schema change detected.\")\n",
      "            return\n",
      "        self._drop_historized_mlv()\n",
      "        self._create_historized_mlv(target_columns_ordered)\n",
      "\n",
      "    def _create_historized_mlv(self, target_columns_ordered: list[str]) -> None:\n",
      "        print(f\"MLV: CREATE MLV {self.mlv_name}\")\n",
      "        if not self._is_create_hist_mlv:\n",
      "            return\n",
      "\n",
      "        last_columns_ordered = [\n",
      "            self._row_is_current_column,\n",
      "            self._row_hist_number_column,\n",
      "            self._row_update_dts_column,\n",
      "            self._row_delete_dts_column,\n",
      "            self._row_load_dts_column\n",
      "        ]\n",
      "\n",
      "        silver_columns_ordered_str = \",\\n\".join([f\"`{column}`\" for column in target_columns_ordered])\n",
      "\n",
      "        final_ordered_columns = [\n",
      "            column\n",
      "            for column in target_columns_ordered\n",
      "            if column not in last_columns_ordered\n",
      "        ] + last_columns_ordered\n",
      "\n",
      "        final_ordered_columns_str = \",\\n\".join([f\"`{column}`\" for column in final_ordered_columns])\n",
      "\n",
      "        assert len(set(final_ordered_columns)) == len(final_ordered_columns), \\\n",
      "               f\"Duplicate columns found in final ordered columns {final_ordered_columns_str}.\"\n",
      "\n",
      "        # TODO: wenn ConstantColumns mit part_of_nk, dann muss hier noch nach partitioniert werden!\n",
      "        constant_column_str = \"\"\n",
      "        for constant_column in self._constant_columns:\n",
      "            if constant_column.part_of_nk:\n",
      "                constant_column_str = f\", `{constant_column.name}`\"\n",
      "                break\n",
      "\n",
      "        self._mlv_code = f\"\"\"\n",
      "CREATE MATERIALIZED LAKE VIEW {self.mlv_name}\n",
      "AS\n",
      "WITH cte_mlv AS (\n",
      "    SELECT\n",
      "        {silver_columns_ordered_str}\n",
      "        ,LEAD({self._row_load_dts_column}) OVER (PARTITION BY {self._nk_column_name} {constant_column_str} ORDER BY {self._row_load_dts_column} DESC) AS {self._row_update_dts_column}\n",
      "        ,ROW_NUMBER() OVER (PARTITION BY {self._nk_column_name} {constant_column_str} ORDER BY {self._row_load_dts_column} DESC) AS {self._row_hist_number_column}\n",
      "    FROM {self._dest_table.table_path}\n",
      "), cte_mlv_final AS (\n",
      "    SELECT\n",
      "        *\n",
      "        ,IIF({self._row_hist_number_column} = 1 AND {self._row_delete_dts_column} IS NULL, 1, 0) AS {self._row_is_current_column}\n",
      ")\n",
      "SELECT\n",
      "{final_ordered_columns_str}\n",
      "FROM cte_mlv\n",
      "\"\"\"\n",
      "        if self._is_testing_mock:\n",
      "            return\n",
      "\n",
      "        self._spark.sql(self._mlv_code)\n",
      "\n",
      "    def _drop_historized_mlv(self) -> None:\n",
      "        drop_mlv_sql = f\"DROP MATERIALIZED LAKE VIEW IF EXISTS {self.mlv_name}\"\n",
      "        print(drop_mlv_sql)\n",
      "\n",
      "        if self._is_testing_mock:\n",
      "            return\n",
      "\n",
      "        self._spark.sql(drop_mlv_sql)\n",
      "\n",
      "    def _refresh_historized_mlv(self) -> None:\n",
      "        refresh_mlv_sql = f\"REFRESH MATERIALIZED LAKE VIEW {self.mlv_name}\"\n",
      "        print(refresh_mlv_sql)\n",
      "\n",
      "        if self._is_testing_mock:\n",
      "            return\n",
      "\n",
      "        self._spark.sql(refresh_mlv_sql)\n",
      "\n",
      "    def _has_schema_change(self, df_bronze: DataFrame, df_silver: DataFrame) -> bool:\n",
      "        \"\"\"Check if the schema of the bronze DataFrame is different from the silver DataFrame.\n",
      "\n",
      "        Args:\n",
      "            df_bronze (DataFrame): Bronze DataFrame.\n",
      "            df_silver (DataFrame): Silver DataFrame.\n",
      "\n",
      "        Returns:\n",
      "            bool: True if the schema has changed, False otherwise.\n",
      "        \"\"\"\n",
      "        if df_silver is None:\n",
      "            return True\n",
      "        return set(df_bronze.columns) != set(df_silver.columns)\n",
      "\n",
      "    def _write_df(self, df: DataFrame, write_mode: str) -> None:\n",
      "        writer = df.write \\\n",
      "            .format(\"delta\") \\\n",
      "            .mode(write_mode) \\\n",
      "            .option(\"mergeSchema\", \"true\") \\\n",
      "            .partitionBy(*self._partition_by)\n",
      "\n",
      "        if self._is_testing_mock:\n",
      "            writer.save(get_mock_table_path(self._dest_table))\n",
      "            return\n",
      "\n",
      "        writer.saveAsTable(self._dest_table.table_path)\n",
      "\n",
      "\n",
      "etl = SilverIngestionInsertOnlyService()\n"
     ]
    }
   ],
   "source": [
    "code = import_module(\"transform.silver.insertonly\", VERSION)\n",
    "\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d24a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./code.py\", \"w\") as f:\n",
    "    f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e928341",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb3e8d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SilverIngestionInsertOnlyService at 0x10a544980>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40d48cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LakehouseTable(lakehouse='test_lakehouse', schema='test_schema', table='test_table')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = LakehouseTable(\n",
    "    lakehouse=\"test_lakehouse\",\n",
    "    schema=\"test_schema\",\n",
    "    table=\"test_table\"\n",
    ")\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ff41e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import Any\n",
      "from pyspark.sql import DataFrame, SparkSession\n",
      "\n",
      "\n",
      "def to_spark_sql(sql: str) -> str:\n",
      "    return sql \\\n",
      "            .replace(\"[\", \"`\") \\\n",
      "            .replace(\"]\", \"`\")\n",
      "\n",
      "\n",
      "class MaterializedLakeView:\n",
      "    def __init__(\n",
      "        self,\n",
      "        lakehouse: str = None,\n",
      "        schema: str = None,\n",
      "        table: str = None,\n",
      "        table_suffix: str = \"_mlv\",\n",
      "        spark_: SparkSession = None,\n",
      "        notebookutils_: Any = None,\n",
      "        is_testing_mock: bool = False\n",
      "    ) -> None:\n",
      "        self.init(\n",
      "            lakehouse=lakehouse,\n",
      "            schema=schema,\n",
      "            table=table,\n",
      "            table_suffix=table_suffix,\n",
      "            spark_=spark_,\n",
      "            notebookutils_=notebookutils_,\n",
      "            is_testing_mock=is_testing_mock\n",
      "        )\n",
      "\n",
      "    def init(\n",
      "        self,\n",
      "        lakehouse: str,\n",
      "        schema: str,\n",
      "        table: str,\n",
      "        table_suffix: str = \"_mlv\",\n",
      "        spark_: SparkSession = None,\n",
      "        notebookutils_: Any = None,\n",
      "        is_testing_mock: bool = False\n",
      "    ) -> 'MaterializedLakeView':\n",
      "        self._lakehouse = lakehouse\n",
      "        self._schema = schema\n",
      "        self._table = table\n",
      "        self._table_suffix = table_suffix\n",
      "        self._is_testing_mock = is_testing_mock\n",
      "\n",
      "        # 'spark' and 'notebookutils' are available in Fabric Notebook\n",
      "        self._spark = self._get_init_spark(spark_)\n",
      "        self._notebookutils = self._get_init_notebookutils(notebookutils_)\n",
      "        return self\n",
      "\n",
      "    def _get_init_spark(self, spark_: SparkSession) -> SparkSession | None:\n",
      "        if isinstance(spark_, SparkSession):\n",
      "            return spark_\n",
      "        try:\n",
      "            if spark is not None:  # noqa: F821 # type: ignore\n",
      "                return spark  # noqa: F821 # type: ignore\n",
      "            return spark_\n",
      "        except Exception:\n",
      "            return None\n",
      "\n",
      "    def _get_init_notebookutils(self, notebookutils_: Any) -> Any | None:\n",
      "        if notebookutils_ is not None:\n",
      "            return notebookutils_\n",
      "        try:\n",
      "            if notebookutils is not None:  # noqa: F821 # type: ignore\n",
      "                return notebookutils  # noqa: F821 # type: ignore\n",
      "            return None\n",
      "        except Exception:\n",
      "            return None\n",
      "\n",
      "    @property\n",
      "    def lakehouse(self) -> str:\n",
      "        if self._lakehouse is None:\n",
      "            raise ValueError(\"Lakehouse is not initialized.\")\n",
      "        return self._lakehouse\n",
      "\n",
      "    @property\n",
      "    def schema(self) -> str:\n",
      "        if self._schema is None:\n",
      "            raise ValueError(\"Schema is not initialized.\")\n",
      "        return self._schema\n",
      "\n",
      "    @property\n",
      "    def table(self) -> str:\n",
      "        if self._table is None:\n",
      "            raise ValueError(\"Table is not initialized.\")\n",
      "        return self._table\n",
      "\n",
      "    @property\n",
      "    def table_suffix(self) -> str:\n",
      "        return self._table_suffix\n",
      "\n",
      "    @property\n",
      "    def spark(self) -> SparkSession:\n",
      "        if self._spark is None:\n",
      "            raise ValueError(\"SparkSession is not initialized.\")\n",
      "        return self._spark\n",
      "\n",
      "    @property\n",
      "    def notebookutils(self) -> Any:\n",
      "        if self._notebookutils is None:\n",
      "            raise ValueError(\"NotebookUtils is not initialized.\")\n",
      "        return self._notebookutils\n",
      "\n",
      "    @property\n",
      "    def table_name(self) -> str:\n",
      "        table_suffix = self.table_suffix or \"\"\n",
      "        return f\"{self.table}{table_suffix}\"\n",
      "\n",
      "    @property\n",
      "    def file_path(self) -> str:\n",
      "        path = f\"Files/mlv/{self.lakehouse}/{self.schema}/{self.table_name}.sql.txt\"\n",
      "        return path\n",
      "\n",
      "    @property\n",
      "    def table_path(self) -> str:\n",
      "        table_path = f\"{self.lakehouse}.{self.schema}.{self.table_name}\"\n",
      "        return table_path\n",
      "\n",
      "    @property\n",
      "    def schema_path(self) -> str:\n",
      "        schema_path = f\"{self.lakehouse}.{self.schema}\"\n",
      "        return schema_path\n",
      "\n",
      "    def read_file(self) -> str | None:\n",
      "        path = self.file_path\n",
      "        try:\n",
      "            if not self.notebookutils.fs.exists(path):\n",
      "                return None\n",
      "            if self._is_testing_mock:\n",
      "                with open(path, 'r') as file:\n",
      "                    return file.read()\n",
      "            df = self.spark.read.text(path, wholetext=True)\n",
      "            mlv_code = df.collect()[0][0]\n",
      "            return mlv_code\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(f\"Fehler beim Lesen der Datei: {e}\")\n",
      "\n",
      "    def write_file(self, sql: str) -> bool:\n",
      "        try:\n",
      "            result = self.notebookutils.fs.put(\n",
      "                file=self.file_path,\n",
      "                content=sql,\n",
      "                overwrite=True\n",
      "            )\n",
      "            return result\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(f\"Fehler beim Schreiben der Datei: {e}\")\n",
      "\n",
      "    def create_schema(self) -> None:\n",
      "        create_schema = f\"CREATE SCHEMA IF NOT EXISTS {self.schema_path}\"\n",
      "        print(create_schema)\n",
      "\n",
      "        if self._is_testing_mock:\n",
      "            return None\n",
      "\n",
      "        return self.spark.sql(create_schema)\n",
      "\n",
      "    def create(self, sql: str) -> DataFrame:\n",
      "        create_mlv = f\"CREATE MATERIALIZED LAKE VIEW {self.table_path}\\nAS\\n{sql}\"\n",
      "\n",
      "        self.create_schema()\n",
      "        print(f\"CREATE MLV: {self.table_path}\")\n",
      "        if self._is_testing_mock:\n",
      "            return None\n",
      "\n",
      "        return self.spark.sql(create_mlv)\n",
      "\n",
      "    def drop(self) -> str:\n",
      "        drop_mlv = f\"DROP MATERIALIZED LAKE VIEW IF EXISTS {self.table_path}\"\n",
      "        print(drop_mlv)\n",
      "\n",
      "        if self._is_testing_mock:\n",
      "            return None\n",
      "\n",
      "        return self.spark.sql(drop_mlv)\n",
      "\n",
      "    def create_or_replace(self, sql: str, mock_is_existing: bool = None) -> DataFrame:\n",
      "        mlv_code_current = self.read_file()\n",
      "        is_existing = (\n",
      "            mock_is_existing\n",
      "            if mock_is_existing is not None\n",
      "            else self.spark.catalog.tableExists(self.table_path)\n",
      "        )\n",
      "\n",
      "        if mlv_code_current is None and not is_existing:\n",
      "            res = self.create(sql)\n",
      "            self.write_file(sql)\n",
      "            return res\n",
      "\n",
      "        elif mlv_code_current is None and is_existing:\n",
      "            print(\"WARN: file=None, is_existing=True. RECREATE.\")\n",
      "            self.drop()\n",
      "            res = self.create(sql)\n",
      "            self.write_file(sql)\n",
      "            return res\n",
      "\n",
      "        elif sql == mlv_code_current and is_existing:\n",
      "            print(\"Nothing has changed.\")\n",
      "            return None\n",
      "\n",
      "        print(f\"REPLACE MLV: {self.table_path}\")\n",
      "        self.drop()\n",
      "        res = self.create(sql)\n",
      "        self.write_file(sql)\n",
      "        return res\n",
      "\n",
      "    def refresh(self, full_refresh: bool) -> DataFrame:\n",
      "        full_refresh_str = \"FULL\" if full_refresh else \"\"\n",
      "        refresh_mlv = f\"REFRESH MATERIALIZED LAKE VIEW {self.table_path} {full_refresh_str}\"\n",
      "        print(refresh_mlv)\n",
      "\n",
      "        if self._is_testing_mock:\n",
      "            return None\n",
      "\n",
      "        return self.spark.sql(refresh_mlv)\n",
      "\n",
      "    def to_dict(self) -> None:\n",
      "        return {\n",
      "            \"lakehouse\": self.lakehouse,\n",
      "            \"schema\": self.schema,\n",
      "            \"table\": self.table,\n",
      "            \"table_path\": self.table_path\n",
      "        }\n",
      "\n",
      "\n",
      "mlv = MaterializedLakeView()\n"
     ]
    }
   ],
   "source": [
    "code = import_module(\"transform.silver.mlv\", VERSION)\n",
    "\n",
    "print(code, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "637aeca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(code, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7b8cfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLACE MLV: TestLakehouse.schema.table_mlv\n",
      "DROP MATERIALIZED LAKE VIEW IF EXISTS TestLakehouse.schema.table_mlv\n",
      "CREATE SCHEMA IF NOT EXISTS TestLakehouse.schema\n",
      "CREATE MLV: TestLakehouse.schema.table_mlv\n"
     ]
    }
   ],
   "source": [
    "mlv.init(\n",
    "    lakehouse=\"TestLakehouse\",\n",
    "    schema=\"schema\",\n",
    "    table=\"table\",\n",
    "    is_testing_mock=True\n",
    ")\n",
    "\n",
    "sql = \"SELECT * FROM TestLakehouse.schema.table2\"\n",
    "mlv.create_or_replace(sql, mock_is_existing=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
