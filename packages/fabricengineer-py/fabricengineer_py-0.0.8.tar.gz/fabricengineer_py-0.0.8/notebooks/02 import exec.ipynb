{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43180ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from fabricengineer.import_module.import_module import import_module\n",
    "\n",
    "VERSION = \"0.0.8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3602f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "\n",
      "from datetime import datetime\n",
      "from typing import Callable\n",
      "from abc import ABC, abstractmethod\n",
      "from dataclasses import dataclass\n",
      "from uuid import uuid4\n",
      "\n",
      "from pyspark.sql import (\n",
      "    SparkSession,\n",
      "    DataFrame,\n",
      "    functions as F,\n",
      "    types as T,\n",
      "    Window\n",
      ")\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class LakehouseTable:\n",
      "    lakehouse: str\n",
      "    schema: str\n",
      "    table: str\n",
      "\n",
      "    @property\n",
      "    def table_path(self) -> str:\n",
      "        return f\"{self.lakehouse}.{self.schema}.{self.table}\"\n",
      "\n",
      "\n",
      "@F.udf(returnType=T.StringType())\n",
      "def generate_uuid():\n",
      "    \"\"\"Generiert eine UUID4\"\"\"\n",
      "    return str(uuid4())\n",
      "\n",
      "\n",
      "@dataclass(frozen=True)\n",
      "class ConstantColumn:\n",
      "    \"\"\"Class for adding a column with constant value to etl\"\"\"\n",
      "    name: str\n",
      "    value: str\n",
      "    part_of_nk: bool = False\n",
      "\n",
      "    def __post_init__(self):\n",
      "        \"\"\"\n",
      "        Nach initialisierung wird der name in UPPERCASE umgewandelt.\n",
      "        \"\"\"\n",
      "        object.__setattr__(self, \"name\", self.name.upper())\n",
      "\n",
      "\n",
      "def get_mock_table_path(table: LakehouseTable) -> str:\n",
      "    if table is None:\n",
      "        raise ValueError(\"Table is not initialized.\")\n",
      "    table_path = table.table_path.replace(\".\", \"/\")\n",
      "    full_table_path = f\"tmp/lakehouse/{table_path}\"\n",
      "    return full_table_path\n",
      "\n",
      "\n",
      "class BaseSilverIngestionService(ABC):\n",
      "    @abstractmethod\n",
      "    def init(self, **kwargs): pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def ingest(self, **kwargs): pass\n",
      "\n",
      "\n",
      "class SilverIngestionInsertOnlyService(BaseSilverIngestionService):\n",
      "    _is_initialized: bool = False\n",
      "\n",
      "    def init(\n",
      "        self,\n",
      "        *,\n",
      "        spark_: SparkSession,\n",
      "        source_table: LakehouseTable,\n",
      "        destination_table: LakehouseTable,\n",
      "        nk_columns: list[str],\n",
      "        constant_columns: list[ConstantColumn],\n",
      "        is_delta_load: bool,\n",
      "        delta_load_use_broadcast: bool,\n",
      "        transformations: dict,\n",
      "        exclude_comparing_columns: list[str] | None = None,\n",
      "        include_comparing_columns: list[str] | None = None,\n",
      "        historize: bool = True,\n",
      "        partition_by_columns: list[str] = None,\n",
      "        df_bronze: DataFrame = None,\n",
      "        create_historized_mlv: bool = True,\n",
      "        mlv_suffix: str = \"_h\",\n",
      "\n",
      "        pk_column_name: str = \"PK\",\n",
      "        nk_column_name: str = \"NK\",\n",
      "        nk_column_concate_str: str = \"_\",\n",
      "        row_is_current_column: str = \"ROW_IS_CURRENT\",\n",
      "        row_hist_number_column: str = \"ROW_HIST_NUMBER\",\n",
      "        row_update_dts_column: str = \"ROW_UPDATE_DTS\",\n",
      "        row_delete_dts_column: str = \"ROW_DELETE_DTS\",\n",
      "        row_load_dts_column: str = \"ROW_LOAD_DTS\",\n",
      "\n",
      "        is_testing_mock: bool = False\n",
      "    ) -> None:\n",
      "        self._mlv_code = None\n",
      "\n",
      "        self._is_testing_mock = is_testing_mock\n",
      "        self._spark = spark_\n",
      "        self._df_bronze = df_bronze\n",
      "        self._historize = historize\n",
      "        self._is_create_hist_mlv = create_historized_mlv\n",
      "        self._mlv_suffix = mlv_suffix\n",
      "        self._is_delta_load = is_delta_load\n",
      "        self._delta_load_use_broadcast = delta_load_use_broadcast\n",
      "        self._src_table = source_table\n",
      "        self._dest_table = destination_table\n",
      "        self._nk_columns = nk_columns\n",
      "        self._include_comparing_columns = include_comparing_columns\n",
      "\n",
      "        self._exclude_comparing_columns = exclude_comparing_columns or []\n",
      "        self._transformations: dict[str, Callable] = transformations or {}\n",
      "        self._constant_columns: list[ConstantColumn] = constant_columns or []\n",
      "        self._partition_by: list[str] = partition_by_columns or []\n",
      "\n",
      "        self._pk_column_name = pk_column_name\n",
      "        self._nk_column_name = nk_column_name\n",
      "        self._nk_column_concate_str = nk_column_concate_str\n",
      "        self._row_hist_number_column = row_hist_number_column\n",
      "        self._row_is_current_column = row_is_current_column\n",
      "        self._row_update_dts_column = row_update_dts_column\n",
      "        self._row_delete_dts_column = row_delete_dts_column\n",
      "        self._row_load_dts_column = row_load_dts_column\n",
      "\n",
      "        self._validate_parameters()\n",
      "        self._set_spark_config()\n",
      "\n",
      "        self._dw_columns = [\n",
      "            self._pk_column_name,\n",
      "            self._nk_column_name,\n",
      "            self._row_delete_dts_column,\n",
      "            self._row_load_dts_column\n",
      "        ]\n",
      "\n",
      "        self._exclude_comparing_columns = set(\n",
      "            [self._pk_column_name]\n",
      "            + self._nk_columns\n",
      "            + self._dw_columns\n",
      "            + self._exclude_comparing_columns\n",
      "            + [column.name for column in self._constant_columns]\n",
      "        )\n",
      "\n",
      "        self._spark.catalog.clearCache()\n",
      "        self._is_initialized = True\n",
      "\n",
      "    @property\n",
      "    def mlv_name(self) -> str:\n",
      "        return f\"{self._dest_table.table_path}{self._mlv_suffix}\"\n",
      "\n",
      "    @property\n",
      "    def mlv_code(self) -> str:\n",
      "        return self._mlv_code\n",
      "\n",
      "    def __str__(self) -> str:\n",
      "        if not self._is_initialized:\n",
      "            return super.__str__(self)\n",
      "\n",
      "        return str({\n",
      "            \"historize\": self._historize,\n",
      "            \"is_delta_load\": self._is_delta_load,\n",
      "            \"delta_load_use_broadcast\": self._delta_load_use_broadcast,\n",
      "            \"src_table_path\": self._src_table.table_path,\n",
      "            \"dist_table_path\": self._dest_table.table_path,\n",
      "            \"nk_columns\": self._nk_columns,\n",
      "            \"include_comparing_columns\": self._include_comparing_columns,\n",
      "            \"exclude_comparing_columns\": self._exclude_comparing_columns,\n",
      "            \"transformations\": self._transformations,\n",
      "            \"constant_columns\": self._constant_columns,\n",
      "            \"partition_by\": self._partition_by,\n",
      "            \"pk_column\": self._pk_column_name,\n",
      "            \"nk_column\": self._nk_column_name,\n",
      "            \"nk_column_concate_str\": self._nk_column_concate_str,\n",
      "            \"row_update_dts_column\": self._row_update_dts_column,\n",
      "            \"row_delete_dts_column\": self._row_delete_dts_column,\n",
      "            \"ldts_column\": self._row_load_dts_column,\n",
      "            \"dw_columns\": self._dw_columns\n",
      "        })\n",
      "\n",
      "    def _validate_parameters(self) -> None:\n",
      "        \"\"\"Validates the in constructor setted parameters, so the etl can run.\n",
      "\n",
      "        Raises:\n",
      "            ValueError: when a valueerror occurs\n",
      "            TypeError: when a typerror occurs\n",
      "            Exception: generic exception\n",
      "        \"\"\"\n",
      "\n",
      "        if self._df_bronze is not None:\n",
      "            self._validate_param_isinstance(self._df_bronze, \"df_bronze\", DataFrame)\n",
      "\n",
      "        self._validate_param_isinstance(self._spark, \"spark\", SparkSession)\n",
      "        self._validate_param_isinstance(self._historize, \"historize\", bool)\n",
      "        self._validate_param_isinstance(self._is_create_hist_mlv, \"create_historized_mlv\", bool)\n",
      "        self._validate_param_isinstance(self._is_delta_load, \"is_delta_load\", bool)\n",
      "        self._validate_param_isinstance(self._delta_load_use_broadcast, \"delta_load_use_broadcast\", bool)\n",
      "        self._validate_param_isinstance(self._transformations, \"transformations\", dict)\n",
      "        self._validate_param_isinstance(self._src_table, \"src_table\", LakehouseTable)\n",
      "        self._validate_param_isinstance(self._dest_table, \"dest_table\", LakehouseTable)\n",
      "        self._validate_param_isinstance(self._include_comparing_columns, \"include_columns_from_comparing\", list)\n",
      "        self._validate_param_isinstance(self._exclude_comparing_columns, \"exclude_columns_from_comparing\", list)\n",
      "        self._validate_param_isinstance(self._partition_by, \"partition_by_columns\", list)\n",
      "        self._validate_param_isinstance(self._pk_column_name, \"pk_column\", str)\n",
      "        self._validate_param_isinstance(self._nk_column_name, \"nk_column\", str)\n",
      "        self._validate_param_isinstance(self._nk_columns, \"nk_columns\", list)\n",
      "        self._validate_param_isinstance(self._nk_column_concate_str, \"nk_column_concate_str\", str)\n",
      "        self._validate_param_isinstance(self._mlv_suffix, \"mlv_suffix\", str)\n",
      "        self._validate_param_isinstance(self._constant_columns, \"constant_columns\", list)\n",
      "        self._validate_param_isinstance(self._row_load_dts_column, \"row_load_dts_column\", str)\n",
      "        self._validate_param_isinstance(self._row_hist_number_column, \"row_hist_number_column\", str)\n",
      "        self._validate_param_isinstance(self._row_is_current_column, \"row_is_current_column\", str)\n",
      "        self._validate_param_isinstance(self._row_update_dts_column, \"row_update_dts_column\", str)\n",
      "        self._validate_param_isinstance(self._row_delete_dts_column, \"row_delete_dts_column\", str)\n",
      "\n",
      "        self._validate_min_length(self._pk_column_name, \"pk_column\", 2)\n",
      "        self._validate_min_length(self._nk_column_name, \"nk_column\", 2)\n",
      "        self._validate_min_length(self._src_table.lakehouse, \"src_lakehouse\", 3)\n",
      "        self._validate_min_length(self._src_table.schema, \"src_schema\", 1)\n",
      "        self._validate_min_length(self._src_table.table, \"src_tablename\", 3)\n",
      "        self._validate_min_length(self._dest_table.lakehouse, \"dest_lakehouse\", 3)\n",
      "        self._validate_min_length(self._dest_table.schema, \"dest_schema\", 1)\n",
      "        self._validate_min_length(self._dest_table.table, \"dest_tablename\", 3)\n",
      "        self._validate_min_length(self._nk_columns, \"nk_columns\", 1)\n",
      "        self._validate_min_length(self._nk_column_concate_str, \"nk_column_concate_str\", 1)\n",
      "        self._validate_min_length(self._mlv_suffix, \"mlv_suffix\", 1)\n",
      "        self._validate_min_length(self._row_load_dts_column, \"row_load_dts_column\", 3)\n",
      "        self._validate_min_length(self._row_hist_number_column, \"row_hist_number_column\", 3)\n",
      "        self._validate_min_length(self._row_is_current_column, \"row_is_current_column\", 3)\n",
      "        self._validate_min_length(self._row_update_dts_column, \"row_update_dts_column\", 3)\n",
      "        self._validate_min_length(self._row_delete_dts_column, \"row_delete_dts_column\", 3)\n",
      "\n",
      "        for key, fn in self._transformations.items():\n",
      "            print(\"Transformation function:\", fn)\n",
      "            if not callable(fn):\n",
      "                err_msg = f\"The transformation function for key '{key}' is not callable.\"\n",
      "                raise TypeError(err_msg)\n",
      "\n",
      "        for constant_column in self._constant_columns:\n",
      "            self._validate_param_isinstance(constant_column, \"constant_column\", ConstantColumn)\n",
      "\n",
      "    def _validate_param_isinstance(self, param, param_name: str, obj_class) -> None:\n",
      "        \"\"\"Validates a parameter to be the expected class instance\n",
      "\n",
      "        Args:\n",
      "            param (any): parameter\n",
      "            param_name (str): parametername\n",
      "            obj_class (_type_): class\n",
      "\n",
      "        Raises:\n",
      "            TypeError: when actual type is different from expected type\n",
      "        \"\"\"\n",
      "        if not isinstance(param, obj_class):\n",
      "            err_msg = f\"The param '{param_name}' should be type of {obj_class.__name__}, but was {str(param.__class__)}\"\n",
      "            raise TypeError(err_msg)\n",
      "\n",
      "    def _validate_min_length(self, param, param_name: str, min_length: int) -> None:\n",
      "        \"\"\"Validates a string or list to be not none and has a minimum length\n",
      "\n",
      "        Args:\n",
      "            param (_type_): parameter\n",
      "            param_name (str): parametername\n",
      "            min_length (int): minimum lenght\n",
      "\n",
      "        Raises:\n",
      "            TypeError: when actual type is different from expected type\n",
      "            ValueError: when parametervalue is to short\n",
      "        \"\"\"\n",
      "        if not isinstance(param, str) and not isinstance(param, list):\n",
      "            err_msg = f\"The param '{param_name}' should be type of string or list, but was {str(param.__class__)}\"\n",
      "            raise TypeError(err_msg)\n",
      "\n",
      "        param_length = len(param)\n",
      "        if param_length < min_length:\n",
      "            err_msg = f\"Param length to short. The minimum length of the param '{param_name}' is {min_length} but was {param_length}\"\n",
      "            raise ValueError(err_msg)\n",
      "\n",
      "    def _validate_constant_columns(self) -> None:\n",
      "        \"\"\"Validates the given constant columns to be an instance of ConstantColumns and\n",
      "        list contains only one part_of_nk=True, because of the following filtering of the dataframe.\n",
      "\n",
      "        It should have just one part_of_nk=True, because the dataframe will filtered later by the\n",
      "        constant_column.name, if part_of_nk=True.\n",
      "        If part_of_nk=True should be supported more then once, then we need to implement\n",
      "        an \"and\" filtering.\n",
      "\n",
      "        Raises:\n",
      "            TypeError: when an item of the list is not an instance of ConstantColumn\n",
      "            ValueError: when list contains more then one ConstantColumn with part_of_nk=True\n",
      "        \"\"\"\n",
      "        nk_count = 0\n",
      "        for column in self._constant_columns:\n",
      "            if column.part_of_nk:\n",
      "                nk_count += 1\n",
      "\n",
      "            if not isinstance(column, ConstantColumn):\n",
      "                err_msg = \"Invalid items in constant_columns found. All items should be instance of ConstantColumn\"\n",
      "                raise TypeError(err_msg)\n",
      "\n",
      "            if nk_count > 1:\n",
      "                err_msg = \"In constant_columns are more then one part_of_nk=True, what is not supported!\"\n",
      "                raise ValueError(err_msg)\n",
      "\n",
      "    def _validate_nk_columns_in_df(self, df: DataFrame) -> None:\n",
      "        \"\"\"Validates the given dataframe. The given dataframe should contain all natural key columns,\n",
      "        because all natural key columns will selected and used for concatitation.\n",
      "\n",
      "        Args:\n",
      "            df (DataFrame): dataframe to validate\n",
      "\n",
      "        Raises:\n",
      "            ValueError: when dataframe does not contain all natural key columns\n",
      "        \"\"\"\n",
      "        df_columns = set(df.columns)\n",
      "        for column in self._nk_columns:\n",
      "            if column in df_columns:\n",
      "                continue\n",
      "\n",
      "            err_msg = f\"The NK Column '{column}' does not exist in df columns: {df_columns}\"\n",
      "            raise ValueError(err_msg)\n",
      "\n",
      "    def _validate_include_comparing_columns(self, df: DataFrame) -> None:\n",
      "        self._validate_param_isinstance(self._include_comparing_columns, \"include_comparing_columns\", list)\n",
      "\n",
      "        if len(self._include_comparing_columns) == 0:\n",
      "            err_msg = \"The param 'include_comparing_columns' is present, but don't contains any columns.\"\n",
      "            raise ValueError(err_msg)\n",
      "\n",
      "        for include_column in self._include_comparing_columns:\n",
      "            if include_column in df.columns:\n",
      "                continue\n",
      "\n",
      "            err_msg = f\"The column '{include_column}' should be compared, but is not given in df.\"\n",
      "            raise ValueError(err_msg)\n",
      "\n",
      "    def _validate_partition_by_columns(self, df: DataFrame) -> None:\n",
      "        self._validate_param_isinstance(self._partition_by, \"partition_by\", list)\n",
      "\n",
      "        for partition_column in self._partition_by:\n",
      "            if partition_column in df.columns:\n",
      "                continue\n",
      "\n",
      "            err_msg = f\"The column '{partition_column}' should be partitioned, but is not given in df.\"\n",
      "            raise ValueError(err_msg)\n",
      "\n",
      "    def _set_spark_config(self) -> None:\n",
      "        \"\"\"Sets additional spark configurations\n",
      "\n",
      "        spark.sql.parquet.vorder.enabled: Setting \"spark.sql.parquet.vorder.enabled\" to \"true\" in PySpark config enables a feature called vectorized parquet decoding.\n",
      "                                                  This optimizes the performance of reading Parquet files by leveraging vectorized instructions and processing multiple values at once, enhancing overall processing speed.\n",
      "\n",
      "        Setting \"spark.sql.parquet.int96RebaseModeInRead\" and \"spark.sql.legacy.parquet.int96RebaseModeInWrite\" to \"CORRECTED\" ensures that Int96 values (a specific timestamp representation used in Parquet files) are correctly rebased during both reading and writing operations.\n",
      "        This is crucial for maintaining consistency and accuracy, especially when dealing with timestamp data across different systems or time zones.\n",
      "        Similarly, configuring \"spark.sql.parquet.datetimeRebaseModeInRead\" and \"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\" to \"CORRECTED\" ensures correct handling of datetime values during Parquet file operations.\n",
      "        By specifying this rebasing mode, potential discrepancies or errors related to datetime representations are mitigated, resulting in more reliable data processing and analysis workflows.\n",
      "        \"\"\"\n",
      "        self._spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
      "\n",
      "        self._spark.conf.set(\"spark.sql.parquet.int96RebaseModeInRead\", \"CORRECTED\")\n",
      "        self._spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
      "        self._spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n",
      "        self._spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
      "\n",
      "    def ingest(self) -> DataFrame:\n",
      "        if not self._is_initialized:\n",
      "            raise RuntimeError(\"The SilverIngestionInsertOnlyService is not initialized. Call the init method first.\")\n",
      "\n",
      "        self._current_timestamp = datetime.now()\n",
      "\n",
      "        # 1.\n",
      "        df_bronze, df_silver, has_schema_changed = self._generate_dataframes()\n",
      "\n",
      "        target_columns_ordered = self._get_columns_ordered(df_bronze)\n",
      "\n",
      "        do_overwrite = (\n",
      "            df_silver is None or\n",
      "            (\n",
      "                not self._historize and\n",
      "                not self._is_delta_load\n",
      "                # If we are not historizing but performing a delta load,\n",
      "                # we need to update the silver-layer data.\n",
      "                # We should not overwrite the silver-layer data,\n",
      "                # because the delta load (bronze layer) do not contain all the data!\n",
      "            )\n",
      "        )\n",
      "        if do_overwrite:\n",
      "            df_inital_load = df_bronze.select(target_columns_ordered)\n",
      "            self._write_df(df_inital_load, \"overwrite\")\n",
      "            self._create_or_replace_historized_mlv(has_schema_changed, target_columns_ordered)\n",
      "            return df_inital_load\n",
      "\n",
      "        # 2.\n",
      "        columns_to_compare = self._get_columns_to_compare(df_bronze)\n",
      "\n",
      "        join_condition = (df_bronze[self._nk_column_name] == df_silver[self._nk_column_name])\n",
      "        df_joined = df_bronze.join(df_silver, join_condition, \"outer\")\n",
      "\n",
      "        # 3.\n",
      "        _, neq_condition = self._get_compare_condition(df_bronze, df_silver, columns_to_compare)\n",
      "        updated_filter_condition = self._get_updated_filter(df_bronze, df_silver, neq_condition)\n",
      "\n",
      "        df_new_records = self._filter_new_records(df_joined, df_bronze, df_silver)\n",
      "        df_updated_records = self._filter_updated_records(df_joined, df_bronze, updated_filter_condition)\n",
      "\n",
      "        # 4.\n",
      "        df_data_to_insert = df_new_records.unionByName(df_updated_records).select(target_columns_ordered).dropDuplicates([\"PK\"])\n",
      "\n",
      "        # 6.\n",
      "        if self._is_delta_load:\n",
      "            self._write_df(df_data_to_insert, \"append\")\n",
      "            self._create_or_replace_historized_mlv(has_schema_changed, target_columns_ordered)\n",
      "            return df_data_to_insert\n",
      "\n",
      "        df_deleted_records = self._filter_deleted_records(df_joined, df_bronze, df_silver).select(target_columns_ordered)\n",
      "        df_data_to_insert = df_data_to_insert.unionByName(df_deleted_records)\n",
      "        self._write_df(df_data_to_insert, \"append\")\n",
      "        self._create_or_replace_historized_mlv(has_schema_changed, target_columns_ordered)\n",
      "        return df_data_to_insert\n",
      "\n",
      "    def read_silver_df(self) -> DataFrame:\n",
      "        if self._is_testing_mock:\n",
      "            if not os.path.exists(get_mock_table_path(self._dest_table)):\n",
      "                return None\n",
      "        elif not self._spark.catalog.tableExists(self._dest_table.table_path):\n",
      "            return None\n",
      "\n",
      "        sql_select_destination = f\"SELECT * FROM {self._dest_table.table_path}\"\n",
      "        df = self._spark.sql(sql_select_destination) if not self._is_testing_mock else self._spark.read.format(\"delta\").load(get_mock_table_path(self._dest_table))\n",
      "        return df\n",
      "\n",
      "    def _generate_dataframes(self) -> tuple[DataFrame, DataFrame, bool]:\n",
      "        df_bronze = self._create_bronze_df()\n",
      "        df_bronze = self._apply_transformations(df_bronze)\n",
      "\n",
      "        df_silver = self._create_silver_df()\n",
      "\n",
      "        has_schema_changed = self._has_schema_change(df_bronze, df_silver)\n",
      "\n",
      "        if df_silver is None:\n",
      "            return df_bronze, df_silver, has_schema_changed\n",
      "\n",
      "        df_bronze = self._add_missing_columns(df_bronze, df_silver)\n",
      "        df_silver = self._add_missing_columns(df_silver, df_bronze)\n",
      "\n",
      "        if self._is_delta_load and self._delta_load_use_broadcast:\n",
      "            df_bronze = F.broadcast(df_bronze)\n",
      "\n",
      "        return df_bronze, df_silver, has_schema_changed\n",
      "\n",
      "    def _get_compare_condition(self, df_bronze: DataFrame, df_silver: DataFrame, columns_to_compare: list[str]):\n",
      "        eq_condition = (\n",
      "            (df_bronze[columns_to_compare[0]] == df_silver[columns_to_compare[0]]) |\n",
      "            (df_bronze[columns_to_compare[0]].isNull() & df_silver[columns_to_compare[0]].isNull())\n",
      "        )\n",
      "\n",
      "        if len(columns_to_compare) == 1:\n",
      "            return eq_condition, ~eq_condition\n",
      "\n",
      "        for compare_column in columns_to_compare[1:]:\n",
      "            eq_condition &= (\n",
      "                (df_bronze[compare_column] == df_silver[compare_column]) |\n",
      "                (df_bronze[compare_column].isNull() & df_silver[compare_column].isNull())\n",
      "            )\n",
      "\n",
      "        return eq_condition, ~eq_condition\n",
      "\n",
      "    def _get_updated_filter(self, df_bronze: DataFrame, df_silver: DataFrame, neq_condition):\n",
      "        updated_filter = (\n",
      "            (df_bronze[self._nk_column_name].isNotNull()) &\n",
      "            (df_silver[self._nk_column_name].isNotNull()) &\n",
      "            (neq_condition)\n",
      "        )\n",
      "\n",
      "        return updated_filter\n",
      "\n",
      "    def _filter_new_records(self, df_joined: DataFrame, df_bronze: DataFrame, df_silver: DataFrame) -> DataFrame:\n",
      "        new_records_filter = (df_silver[self._nk_column_name].isNull())\n",
      "        df_new_records = df_joined.filter(new_records_filter) \\\n",
      "                                  .select(df_bronze[\"*\"])\n",
      "\n",
      "        return df_new_records\n",
      "\n",
      "    def _filter_updated_records(self, df_joined: DataFrame, df_bronze: DataFrame, updated_filter) -> DataFrame:\n",
      "        # Select not matching bronze columns\n",
      "        df_updated_records = df_joined.filter(updated_filter) \\\n",
      "                                      .select(df_bronze[\"*\"])\n",
      "\n",
      "        return df_updated_records\n",
      "\n",
      "    def _filter_expired_records(self, df_joined: DataFrame, df_silver: DataFrame, updated_filter) -> DataFrame:\n",
      "        # Select not matching silver columns\n",
      "        df_expired_records = df_joined.filter(updated_filter) \\\n",
      "                                      .select(df_silver[\"*\"]) \\\n",
      "                                      .withColumn(self._row_delete_dts_column, F.lit(None).cast(\"timestamp\"))\n",
      "\n",
      "        return df_expired_records\n",
      "\n",
      "    def _filter_deleted_records(self, df_joined: DataFrame, df_bronze: DataFrame, df_silver: DataFrame) -> DataFrame:\n",
      "        df_deleted_records = df_joined.filter((df_bronze[self._nk_column_name].isNull()) & (df_silver[self._row_delete_dts_column].isNull())) \\\n",
      "                                      .select(df_silver[\"*\"]) \\\n",
      "                                      .withColumn(self._pk_column_name, generate_uuid()) \\\n",
      "                                      .withColumn(self._row_delete_dts_column, F.lit(self._current_timestamp)) \\\n",
      "                                      .withColumn(self._row_load_dts_column, F.lit(self._current_timestamp))\n",
      "\n",
      "        return df_deleted_records\n",
      "\n",
      "    def _create_bronze_df(self) -> DataFrame:\n",
      "        sql_select_source = f\"SELECT * FROM {self._src_table.table_path}\"\n",
      "        if isinstance(self._df_bronze, DataFrame):\n",
      "            df = self._df_bronze\n",
      "        elif not self._is_testing_mock:\n",
      "            df = self._spark.sql(sql_select_source)\n",
      "        else:\n",
      "            df = self._spark.read.format(\"parquet\").load(get_mock_table_path(self._src_table))\n",
      "\n",
      "        self._validate_nk_columns_in_df(df)\n",
      "\n",
      "        for constant_column in self._constant_columns:\n",
      "            if constant_column.name not in df.columns:\n",
      "                df = df.withColumn(constant_column.name, F.lit(constant_column.value))\n",
      "\n",
      "        df = df.withColumn(self._pk_column_name, generate_uuid())  \\\n",
      "               .withColumn(self._nk_column_name, F.concat_ws(self._nk_column_concate_str, *self._nk_columns)) \\\n",
      "               .withColumn(self._row_delete_dts_column, F.lit(None).cast(\"timestamp\")) \\\n",
      "               .withColumn(self._row_load_dts_column, F.lit(self._current_timestamp))\n",
      "\n",
      "        return df\n",
      "\n",
      "    def _create_silver_df(self) -> DataFrame:\n",
      "        if self._is_testing_mock:\n",
      "            if not os.path.exists(get_mock_table_path(self._dest_table)):\n",
      "                return None\n",
      "        elif not self._spark.catalog.tableExists(self._dest_table.table_path):\n",
      "            return None\n",
      "\n",
      "        sql_select_destination = f\"SELECT * FROM {self._dest_table.table_path}\"\n",
      "        df = self._spark.sql(sql_select_destination) if not self._is_testing_mock else self._spark.read.format(\"parquet\").load(get_mock_table_path(self._dest_table))\n",
      "\n",
      "        self._validate_nk_columns_in_df(df)\n",
      "\n",
      "        for constant_column in self._constant_columns:\n",
      "            if constant_column.name not in df.columns:\n",
      "                df = df.withColumn(constant_column.name, F.lit(None))\n",
      "\n",
      "            if constant_column.part_of_nk:\n",
      "                df = df.filter(F.col(constant_column.name) == constant_column.value)\n",
      "\n",
      "        df = df.withColumn(self._nk_column_name, F.concat_ws(self._nk_column_concate_str, *self._nk_columns))\n",
      "\n",
      "        return_columns = df.columns\n",
      "\n",
      "        window_spec = Window.partitionBy(self._nk_columns).orderBy(df[self._row_load_dts_column].desc())\n",
      "        df_with_rownum = df.withColumn(\"ROW_NUMBER\", F.row_number().over(window_spec))\n",
      "        df = df_with_rownum.filter(df_with_rownum[\"ROW_NUMBER\"] == 1).select(return_columns)\n",
      "\n",
      "        return df\n",
      "\n",
      "    def _add_missing_columns(self, df_target: DataFrame, df_source: DataFrame) -> DataFrame:\n",
      "        missing_columns = [missing_column for missing_column in df_source.columns if missing_column not in df_target.columns]\n",
      "\n",
      "        for missing_column in missing_columns:\n",
      "            df_target = df_target.withColumn(missing_column, F.lit(None))\n",
      "\n",
      "        return df_target\n",
      "\n",
      "    def _get_columns_to_compare(self, df: DataFrame) -> list[str]:\n",
      "        if isinstance(self._include_comparing_columns, list) and len(self._include_comparing_columns) >= 1:\n",
      "            self._validate_include_comparing_columns(df)\n",
      "            return self._include_comparing_columns\n",
      "\n",
      "        comparison_columns = [column for column in df.columns if column not in self._exclude_comparing_columns]\n",
      "\n",
      "        return comparison_columns\n",
      "\n",
      "    def _get_columns_ordered(self, df: DataFrame) -> list[str]:\n",
      "        all_columns = [column for column in df.columns if column not in self._dw_columns]\n",
      "\n",
      "        return [self._pk_column_name, self._nk_column_name] + all_columns + [\n",
      "            self._row_load_dts_column,\n",
      "            self._row_delete_dts_column\n",
      "        ]\n",
      "\n",
      "    def _apply_transformations(self, df: DataFrame) -> DataFrame:\n",
      "        transform_fn: Callable = self._transformations.get(self._src_table.table)\n",
      "        transform_fn_all: Callable = self._transformations.get(\"*\")\n",
      "\n",
      "        if transform_fn_all is not None:\n",
      "            df = transform_fn_all(df, self)\n",
      "\n",
      "        if transform_fn is None:\n",
      "            return df\n",
      "\n",
      "        return transform_fn(df, self)\n",
      "\n",
      "    def _create_or_replace_historized_mlv(\n",
      "            self,\n",
      "            has_schema_changed: bool,\n",
      "            target_columns_ordered: list[str]\n",
      "    ) -> None:\n",
      "        if not has_schema_changed:\n",
      "            print(\"MLV: No schema change detected.\")\n",
      "            return\n",
      "        self._drop_historized_mlv()\n",
      "        self._create_historized_mlv(target_columns_ordered)\n",
      "\n",
      "    def _create_historized_mlv(self, target_columns_ordered: list[str]) -> None:\n",
      "        print(f\"MLV: CREATE MLV {self.mlv_name}\")\n",
      "        if not self._is_create_hist_mlv:\n",
      "            return\n",
      "\n",
      "        last_columns_ordered = [\n",
      "            self._row_is_current_column,\n",
      "            self._row_hist_number_column,\n",
      "            self._row_update_dts_column,\n",
      "            self._row_delete_dts_column,\n",
      "            self._row_load_dts_column\n",
      "        ]\n",
      "\n",
      "        silver_columns_ordered_str = \",\\n\".join([f\"`{column}`\" for column in target_columns_ordered])\n",
      "\n",
      "        final_ordered_columns = [\n",
      "            column\n",
      "            for column in target_columns_ordered\n",
      "            if column not in last_columns_ordered\n",
      "        ] + last_columns_ordered\n",
      "\n",
      "        final_ordered_columns_str = \",\\n\".join([f\"`{column}`\" for column in final_ordered_columns])\n",
      "\n",
      "        assert len(set(final_ordered_columns)) == len(final_ordered_columns), \\\n",
      "               f\"Duplicate columns found in final ordered columns {final_ordered_columns_str}.\"\n",
      "\n",
      "        # TODO: wenn ConstantColumns mit part_of_nk, dann muss hier noch nach partitioniert werden!\n",
      "        constant_column_str = \"\"\n",
      "        for constant_column in self._constant_columns:\n",
      "            if constant_column.part_of_nk:\n",
      "                constant_column_str = f\", `{constant_column.name}`\"\n",
      "                break\n",
      "\n",
      "        self._mlv_code = f\"\"\"\n",
      "CREATE MATERIALIZED LAKE VIEW {self.mlv_name}\n",
      "AS\n",
      "WITH cte_mlv AS (\n",
      "    SELECT\n",
      "        {silver_columns_ordered_str}\n",
      "        ,LEAD({self._row_load_dts_column}) OVER (PARTITION BY {self._nk_column_name} {constant_column_str} ORDER BY {self._row_load_dts_column} DESC) AS {self._row_update_dts_column}\n",
      "        ,ROW_NUMBER() OVER (PARTITION BY {self._nk_column_name} {constant_column_str} ORDER BY {self._row_load_dts_column} DESC) AS {self._row_hist_number_column}\n",
      "    FROM {self._dest_table.table_path}\n",
      "), cte_mlv_final AS (\n",
      "    SELECT\n",
      "        *\n",
      "        ,IIF({self._row_hist_number_column} = 1 AND {self._row_delete_dts_column} IS NULL, 1, 0) AS {self._row_is_current_column}\n",
      ")\n",
      "SELECT\n",
      "{final_ordered_columns_str}\n",
      "FROM cte_mlv\n",
      "\"\"\"\n",
      "        if self._is_testing_mock:\n",
      "            return\n",
      "\n",
      "        self._spark.sql(self._mlv_code)\n",
      "\n",
      "    def _drop_historized_mlv(self) -> None:\n",
      "        drop_mlv_sql = f\"DROP MATERIALIZED LAKE VIEW IF EXISTS {self.mlv_name}\"\n",
      "        print(drop_mlv_sql)\n",
      "\n",
      "        if self._is_testing_mock:\n",
      "            return\n",
      "\n",
      "        self._spark.sql(drop_mlv_sql)\n",
      "\n",
      "    def _refresh_historized_mlv(self) -> None:\n",
      "        refresh_mlv_sql = f\"REFRESH MATERIALIZED LAKE VIEW {self.mlv_name}\"\n",
      "        print(refresh_mlv_sql)\n",
      "\n",
      "        if self._is_testing_mock:\n",
      "            return\n",
      "\n",
      "        self._spark.sql(refresh_mlv_sql)\n",
      "\n",
      "    def _has_schema_change(self, df_bronze: DataFrame, df_silver: DataFrame) -> bool:\n",
      "        \"\"\"Check if the schema of the bronze DataFrame is different from the silver DataFrame.\n",
      "\n",
      "        Args:\n",
      "            df_bronze (DataFrame): Bronze DataFrame.\n",
      "            df_silver (DataFrame): Silver DataFrame.\n",
      "\n",
      "        Returns:\n",
      "            bool: True if the schema has changed, False otherwise.\n",
      "        \"\"\"\n",
      "        if df_silver is None:\n",
      "            return True\n",
      "        return set(df_bronze.columns) != set(df_silver.columns)\n",
      "\n",
      "    def _write_df(self, df: DataFrame, write_mode: str) -> None:\n",
      "        writer = df.write \\\n",
      "            .format(\"delta\") \\\n",
      "            .mode(write_mode) \\\n",
      "            .option(\"mergeSchema\", \"true\") \\\n",
      "            .partitionBy(*self._partition_by)\n",
      "\n",
      "        if self._is_testing_mock:\n",
      "            writer.save(get_mock_table_path(self._dest_table))\n",
      "            return\n",
      "\n",
      "        writer.saveAsTable(self._dest_table.table_path)\n",
      "\n",
      "\n",
      "etl = SilverIngestionInsertOnlyService()\n"
     ]
    }
   ],
   "source": [
    "code = import_module(\"transform.silver.insertonly\", VERSION)\n",
    "\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d24a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./code.py\", \"w\") as f:\n",
    "    f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e928341",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb3e8d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SilverIngestionInsertOnlyService at 0x10a544980>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40d48cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LakehouseTable(lakehouse='test_lakehouse', schema='test_schema', table='test_table')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = LakehouseTable(\n",
    "    lakehouse=\"test_lakehouse\",\n",
    "    schema=\"test_schema\",\n",
    "    table=\"test_table\"\n",
    ")\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ff41e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Module content is malformed: https://raw.githubusercontent.com/enricogoerlitz/fabricengineer-py/refs/tags/0.0.7/src/fabricengineer/transform/mlv/mlv.py.', 'Content: from typing import Any\\nfrom pyspark.sql import DataFrame, SparkSession\\n\\n\\nclass MaterializedLakeView:\\n    def __init__(\\n        self,\\n        lakehouse: str = None,\\n        schema: str = None,\\n        table: str = None,\\n        table_suffix: str = \"_mlv\",\\n        spark_: SparkSession = None,\\n        notebookutils_: Any = None,\\n        is_testing_mock: bool = False\\n    ) -> None:\\n        self.init(\\n            lakehouse=lakehouse,\\n            schema=schema,\\n            table=table,\\n            table_suffix=table_suffix,\\n            spark_=spark_,\\n            notebookutils_=notebookutils_,\\n            is_testing_mock=is_testing_mock\\n        )\\n\\n    def init(\\n        self,\\n        lakehouse: str,\\n        schema: str,\\n        table: str,\\n        table_suffix: str = \"_mlv\",\\n        spark_: SparkSession = None,\\n        notebookutils_: Any = None,\\n        is_testing_mock: bool = False\\n    ) -> \\'MaterializedLakeView\\':\\n        self._lakehouse = lakehouse\\n        self._schema = schema\\n        self._table = table\\n        self._table_suffix = table_suffix\\n        self._is_testing_mock = is_testing_mock\\n\\n        # \\'spark\\' and \\'notebookutils\\' are available in Fabric Notebook\\n        self._spark = self._get_init_spark(spark_)\\n        self._notebookutils = self._get_init_notebookutils(notebookutils_)\\n        return self\\n\\n    def _get_init_spark(self, spark_: SparkSession) -> SparkSession | None:\\n        if isinstance(spark_, SparkSession):\\n            return spark_\\n        try:\\n            if spark is not None:  # noqa: F821 # type: ignore\\n                return spark  # noqa: F821 # type: ignore\\n            return spark_\\n        except Exception:\\n            return None\\n\\n    def _get_init_notebookutils(self, notebookutils_: Any) -> Any | None:\\n        if notebookutils_ is not None:\\n            return notebookutils_\\n        try:\\n            if notebookutils is not None:  # noqa: F821 # type: ignore\\n                return notebookutils  # noqa: F821 # type: ignore\\n            return None\\n        except Exception:\\n            return None\\n\\n    @property\\n    def lakehouse(self) -> str:\\n        if self._lakehouse is None:\\n            raise ValueError(\"Lakehouse is not initialized.\")\\n        return self._lakehouse\\n\\n    @property\\n    def schema(self) -> str:\\n        if self._schema is None:\\n            raise ValueError(\"Schema is not initialized.\")\\n        return self._schema\\n\\n    @property\\n    def table(self) -> str:\\n        if self._table is None:\\n            raise ValueError(\"Table is not initialized.\")\\n        return self._table\\n\\n    @property\\n    def table_suffix(self) -> str:\\n        return self._table_suffix\\n\\n    @property\\n    def spark(self) -> SparkSession:\\n        if self._spark is None:\\n            raise ValueError(\"SparkSession is not initialized.\")\\n        return self._spark\\n\\n    @property\\n    def notebookutils(self) -> Any:\\n        if self._notebookutils is None:\\n            raise ValueError(\"NotebookUtils is not initialized.\")\\n        return self._notebookutils\\n\\n    @property\\n    def table_name(self) -> str:\\n        table_suffix = self.table_suffix or \"\"\\n        return f\"{self.table}{table_suffix}\"\\n\\n    @property\\n    def file_path(self) -> str:\\n        path = f\"Files/mlv/{self.lakehouse}/{self.schema}/{self.table_name}.sql.txt\"\\n        return path\\n\\n    @property\\n    def table_path(self) -> str:\\n        table_path = f\"{self.lakehouse}.{self.schema}.{self.table_name}\"\\n        return table_path\\n\\n    @property\\n    def schema_path(self) -> str:\\n        schema_path = f\"{self.lakehouse}.{self.schema}\"\\n        return schema_path\\n\\n    def read_file(self) -> str | None:\\n        path = self.file_path\\n        try:\\n            if not self.notebookutils.fs.exists(path):\\n                return None\\n            if self._is_testing_mock:\\n                with open(path, \\'r\\') as file:\\n                    return file.read()\\n            df = self.spark.read.text(path, wholetext=True)\\n            mlv_code = df.collect()[0][0]\\n            return mlv_code\\n        except Exception as e:\\n            raise RuntimeError(f\"Fehler beim Lesen der Datei: {e}\")\\n\\n    def write_file(self, sql: str) -> bool:\\n        try:\\n            result = self.notebookutils.fs.put(\\n                file=self.file_path,\\n                content=sql,\\n                overwrite=True\\n            )\\n            return result\\n        except Exception as e:\\n            raise RuntimeError(f\"Fehler beim Schreiben der Datei: {e}\")\\n\\n    def create_schema(self) -> None:\\n        create_schema = f\"CREATE SCHEMA IF NOT EXISTS {self.schema_path}\"\\n        print(create_schema)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(create_schema)\\n\\n    def create(self, sql: str) -> DataFrame:\\n        create_mlv = f\"CREATE MATERIALIZED LAKE VIEW {self.table_path}\\\\nAS\\\\n{sql}\"\\n\\n        self.create_schema()\\n        print(f\"CREATE MLV: {self.table_path}\")\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(create_mlv)\\n\\n    def drop(self) -> str:\\n        drop_mlv = f\"DROP MATERIALIZED LAKE VIEW IF EXISTS {self.table_path}\"\\n        print(drop_mlv)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(drop_mlv)\\n\\n    def create_or_replace(self, sql: str, mock_is_existing: bool = None) -> DataFrame:\\n        mlv_code_current = self.read_file()\\n        is_existing = (\\n            mock_is_existing\\n            if mock_is_existing is not None\\n            else self.spark.catalog.tableExists(self.table_path)\\n        )\\n\\n        if mlv_code_current is None and not is_existing:\\n            res = self.create(sql)\\n            self.write_file(sql)\\n            return res\\n\\n        elif mlv_code_current is None and is_existing:\\n            print(\"WARN: file=None, is_existing=True. RECREATE.\")\\n            self.drop()\\n            res = self.create(sql)\\n            self.write_file(sql)\\n            return res\\n\\n        elif sql == mlv_code_current and is_existing:\\n            print(\"Nothing has changed.\")\\n            return None\\n\\n        print(f\"REPLACE MLV: {self.table_path}\")\\n        self.drop()\\n        res = self.create(sql)\\n        self.write_file(sql)\\n        return res\\n\\n    def refresh(self, full_refresh: bool) -> DataFrame:\\n        full_refresh_str = \"FULL\" if full_refresh else \"\"\\n        refresh_mlv = f\"REFRESH MATERIALIZED LAKE VIEW {self.table_path} {full_refresh_str}\"\\n        print(refresh_mlv)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(refresh_mlv)\\n\\n    def to_dict(self) -> None:\\n        return {\\n            \"lakehouse\": self.lakehouse,\\n            \"schema\": self.schema,\\n            \"table\": self.table,\\n            \"table_path\": self.table_path\\n        }\\n\\n\\nmlv = MaterializedLakeView()\\n')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m code = \u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtransform.silver.mlv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVERSION\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(code)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/fabricengineer-py/notebooks/../src/fabricengineer/import_module/import_module.py:25\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, version)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m module_map:\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown module: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/fabricengineer-py/notebooks/../src/fabricengineer/import_module/import_module.py:68\u001b[39m, in \u001b[36m_import_module_mlv\u001b[39m\u001b[34m(base_path)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_import_module_mlv\u001b[39m(base_path: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     mlv_module = \u001b[43m_import_transform_mlv_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m     imports = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[33mfrom typing import Any\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[33mfrom pyspark.sql import DataFrame, SparkSession\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[33m\"\"\"\u001b[39m.strip()\n\u001b[32m     75\u001b[39m     code = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join([\n\u001b[32m     76\u001b[39m         imports,\n\u001b[32m     77\u001b[39m         mlv_module\n\u001b[32m     78\u001b[39m     ])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/fabricengineer-py/notebooks/../src/fabricengineer/import_module/import_module.py:85\u001b[39m, in \u001b[36m_import_transform_mlv_module\u001b[39m\u001b[34m(base_path)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_import_transform_mlv_module\u001b[39m(base_path: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     84\u001b[39m     mlv_module = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/transform/mlv/mlv.py\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fetch_module_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlv_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/fabricengineer-py/notebooks/../src/fabricengineer/import_module/import_module.py:114\u001b[39m, in \u001b[36m_fetch_module_content\u001b[39m\u001b[34m(module_path)\u001b[39m\n\u001b[32m    112\u001b[39m code = resp.text.split(_filename(module_path))\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m code \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(code) < \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    115\u001b[39m         (\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModule content is malformed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    116\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mContent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    117\u001b[39m     )\n\u001b[32m    119\u001b[39m code = code[\u001b[32m1\u001b[39m].strip()\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m code\n",
      "\u001b[31mValueError\u001b[39m: ('Module content is malformed: https://raw.githubusercontent.com/enricogoerlitz/fabricengineer-py/refs/tags/0.0.7/src/fabricengineer/transform/mlv/mlv.py.', 'Content: from typing import Any\\nfrom pyspark.sql import DataFrame, SparkSession\\n\\n\\nclass MaterializedLakeView:\\n    def __init__(\\n        self,\\n        lakehouse: str = None,\\n        schema: str = None,\\n        table: str = None,\\n        table_suffix: str = \"_mlv\",\\n        spark_: SparkSession = None,\\n        notebookutils_: Any = None,\\n        is_testing_mock: bool = False\\n    ) -> None:\\n        self.init(\\n            lakehouse=lakehouse,\\n            schema=schema,\\n            table=table,\\n            table_suffix=table_suffix,\\n            spark_=spark_,\\n            notebookutils_=notebookutils_,\\n            is_testing_mock=is_testing_mock\\n        )\\n\\n    def init(\\n        self,\\n        lakehouse: str,\\n        schema: str,\\n        table: str,\\n        table_suffix: str = \"_mlv\",\\n        spark_: SparkSession = None,\\n        notebookutils_: Any = None,\\n        is_testing_mock: bool = False\\n    ) -> \\'MaterializedLakeView\\':\\n        self._lakehouse = lakehouse\\n        self._schema = schema\\n        self._table = table\\n        self._table_suffix = table_suffix\\n        self._is_testing_mock = is_testing_mock\\n\\n        # \\'spark\\' and \\'notebookutils\\' are available in Fabric Notebook\\n        self._spark = self._get_init_spark(spark_)\\n        self._notebookutils = self._get_init_notebookutils(notebookutils_)\\n        return self\\n\\n    def _get_init_spark(self, spark_: SparkSession) -> SparkSession | None:\\n        if isinstance(spark_, SparkSession):\\n            return spark_\\n        try:\\n            if spark is not None:  # noqa: F821 # type: ignore\\n                return spark  # noqa: F821 # type: ignore\\n            return spark_\\n        except Exception:\\n            return None\\n\\n    def _get_init_notebookutils(self, notebookutils_: Any) -> Any | None:\\n        if notebookutils_ is not None:\\n            return notebookutils_\\n        try:\\n            if notebookutils is not None:  # noqa: F821 # type: ignore\\n                return notebookutils  # noqa: F821 # type: ignore\\n            return None\\n        except Exception:\\n            return None\\n\\n    @property\\n    def lakehouse(self) -> str:\\n        if self._lakehouse is None:\\n            raise ValueError(\"Lakehouse is not initialized.\")\\n        return self._lakehouse\\n\\n    @property\\n    def schema(self) -> str:\\n        if self._schema is None:\\n            raise ValueError(\"Schema is not initialized.\")\\n        return self._schema\\n\\n    @property\\n    def table(self) -> str:\\n        if self._table is None:\\n            raise ValueError(\"Table is not initialized.\")\\n        return self._table\\n\\n    @property\\n    def table_suffix(self) -> str:\\n        return self._table_suffix\\n\\n    @property\\n    def spark(self) -> SparkSession:\\n        if self._spark is None:\\n            raise ValueError(\"SparkSession is not initialized.\")\\n        return self._spark\\n\\n    @property\\n    def notebookutils(self) -> Any:\\n        if self._notebookutils is None:\\n            raise ValueError(\"NotebookUtils is not initialized.\")\\n        return self._notebookutils\\n\\n    @property\\n    def table_name(self) -> str:\\n        table_suffix = self.table_suffix or \"\"\\n        return f\"{self.table}{table_suffix}\"\\n\\n    @property\\n    def file_path(self) -> str:\\n        path = f\"Files/mlv/{self.lakehouse}/{self.schema}/{self.table_name}.sql.txt\"\\n        return path\\n\\n    @property\\n    def table_path(self) -> str:\\n        table_path = f\"{self.lakehouse}.{self.schema}.{self.table_name}\"\\n        return table_path\\n\\n    @property\\n    def schema_path(self) -> str:\\n        schema_path = f\"{self.lakehouse}.{self.schema}\"\\n        return schema_path\\n\\n    def read_file(self) -> str | None:\\n        path = self.file_path\\n        try:\\n            if not self.notebookutils.fs.exists(path):\\n                return None\\n            if self._is_testing_mock:\\n                with open(path, \\'r\\') as file:\\n                    return file.read()\\n            df = self.spark.read.text(path, wholetext=True)\\n            mlv_code = df.collect()[0][0]\\n            return mlv_code\\n        except Exception as e:\\n            raise RuntimeError(f\"Fehler beim Lesen der Datei: {e}\")\\n\\n    def write_file(self, sql: str) -> bool:\\n        try:\\n            result = self.notebookutils.fs.put(\\n                file=self.file_path,\\n                content=sql,\\n                overwrite=True\\n            )\\n            return result\\n        except Exception as e:\\n            raise RuntimeError(f\"Fehler beim Schreiben der Datei: {e}\")\\n\\n    def create_schema(self) -> None:\\n        create_schema = f\"CREATE SCHEMA IF NOT EXISTS {self.schema_path}\"\\n        print(create_schema)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(create_schema)\\n\\n    def create(self, sql: str) -> DataFrame:\\n        create_mlv = f\"CREATE MATERIALIZED LAKE VIEW {self.table_path}\\\\nAS\\\\n{sql}\"\\n\\n        self.create_schema()\\n        print(f\"CREATE MLV: {self.table_path}\")\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(create_mlv)\\n\\n    def drop(self) -> str:\\n        drop_mlv = f\"DROP MATERIALIZED LAKE VIEW IF EXISTS {self.table_path}\"\\n        print(drop_mlv)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(drop_mlv)\\n\\n    def create_or_replace(self, sql: str, mock_is_existing: bool = None) -> DataFrame:\\n        mlv_code_current = self.read_file()\\n        is_existing = (\\n            mock_is_existing\\n            if mock_is_existing is not None\\n            else self.spark.catalog.tableExists(self.table_path)\\n        )\\n\\n        if mlv_code_current is None and not is_existing:\\n            res = self.create(sql)\\n            self.write_file(sql)\\n            return res\\n\\n        elif mlv_code_current is None and is_existing:\\n            print(\"WARN: file=None, is_existing=True. RECREATE.\")\\n            self.drop()\\n            res = self.create(sql)\\n            self.write_file(sql)\\n            return res\\n\\n        elif sql == mlv_code_current and is_existing:\\n            print(\"Nothing has changed.\")\\n            return None\\n\\n        print(f\"REPLACE MLV: {self.table_path}\")\\n        self.drop()\\n        res = self.create(sql)\\n        self.write_file(sql)\\n        return res\\n\\n    def refresh(self, full_refresh: bool) -> DataFrame:\\n        full_refresh_str = \"FULL\" if full_refresh else \"\"\\n        refresh_mlv = f\"REFRESH MATERIALIZED LAKE VIEW {self.table_path} {full_refresh_str}\"\\n        print(refresh_mlv)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(refresh_mlv)\\n\\n    def to_dict(self) -> None:\\n        return {\\n            \"lakehouse\": self.lakehouse,\\n            \"schema\": self.schema,\\n            \"table\": self.table,\\n            \"table_path\": self.table_path\\n        }\\n\\n\\nmlv = MaterializedLakeView()\\n')"
     ]
    }
   ],
   "source": [
    "code = import_module(\"transform.silver.mlv\", VERSION)\n",
    "\n",
    "print(code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
