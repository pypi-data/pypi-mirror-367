{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43180ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from fabricengineer.import_module.import_module import import_module\n",
    "\n",
    "VERSION = \"0.0.9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6dbac0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/07 13:23:43 WARN Utils: Your hostname, MacBook-Air-von-Enrico.local resolves to a loopback address: 127.0.0.1; using 192.168.0.7 instead (on interface en0)\n",
      "25/08/07 13:23:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/07 13:23:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/07 13:23:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "class NotebookUtilsFSMock:\n",
    "    def _get_path(self, file: str) -> str:\n",
    "        return os.path.join(os.getcwd(), file)\n",
    "\n",
    "    def exists(self, path: str) -> bool:\n",
    "        return os.path.exists(self._get_path(path))\n",
    "\n",
    "    def put(\n",
    "        self,\n",
    "        file: str,\n",
    "        content: str,\n",
    "        overwrite: bool = False\n",
    "    ) -> None:\n",
    "        path = self._get_path(file)\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "        if os.path.exists(path) and not overwrite:\n",
    "            raise FileExistsError(f\"File {path} already exists and overwrite is set to False.\")\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(content)\n",
    "\n",
    "\n",
    "class NotebookUtilsMock:\n",
    "    def __init__(self):\n",
    "        self.fs = NotebookUtilsFSMock()\n",
    "\n",
    "global spark\n",
    "spark: SparkSession = SparkSession.builder.appName(\"PlaygroundSparkSession\").getOrCreate()\n",
    "\n",
    "global notebookutils\n",
    "notebookutils = NotebookUtilsMock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3602f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import Any\n",
      "from pyspark.sql import DataFrame, SparkSession\n",
      "\n",
      "\n",
      "def to_spark_sql(sql: str) -> str:\n",
      "    return sql \\\n",
      "            .replace(\"[\", \"`\") \\\n",
      "            .replace(\"]\", \"`\")\n",
      "\n",
      "\n",
      "class MaterializedLakeView:\n",
      "    def __init__(\n",
      "        self,\n",
      "        lakehouse: str = None,\n",
      "        schema: str = None,\n",
      "        table: str = None,\n",
      "        table_suffix: str = \"_mlv\",\n",
      "        spark_: SparkSession = None,\n",
      "        notebookutils_: Any = None,\n",
      "        is_testing_mock: bool = False\n",
      "    ) -> None:\n",
      "        self.init(\n",
      "            lakehouse=lakehouse,\n",
      "            schema=schema,\n",
      "            table=table,\n",
      "            table_suffix=table_suffix,\n",
      "            spark_=spark_,\n",
      "            notebookutils_=notebookutils_,\n",
      "            is_testing_mock=is_testing_mock\n",
      "        )\n",
      "\n",
      "    def init(\n",
      "        self,\n",
      "        lakehouse: str,\n",
      "        schema: str,\n",
      "        table: str,\n",
      "        table_suffix: str = \"_mlv\",\n",
      "        spark_: SparkSession = None,\n",
      "        notebookutils_: Any = None,\n",
      "        is_testing_mock: bool = False\n",
      "    ) -> 'MaterializedLakeView':\n",
      "        \"\"\"Initializes the MaterializedLakeView instance.\n",
      "\n",
      "        Args:\n",
      "            lakehouse (str): The lakehouse name.\n",
      "            schema (str): The schema name.\n",
      "            table (str): The table name.\n",
      "            table_suffix (str, optional): The table suffix. Defaults to \"_mlv\".\n",
      "            spark_ (SparkSession, optional): The SparkSession instance. Defaults to None.\n",
      "            notebookutils_ (Any, optional): The NotebookUtils instance. Defaults to None.\n",
      "            is_testing_mock (bool, optional): Whether the instance is a testing mock. Defaults to False.\n",
      "\n",
      "        Returns:\n",
      "            MaterializedLakeView: The initialized MaterializedLakeView instance.\n",
      "        \"\"\"\n",
      "        self._lakehouse = lakehouse\n",
      "        self._schema = schema\n",
      "        self._table = table\n",
      "        self._table_suffix = table_suffix\n",
      "        self._is_testing_mock = is_testing_mock\n",
      "\n",
      "        # 'spark' and 'notebookutils' are available in Fabric notebook\n",
      "        self._spark = self._get_init_spark(spark_)\n",
      "        self._notebookutils = self._get_init_notebookutils(notebookutils_)\n",
      "        return self\n",
      "\n",
      "    def _get_init_spark(self, spark_: SparkSession) -> SparkSession | None:\n",
      "        \"\"\"Initializes the SparkSession instance.\n",
      "        If a SparkSession is provided, it is returned. Otherwise, it tries to use the global 'spark' variable.\n",
      "\n",
      "        Args:\n",
      "            spark_ (SparkSession): The SparkSession instance.\n",
      "\n",
      "        Returns:\n",
      "            SparkSession | None: The initialized SparkSession instance or None.\n",
      "        \"\"\"\n",
      "        if isinstance(spark_, SparkSession):\n",
      "            return spark_\n",
      "        try:\n",
      "            if spark is not None:  # noqa: F821 # type: ignore\n",
      "                return spark  # noqa: F821 # type: ignore\n",
      "            return spark_\n",
      "        except Exception:\n",
      "            return None\n",
      "\n",
      "    def _get_init_notebookutils(self, notebookutils_: Any) -> Any | None:\n",
      "        \"\"\"Initializes the NotebookUtils instance.\n",
      "        If a NotebookUtils instance is provided, it is returned. Otherwise, it tries to use the global 'notebookutils' variable.\n",
      "\n",
      "        Args:\n",
      "            notebookutils_ (Any): The NotebookUtils instance.\n",
      "\n",
      "        Returns:\n",
      "            Any | None: The initialized NotebookUtils instance or None.\n",
      "        \"\"\"\n",
      "        if notebookutils_ is not None:\n",
      "            return notebookutils_\n",
      "        try:\n",
      "            if notebookutils is not None:  # noqa: F821 # type: ignore\n",
      "                return notebookutils  # noqa: F821 # type: ignore\n",
      "            return None\n",
      "        except Exception:\n",
      "            return None\n",
      "\n",
      "    @property\n",
      "    def lakehouse(self) -> str:\n",
      "        if self._lakehouse is None:\n",
      "            raise ValueError(\"Lakehouse is not initialized.\")\n",
      "        return self._lakehouse\n",
      "\n",
      "    @property\n",
      "    def schema(self) -> str:\n",
      "        if self._schema is None:\n",
      "            raise ValueError(\"Schema is not initialized.\")\n",
      "        return self._schema\n",
      "\n",
      "    @property\n",
      "    def table(self) -> str:\n",
      "        if self._table is None:\n",
      "            raise ValueError(\"Table is not initialized.\")\n",
      "        return self._table\n",
      "\n",
      "    @property\n",
      "    def table_suffix(self) -> str:\n",
      "        return self._table_suffix\n",
      "\n",
      "    @property\n",
      "    def spark(self) -> SparkSession:\n",
      "        if self._spark is None:\n",
      "            raise ValueError(\"SparkSession is not initialized.\")\n",
      "        return self._spark\n",
      "\n",
      "    @property\n",
      "    def notebookutils(self) -> Any:\n",
      "        if self._notebookutils is None:\n",
      "            raise ValueError(\"NotebookUtils is not initialized.\")\n",
      "        return self._notebookutils\n",
      "\n",
      "    @property\n",
      "    def table_name(self) -> str:\n",
      "        table_suffix = self.table_suffix or \"\"\n",
      "        return f\"{self.table}{table_suffix}\"\n",
      "\n",
      "    @property\n",
      "    def file_path(self) -> str:\n",
      "        path = f\"Files/mlv/{self.lakehouse}/{self.schema}/{self.table_name}.sql.txt\"\n",
      "        return path\n",
      "\n",
      "    @property\n",
      "    def table_path(self) -> str:\n",
      "        table_path = f\"{self.lakehouse}.{self.schema}.{self.table_name}\"\n",
      "        return table_path\n",
      "\n",
      "    @property\n",
      "    def schema_path(self) -> str:\n",
      "        schema_path = f\"{self.lakehouse}.{self.schema}\"\n",
      "        return schema_path\n",
      "\n",
      "    def read_file(self) -> str | None:\n",
      "        \"\"\"Reads the content of the SQL file from the specified lakehouse.\n",
      "        If the file does not exist, it returns None.\n",
      "\n",
      "        Raises:\n",
      "            RuntimeError: If the file cannot be read.\n",
      "\n",
      "        Returns:\n",
      "            str | None: The content of the file or None if it doesn't exist.\n",
      "        \"\"\"\n",
      "        path = self.file_path\n",
      "        try:\n",
      "            if not self.notebookutils.fs.exists(path):\n",
      "                return None\n",
      "            if self._is_testing_mock:\n",
      "                with open(path, \"r\") as file:\n",
      "                    return file.read()\n",
      "            df = self.spark.read.text(path, wholetext=True)\n",
      "            mlv_code = df.collect()[0][0]\n",
      "            return mlv_code\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(f\"Fehler beim Lesen der Datei: {e}\")\n",
      "\n",
      "    def write_file(self, sql: str) -> bool:\n",
      "        \"\"\"Writes the SQL content to the specified file in a lakehouse.\n",
      "        If the file already exists, it will be overwritten.\n",
      "        If the file cannot be written, it raises a RuntimeError.\n",
      "\n",
      "        Args:\n",
      "            sql (str): The SQL content to write.\n",
      "\n",
      "        Raises:\n",
      "            RuntimeError: If the file cannot be written.\n",
      "\n",
      "        Returns:\n",
      "            bool: True if the file was written successfully, False otherwise.\n",
      "        \"\"\"\n",
      "        try:\n",
      "            result = self.notebookutils.fs.put(\n",
      "                file=self.file_path,\n",
      "                content=sql,\n",
      "                overwrite=True\n",
      "            )\n",
      "            return result\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(f\"Fehler beim Schreiben der Datei: {e}\")\n",
      "\n",
      "    def create_schema(self) -> DataFrame | None:\n",
      "        \"\"\"Creates the schema in the lakehouse if it does not exist.\"\"\"\n",
      "        create_schema = f\"CREATE SCHEMA IF NOT EXISTS {self.schema_path}\"\n",
      "        logger.info(create_schema)\n",
      "\n",
      "        if self._is_testing_mock:\n",
      "            return None\n",
      "\n",
      "        return self.spark.sql(create_schema)\n",
      "\n",
      "    def create(self, sql: str) -> DataFrame | None:\n",
      "        \"\"\"Creates a Materialized Lake View (MLV) in the lakehouse with the given SQL query.\"\"\"\n",
      "        self.create_schema()\n",
      "\n",
      "        create_mlv = f\"CREATE MATERIALIZED LAKE VIEW {self.table_path}\\nAS\\n{sql}\"\n",
      "        logger.info(f\"CREATE MLV: {self.table_path}\")\n",
      "\n",
      "        if self._is_testing_mock:\n",
      "            return None\n",
      "\n",
      "        return self.spark.sql(create_mlv)\n",
      "\n",
      "    def drop(self) -> str:\n",
      "        \"\"\"Drops the Materialized Lake View (MLV) if it exists.\"\"\"\n",
      "        drop_mlv = f\"DROP MATERIALIZED LAKE VIEW IF EXISTS {self.table_path}\"\n",
      "        logger.info(drop_mlv)\n",
      "\n",
      "        if self._is_testing_mock:\n",
      "            return None\n",
      "\n",
      "        return self.spark.sql(drop_mlv)\n",
      "\n",
      "    def create_or_replace(self, sql: str, mock_is_existing: bool = None) -> DataFrame:\n",
      "        \"\"\"Creates or replaces the Materialized Lake View (MLV) in the lakehouse.\n",
      "\n",
      "        Args:\n",
      "            sql (str): The SQL query to create the MLV.\n",
      "            mock_is_existing (bool, optional): If True, it simulates the existence of the MLV. Defaults to None.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: The result of the create or replace operation.\n",
      "        \"\"\"\n",
      "        mlv_code_current = self.read_file()\n",
      "        is_existing = (\n",
      "            mock_is_existing\n",
      "            if mock_is_existing is not None\n",
      "            else self.spark.catalog.tableExists(self.table_path)\n",
      "        )\n",
      "\n",
      "        if mlv_code_current is None and not is_existing:\n",
      "            res = self.create(sql)\n",
      "            self.write_file(sql)\n",
      "            return res\n",
      "\n",
      "        elif mlv_code_current is None and is_existing:\n",
      "            logger.warning(\"WARN: file=None, is_existing=True. RECREATE.\")\n",
      "            self.drop()\n",
      "            res = self.create(sql)\n",
      "            self.write_file(sql)\n",
      "            return res\n",
      "\n",
      "        elif sql == mlv_code_current and is_existing:\n",
      "            logger.info(\"Nothing has changed.\")\n",
      "            return None\n",
      "\n",
      "        logger.info(f\"REPLACE MLV: {self.table_path}\")\n",
      "        self.drop()\n",
      "        res = self.create(sql)\n",
      "        self.write_file(sql)\n",
      "        return res\n",
      "\n",
      "    def refresh(self, full_refresh: bool) -> DataFrame:\n",
      "        \"\"\"Refreshes the Materialized Lake View (MLV) in the lakehouse.\"\"\"\n",
      "        full_refresh_str = \"FULL\" if full_refresh else \"\"\n",
      "        refresh_mlv = f\"REFRESH MATERIALIZED LAKE VIEW {self.table_path} {full_refresh_str}\"\n",
      "        logger.info(refresh_mlv)\n",
      "\n",
      "        if self._is_testing_mock:\n",
      "            return None\n",
      "\n",
      "        return self.spark.sql(refresh_mlv)\n",
      "\n",
      "    def to_dict(self) -> None:\n",
      "        \"\"\"Returns a dictionary representation of the Materialized Lake View.\"\"\"\n",
      "        return {\n",
      "            \"lakehouse\": self.lakehouse,\n",
      "            \"schema\": self.schema,\n",
      "            \"table\": self.table,\n",
      "            \"table_path\": self.table_path\n",
      "        }\n",
      "\n",
      "\n",
      "mlv = MaterializedLakeView()\n"
     ]
    }
   ],
   "source": [
    "code = import_module(\"transform.mlv\", VERSION)\n",
    "\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d24a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./code.py\", \"w\") as f:\n",
    "#     f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e928341",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(code, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb3e8d5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'etl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43metl\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'etl' is not defined"
     ]
    }
   ],
   "source": [
    "etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40d48cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LakehouseTable(lakehouse='test_lakehouse', schema='test_schema', table='test_table')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = LakehouseTable(\n",
    "    lakehouse=\"test_lakehouse\",\n",
    "    schema=\"test_schema\",\n",
    "    table=\"test_table\"\n",
    ")\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10ff41e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import Any\n",
      "from pyspark.sql import DataFrame, SparkSession\n",
      "from fabricengineer.logging.logger import logger\n",
      "\n",
      "\n",
      "def to_spark_sql(sql: str) -> str:\n",
      "    return sql \\\n",
      "            .replace(\"[\", \"`\") \\\n",
      "            .replace(\"]\", \"`\")\n",
      "\n",
      "\n",
      "class MaterializedLakeView:\n",
      "    def __init__(\n",
      "        self,\n",
      "        lakehouse: str = None,\n",
      "        schema: str = None,\n",
      "        table: str = None,\n",
      "        table_suffix: str = \"_mlv\",\n",
      "        spark_: SparkSession = None,\n",
      "        notebookutils_: Any = None,\n",
      "        is_testing_mock: bool = False\n",
      "    ) -> None:\n",
      "        self.init(\n",
      "            lakehouse=lakehouse,\n",
      "            schema=schema,\n",
      "            table=table,\n",
      "            table_suffix=table_suffix,\n",
      "            spark_=spark_,\n",
      "            notebookutils_=notebookutils_,\n",
      "            is_testing_mock=is_testing_mock\n",
      "        )\n",
      "\n",
      "    def init(\n",
      "        self,\n",
      "        lakehouse: str,\n",
      "        schema: str,\n",
      "        table: str,\n",
      "        table_suffix: str = \"_mlv\",\n",
      "        spark_: SparkSession = None,\n",
      "        notebookutils_: Any = None,\n",
      "        is_testing_mock: bool = False\n",
      "    ) -> 'MaterializedLakeView':\n",
      "        \"\"\"Initializes the MaterializedLakeView instance.\n",
      "\n",
      "        Args:\n",
      "            lakehouse (str): The lakehouse name.\n",
      "            schema (str): The schema name.\n",
      "            table (str): The table name.\n",
      "            table_suffix (str, optional): The table suffix. Defaults to \"_mlv\".\n",
      "            spark_ (SparkSession, optional): The SparkSession instance. Defaults to None.\n",
      "            notebookutils_ (Any, optional): The NotebookUtils instance. Defaults to None.\n",
      "            is_testing_mock (bool, optional): Whether the instance is a testing mock. Defaults to False.\n",
      "\n",
      "        Returns:\n",
      "            MaterializedLakeView: The initialized MaterializedLakeView instance.\n",
      "        \"\"\"\n",
      "        self._lakehouse = lakehouse\n",
      "        self._schema = schema\n",
      "        self._table = table\n",
      "        self._table_suffix = table_suffix\n",
      "        self._is_testing_mock = is_testing_mock\n",
      "\n",
      "        # 'spark' and 'notebookutils' are available in Fabric notebook\n",
      "        self._spark = self._get_init_spark(spark_)\n",
      "        self._notebookutils = self._get_init_notebookutils(notebookutils_)\n",
      "        return self\n",
      "\n",
      "    def _get_init_spark(self, spark_: SparkSession) -> SparkSession | None:\n",
      "        \"\"\"Initializes the SparkSession instance.\n",
      "        If a SparkSession is provided, it is returned. Otherwise, it tries to use the global 'spark' variable.\n",
      "\n",
      "        Args:\n",
      "            spark_ (SparkSession): The SparkSession instance.\n",
      "\n",
      "        Returns:\n",
      "            SparkSession | None: The initialized SparkSession instance or None.\n",
      "        \"\"\"\n",
      "        if isinstance(spark_, SparkSession):\n",
      "            return spark_\n",
      "        try:\n",
      "            if spark is not None:  # noqa: F821 # type: ignore\n",
      "                return spark  # noqa: F821 # type: ignore\n",
      "            return spark_\n",
      "        except Exception:\n",
      "            return None\n",
      "\n",
      "    def _get_init_notebookutils(self, notebookutils_: Any) -> Any | None:\n",
      "        \"\"\"Initializes the NotebookUtils instance.\n",
      "        If a NotebookUtils instance is provided, it is returned. Otherwise, it tries to use the global 'notebookutils' variable.\n",
      "\n",
      "        Args:\n",
      "            notebookutils_ (Any): The NotebookUtils instance.\n",
      "\n",
      "        Returns:\n",
      "            Any | None: The initialized NotebookUtils instance or None.\n",
      "        \"\"\"\n",
      "        if notebookutils_ is not None:\n",
      "            return notebookutils_\n",
      "        try:\n",
      "            if notebookutils is not None:  # noqa: F821 # type: ignore\n",
      "                return notebookutils  # noqa: F821 # type: ignore\n",
      "            return None\n",
      "        except Exception:\n",
      "            return None\n",
      "\n",
      "    @property\n",
      "    def lakehouse(self) -> str:\n",
      "        if self._lakehouse is None:\n",
      "            raise ValueError(\"Lakehouse is not initialized.\")\n",
      "        return self._lakehouse\n",
      "\n",
      "    @property\n",
      "    def schema(self) -> str:\n",
      "        if self._schema is None:\n",
      "            raise ValueError(\"Schema is not initialized.\")\n",
      "        return self._schema\n",
      "\n",
      "    @property\n",
      "    def table(self) -> str:\n",
      "        if self._table is None:\n",
      "            raise ValueError(\"Table is not initialized.\")\n",
      "        return self._table\n",
      "\n",
      "    @property\n",
      "    def table_suffix(self) -> str:\n",
      "        return self._table_suffix\n",
      "\n",
      "    @property\n",
      "    def spark(self) -> SparkSession:\n",
      "        if self._spark is None:\n",
      "            raise ValueError(\"SparkSession is not initialized.\")\n",
      "        return self._spark\n",
      "\n",
      "    @property\n",
      "    def notebookutils(self) -> Any:\n",
      "        if self._notebookutils is None:\n",
      "            raise ValueError(\"NotebookUtils is not initialized.\")\n",
      "        return self._notebookutils\n",
      "\n",
      "    @property\n",
      "    def table_name(self) -> str:\n",
      "        table_suffix = self.table_suffix or \"\"\n",
      "        return f\"{self.table}{table_suffix}\"\n",
      "\n",
      "    @property\n",
      "    def file_path(self) -> str:\n",
      "        path = f\"Files/mlv/{self.lakehouse}/{self.schema}/{self.table_name}.sql.txt\"\n",
      "        return path\n",
      "\n",
      "    @property\n",
      "    def table_path(self) -> str:\n",
      "        table_path = f\"{self.lakehouse}.{self.schema}.{self.table_name}\"\n",
      "        return table_path\n",
      "\n",
      "    @property\n",
      "    def schema_path(self) -> str:\n",
      "        schema_path = f\"{self.lakehouse}.{self.schema}\"\n",
      "        return schema_path\n",
      "\n",
      "    def read_file(self) -> str | None:\n",
      "        \"\"\"Reads the content of the SQL file from the specified lakehouse.\n",
      "        If the file does not exist, it returns None.\n",
      "\n",
      "        Raises:\n",
      "            RuntimeError: If the file cannot be read.\n",
      "\n",
      "        Returns:\n",
      "            str | None: The content of the file or None if it doesn't exist.\n",
      "        \"\"\"\n",
      "        path = self.file_path\n",
      "        try:\n",
      "            if not self.notebookutils.fs.exists(path):\n",
      "                return None\n",
      "            if self._is_testing_mock:\n",
      "                with open(path, \"r\") as file:\n",
      "                    return file.read()\n",
      "            df = self.spark.read.text(path, wholetext=True)\n",
      "            mlv_code = df.collect()[0][0]\n",
      "            return mlv_code\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(f\"Fehler beim Lesen der Datei: {e}\")\n",
      "\n",
      "    def write_file(self, sql: str) -> bool:\n",
      "        \"\"\"Writes the SQL content to the specified file in a lakehouse.\n",
      "        If the file already exists, it will be overwritten.\n",
      "        If the file cannot be written, it raises a RuntimeError.\n",
      "\n",
      "        Args:\n",
      "            sql (str): The SQL content to write.\n",
      "\n",
      "        Raises:\n",
      "            RuntimeError: If the file cannot be written.\n",
      "\n",
      "        Returns:\n",
      "            bool: True if the file was written successfully, False otherwise.\n",
      "        \"\"\"\n",
      "        try:\n",
      "            result = self.notebookutils.fs.put(\n",
      "                file=self.file_path,\n",
      "                content=sql,\n",
      "                overwrite=True\n",
      "            )\n",
      "            return result\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(f\"Fehler beim Schreiben der Datei: {e}\")\n",
      "\n",
      "    def create_schema(self) -> DataFrame | None:\n",
      "        \"\"\"Creates the schema in the lakehouse if it does not exist.\"\"\"\n",
      "        create_schema = f\"CREATE SCHEMA IF NOT EXISTS {self.schema_path}\"\n",
      "        logger.info(create_schema)\n",
      "\n",
      "        if self._is_testing_mock:\n",
      "            return None\n",
      "\n",
      "        return self.spark.sql(create_schema)\n",
      "\n",
      "    def create(self, sql: str) -> DataFrame | None:\n",
      "        \"\"\"Creates a Materialized Lake View (MLV) in the lakehouse with the given SQL query.\"\"\"\n",
      "        self.create_schema()\n",
      "\n",
      "        create_mlv = f\"CREATE MATERIALIZED LAKE VIEW {self.table_path}\\nAS\\n{sql}\"\n",
      "        logger.info(f\"CREATE MLV: {self.table_path}\")\n",
      "\n",
      "        if self._is_testing_mock:\n",
      "            return None\n",
      "\n",
      "        return self.spark.sql(create_mlv)\n",
      "\n",
      "    def drop(self) -> str:\n",
      "        \"\"\"Drops the Materialized Lake View (MLV) if it exists.\"\"\"\n",
      "        drop_mlv = f\"DROP MATERIALIZED LAKE VIEW IF EXISTS {self.table_path}\"\n",
      "        logger.info(drop_mlv)\n",
      "\n",
      "        if self._is_testing_mock:\n",
      "            return None\n",
      "\n",
      "        return self.spark.sql(drop_mlv)\n",
      "\n",
      "    def create_or_replace(self, sql: str, mock_is_existing: bool = None) -> DataFrame:\n",
      "        \"\"\"Creates or replaces the Materialized Lake View (MLV) in the lakehouse.\n",
      "\n",
      "        Args:\n",
      "            sql (str): The SQL query to create the MLV.\n",
      "            mock_is_existing (bool, optional): If True, it simulates the existence of the MLV. Defaults to None.\n",
      "\n",
      "        Returns:\n",
      "            DataFrame: The result of the create or replace operation.\n",
      "        \"\"\"\n",
      "        mlv_code_current = self.read_file()\n",
      "        is_existing = (\n",
      "            mock_is_existing\n",
      "            if mock_is_existing is not None\n",
      "            else self.spark.catalog.tableExists(self.table_path)\n",
      "        )\n",
      "\n",
      "        if mlv_code_current is None and not is_existing:\n",
      "            res = self.create(sql)\n",
      "            self.write_file(sql)\n",
      "            return res\n",
      "\n",
      "        elif mlv_code_current is None and is_existing:\n",
      "            logger.warning(\"WARN: file=None, is_existing=True. RECREATE.\")\n",
      "            self.drop()\n",
      "            res = self.create(sql)\n",
      "            self.write_file(sql)\n",
      "            return res\n",
      "\n",
      "        elif sql == mlv_code_current and is_existing:\n",
      "            logger.info(\"Nothing has changed.\")\n",
      "            return None\n",
      "\n",
      "        logger.info(f\"REPLACE MLV: {self.table_path}\")\n",
      "        self.drop()\n",
      "        res = self.create(sql)\n",
      "        self.write_file(sql)\n",
      "        return res\n",
      "\n",
      "    def refresh(self, full_refresh: bool) -> DataFrame:\n",
      "        \"\"\"Refreshes the Materialized Lake View (MLV) in the lakehouse.\"\"\"\n",
      "        full_refresh_str = \"FULL\" if full_refresh else \"\"\n",
      "        refresh_mlv = f\"REFRESH MATERIALIZED LAKE VIEW {self.table_path} {full_refresh_str}\"\n",
      "        logger.info(refresh_mlv)\n",
      "\n",
      "        if self._is_testing_mock:\n",
      "            return None\n",
      "\n",
      "        return self.spark.sql(refresh_mlv)\n",
      "\n",
      "    def to_dict(self) -> None:\n",
      "        \"\"\"Returns a dictionary representation of the Materialized Lake View.\"\"\"\n",
      "        return {\n",
      "            \"lakehouse\": self.lakehouse,\n",
      "            \"schema\": self.schema,\n",
      "            \"table\": self.table,\n",
      "            \"table_path\": self.table_path\n",
      "        }\n",
      "\n",
      "\n",
      "mlv = MaterializedLakeView() {'__name__': '__main__', '__doc__': '\\nNote: all executions are function-scoped as we do not assume the code below executes in an isolated kernel environment.\\n', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', 'import sys\\n\\nsys.path.append(\"../src\")\\n\\nfrom fabricengineer.import_module.import_module import import_module\\n\\nVERSION = \"0.0.9\"', 'import os\\nfrom pyspark.sql import SparkSession\\n\\n\\nclass NotebookUtilsFSMock:\\n    def _get_path(self, file: str) -> str:\\n        return os.path.join(os.getcwd(), file)\\n\\n    def exists(self, path: str) -> bool:\\n        return os.path.exists(self._get_path(path))\\n\\n    def put(\\n        self,\\n        file: str,\\n        content: str,\\n        overwrite: bool = False\\n    ) -> None:\\n        path = self._get_path(file)\\n        os.makedirs(os.path.dirname(path), exist_ok=True)\\n\\n        if os.path.exists(path) and not overwrite:\\n            raise FileExistsError(f\"File {path} already exists and overwrite is set to False.\")\\n        with open(path, \\'w\\') as f:\\n            f.write(content)\\n\\n\\nclass NotebookUtilsMock:\\n    def __init__(self):\\n        self.fs = NotebookUtilsFSMock()\\n\\nglobal spark\\nspark: SparkSession = SparkSession.builder.appName(\"PlaygroundSparkSession\").getOrCreate()\\n\\nglobal notebookutils\\nnotebookutils = NotebookUtilsMock()', 'code = import_module(\"transform.silver.insertonly\", VERSION)\\n\\nprint(code)', 'with open(\"./code.py\", \"w\") as f:\\n    f.write(code)', 'exec(code)', 'etl', 'table = LakehouseTable(\\n    lakehouse=\"test_lakehouse\",\\n    schema=\"test_schema\",\\n    table=\"test_table\"\\n)\\ntable', 'code = import_module(\"transform.silver.mlv\", VERSION)\\n\\nprint(code, globals())'], '_oh': {6: <__main__.SilverIngestionInsertOnlyService object at 0x107bbe780>, 7: LakehouseTable(lakehouse='test_lakehouse', schema='test_schema', table='test_table')}, '_dh': [PosixPath('/Users/enricogoerlitz/Developer/fabricengineer-py/notebooks')], 'In': ['', 'import sys\\n\\nsys.path.append(\"../src\")\\n\\nfrom fabricengineer.import_module.import_module import import_module\\n\\nVERSION = \"0.0.9\"', 'import os\\nfrom pyspark.sql import SparkSession\\n\\n\\nclass NotebookUtilsFSMock:\\n    def _get_path(self, file: str) -> str:\\n        return os.path.join(os.getcwd(), file)\\n\\n    def exists(self, path: str) -> bool:\\n        return os.path.exists(self._get_path(path))\\n\\n    def put(\\n        self,\\n        file: str,\\n        content: str,\\n        overwrite: bool = False\\n    ) -> None:\\n        path = self._get_path(file)\\n        os.makedirs(os.path.dirname(path), exist_ok=True)\\n\\n        if os.path.exists(path) and not overwrite:\\n            raise FileExistsError(f\"File {path} already exists and overwrite is set to False.\")\\n        with open(path, \\'w\\') as f:\\n            f.write(content)\\n\\n\\nclass NotebookUtilsMock:\\n    def __init__(self):\\n        self.fs = NotebookUtilsFSMock()\\n\\nglobal spark\\nspark: SparkSession = SparkSession.builder.appName(\"PlaygroundSparkSession\").getOrCreate()\\n\\nglobal notebookutils\\nnotebookutils = NotebookUtilsMock()', 'code = import_module(\"transform.silver.insertonly\", VERSION)\\n\\nprint(code)', 'with open(\"./code.py\", \"w\") as f:\\n    f.write(code)', 'exec(code)', 'etl', 'table = LakehouseTable(\\n    lakehouse=\"test_lakehouse\",\\n    schema=\"test_schema\",\\n    table=\"test_table\"\\n)\\ntable', 'code = import_module(\"transform.silver.mlv\", VERSION)\\n\\nprint(code, globals())'], 'Out': {6: <__main__.SilverIngestionInsertOnlyService object at 0x107bbe780>, 7: LakehouseTable(lakehouse='test_lakehouse', schema='test_schema', table='test_table')}, 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x107b89040>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x107b88e90>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x107b88e90>, 'open': <function open at 0x10678b560>, '_': LakehouseTable(lakehouse='test_lakehouse', schema='test_schema', table='test_table'), '__': <__main__.SilverIngestionInsertOnlyService object at 0x107bbe780>, '___': '', 'logging': <module 'logging' from '/Users/enricogoerlitz/opt/miniconda3/envs/py312/lib/python3.12/logging/__init__.py'>, 'os': <module 'os' (frozen)>, 'uuid': <module 'uuid' from '/Users/enricogoerlitz/opt/miniconda3/envs/py312/lib/python3.12/uuid.py'>, 'json': <module 'json' from '/Users/enricogoerlitz/opt/miniconda3/envs/py312/lib/python3.12/json/__init__.py'>, 'sys': <module 'sys' (built-in)>, 'sc': None, 'spark': <pyspark.sql.session.SparkSession object at 0x10c352540>, 'sqlContext': None, 'spark_session_id': '', 'create_spark_exception': None, 'script_version': '1.0.7', 'display': <function custom_display at 0x107be0540>, 'HTML': <class 'IPython.core.display.HTML'>, 'ipython_display': <function display at 0x1065b42c0>, 'fabric_envs': ['trident-spark-kernel', 'lighter', 'synapse-spark-kernel', 'fabric-synapse-runtime-1-1', 'fabric-synapse-runtime-1-2'], 'is_synapse_kernel': <function is_synapse_kernel at 0x107be04a0>, 'custom_display': <function custom_display at 0x107be0540>, 'textwrap': <module 'textwrap' from '/Users/enricogoerlitz/opt/miniconda3/envs/py312/lib/python3.12/textwrap.py'>, 'msal': <module 'msal' from '/Users/enricogoerlitz/opt/miniconda3/envs/py312/lib/python3.12/site-packages/msal/__init__.py'>, 'requests': <module 'requests' from '/Users/enricogoerlitz/opt/miniconda3/envs/py312/lib/python3.12/site-packages/requests/__init__.py'>, 'time': <module 'time' (built-in)>, 'Thread': <class 'threading.Thread'>, 'platform': <module 'platform' from '/Users/enricogoerlitz/opt/miniconda3/envs/py312/lib/python3.12/platform.py'>, 'os_name': 'Darwin', 'os_version': '24.5.0', 'extension_version': '0.2.0', 'refresh_token': None, 'DEFAULT_SCOPES': ['https://analysis.windows.net/powerbi/api/.default'], 'LOCAL_TOKEN_SERVER_PORT': '18888', 'LOCAL_TOKEN_SERVER_URL': 'http://localhost:18888/token', 'Config': <class '__main__.Config'>, 'PipConfig': <class '__main__.PipConfig'>, 'SessionStatus': <class '__main__.SessionStatus'>, 'SparkStatementStatus': <class '__main__.SparkStatementStatus'>, 'OutPut': <class '__main__.OutPut'>, 'Authentication': <class '__main__.Authentication'>, 'MWCClient': <class '__main__.MWCClient'>, 'where_json': <function where_json at 0x107be1080>, '__vsc_ipynb_file__': '/Users/enricogoerlitz/Developer/fabricengineer-py/notebooks/02 import exec.ipynb', '__DW_SCOPE__': {'output_cache': {}, 'output-f704a7d9-3c36-493c-bb2f-194a04425a39': {'enable_for_types': <function __DW_OUTPUT_FORMATTER__.<locals>.enable_formatter_for_types at 0x107e13a60>, 'disable': <function __DW_OUTPUT_FORMATTER__.<locals>.disable_formatter at 0x107e13b00>}}, '_i': 'table = LakehouseTable(\\n    lakehouse=\"test_lakehouse\",\\n    schema=\"test_schema\",\\n    table=\"test_table\"\\n)\\ntable', '_ii': 'etl', '_iii': 'exec(code)', '_i1': 'import sys\\n\\nsys.path.append(\"../src\")\\n\\nfrom fabricengineer.import_module.import_module import import_module\\n\\nVERSION = \"0.0.9\"', 'import_module': <function import_module at 0x1067a80e0>, 'VERSION': '0.0.9', '_i2': 'import os\\nfrom pyspark.sql import SparkSession\\n\\n\\nclass NotebookUtilsFSMock:\\n    def _get_path(self, file: str) -> str:\\n        return os.path.join(os.getcwd(), file)\\n\\n    def exists(self, path: str) -> bool:\\n        return os.path.exists(self._get_path(path))\\n\\n    def put(\\n        self,\\n        file: str,\\n        content: str,\\n        overwrite: bool = False\\n    ) -> None:\\n        path = self._get_path(file)\\n        os.makedirs(os.path.dirname(path), exist_ok=True)\\n\\n        if os.path.exists(path) and not overwrite:\\n            raise FileExistsError(f\"File {path} already exists and overwrite is set to False.\")\\n        with open(path, \\'w\\') as f:\\n            f.write(content)\\n\\n\\nclass NotebookUtilsMock:\\n    def __init__(self):\\n        self.fs = NotebookUtilsFSMock()\\n\\nglobal spark\\nspark: SparkSession = SparkSession.builder.appName(\"PlaygroundSparkSession\").getOrCreate()\\n\\nglobal notebookutils\\nnotebookutils = NotebookUtilsMock()', 'SparkSession': <class 'pyspark.sql.session.SparkSession'>, 'NotebookUtilsFSMock': <class '__main__.NotebookUtilsFSMock'>, 'NotebookUtilsMock': <class '__main__.NotebookUtilsMock'>, '__annotations__': {'spark': <class 'pyspark.sql.session.SparkSession'>}, 'notebookutils': <__main__.NotebookUtilsMock object at 0x10c2bca70>, '_i3': 'code = import_module(\"transform.silver.insertonly\", VERSION)\\n\\nprint(code)', 'code': 'from typing import Any\\nfrom pyspark.sql import DataFrame, SparkSession\\nfrom fabricengineer.logging.logger import logger\\n\\n\\ndef to_spark_sql(sql: str) -> str:\\n    return sql \\\\\\n            .replace(\"[\", \"`\") \\\\\\n            .replace(\"]\", \"`\")\\n\\n\\nclass MaterializedLakeView:\\n    def __init__(\\n        self,\\n        lakehouse: str = None,\\n        schema: str = None,\\n        table: str = None,\\n        table_suffix: str = \"_mlv\",\\n        spark_: SparkSession = None,\\n        notebookutils_: Any = None,\\n        is_testing_mock: bool = False\\n    ) -> None:\\n        self.init(\\n            lakehouse=lakehouse,\\n            schema=schema,\\n            table=table,\\n            table_suffix=table_suffix,\\n            spark_=spark_,\\n            notebookutils_=notebookutils_,\\n            is_testing_mock=is_testing_mock\\n        )\\n\\n    def init(\\n        self,\\n        lakehouse: str,\\n        schema: str,\\n        table: str,\\n        table_suffix: str = \"_mlv\",\\n        spark_: SparkSession = None,\\n        notebookutils_: Any = None,\\n        is_testing_mock: bool = False\\n    ) -> \\'MaterializedLakeView\\':\\n        \"\"\"Initializes the MaterializedLakeView instance.\\n\\n        Args:\\n            lakehouse (str): The lakehouse name.\\n            schema (str): The schema name.\\n            table (str): The table name.\\n            table_suffix (str, optional): The table suffix. Defaults to \"_mlv\".\\n            spark_ (SparkSession, optional): The SparkSession instance. Defaults to None.\\n            notebookutils_ (Any, optional): The NotebookUtils instance. Defaults to None.\\n            is_testing_mock (bool, optional): Whether the instance is a testing mock. Defaults to False.\\n\\n        Returns:\\n            MaterializedLakeView: The initialized MaterializedLakeView instance.\\n        \"\"\"\\n        self._lakehouse = lakehouse\\n        self._schema = schema\\n        self._table = table\\n        self._table_suffix = table_suffix\\n        self._is_testing_mock = is_testing_mock\\n\\n        # \\'spark\\' and \\'notebookutils\\' are available in Fabric notebook\\n        self._spark = self._get_init_spark(spark_)\\n        self._notebookutils = self._get_init_notebookutils(notebookutils_)\\n        return self\\n\\n    def _get_init_spark(self, spark_: SparkSession) -> SparkSession | None:\\n        \"\"\"Initializes the SparkSession instance.\\n        If a SparkSession is provided, it is returned. Otherwise, it tries to use the global \\'spark\\' variable.\\n\\n        Args:\\n            spark_ (SparkSession): The SparkSession instance.\\n\\n        Returns:\\n            SparkSession | None: The initialized SparkSession instance or None.\\n        \"\"\"\\n        if isinstance(spark_, SparkSession):\\n            return spark_\\n        try:\\n            if spark is not None:  # noqa: F821 # type: ignore\\n                return spark  # noqa: F821 # type: ignore\\n            return spark_\\n        except Exception:\\n            return None\\n\\n    def _get_init_notebookutils(self, notebookutils_: Any) -> Any | None:\\n        \"\"\"Initializes the NotebookUtils instance.\\n        If a NotebookUtils instance is provided, it is returned. Otherwise, it tries to use the global \\'notebookutils\\' variable.\\n\\n        Args:\\n            notebookutils_ (Any): The NotebookUtils instance.\\n\\n        Returns:\\n            Any | None: The initialized NotebookUtils instance or None.\\n        \"\"\"\\n        if notebookutils_ is not None:\\n            return notebookutils_\\n        try:\\n            if notebookutils is not None:  # noqa: F821 # type: ignore\\n                return notebookutils  # noqa: F821 # type: ignore\\n            return None\\n        except Exception:\\n            return None\\n\\n    @property\\n    def lakehouse(self) -> str:\\n        if self._lakehouse is None:\\n            raise ValueError(\"Lakehouse is not initialized.\")\\n        return self._lakehouse\\n\\n    @property\\n    def schema(self) -> str:\\n        if self._schema is None:\\n            raise ValueError(\"Schema is not initialized.\")\\n        return self._schema\\n\\n    @property\\n    def table(self) -> str:\\n        if self._table is None:\\n            raise ValueError(\"Table is not initialized.\")\\n        return self._table\\n\\n    @property\\n    def table_suffix(self) -> str:\\n        return self._table_suffix\\n\\n    @property\\n    def spark(self) -> SparkSession:\\n        if self._spark is None:\\n            raise ValueError(\"SparkSession is not initialized.\")\\n        return self._spark\\n\\n    @property\\n    def notebookutils(self) -> Any:\\n        if self._notebookutils is None:\\n            raise ValueError(\"NotebookUtils is not initialized.\")\\n        return self._notebookutils\\n\\n    @property\\n    def table_name(self) -> str:\\n        table_suffix = self.table_suffix or \"\"\\n        return f\"{self.table}{table_suffix}\"\\n\\n    @property\\n    def file_path(self) -> str:\\n        path = f\"Files/mlv/{self.lakehouse}/{self.schema}/{self.table_name}.sql.txt\"\\n        return path\\n\\n    @property\\n    def table_path(self) -> str:\\n        table_path = f\"{self.lakehouse}.{self.schema}.{self.table_name}\"\\n        return table_path\\n\\n    @property\\n    def schema_path(self) -> str:\\n        schema_path = f\"{self.lakehouse}.{self.schema}\"\\n        return schema_path\\n\\n    def read_file(self) -> str | None:\\n        \"\"\"Reads the content of the SQL file from the specified lakehouse.\\n        If the file does not exist, it returns None.\\n\\n        Raises:\\n            RuntimeError: If the file cannot be read.\\n\\n        Returns:\\n            str | None: The content of the file or None if it doesn\\'t exist.\\n        \"\"\"\\n        path = self.file_path\\n        try:\\n            if not self.notebookutils.fs.exists(path):\\n                return None\\n            if self._is_testing_mock:\\n                with open(path, \"r\") as file:\\n                    return file.read()\\n            df = self.spark.read.text(path, wholetext=True)\\n            mlv_code = df.collect()[0][0]\\n            return mlv_code\\n        except Exception as e:\\n            raise RuntimeError(f\"Fehler beim Lesen der Datei: {e}\")\\n\\n    def write_file(self, sql: str) -> bool:\\n        \"\"\"Writes the SQL content to the specified file in a lakehouse.\\n        If the file already exists, it will be overwritten.\\n        If the file cannot be written, it raises a RuntimeError.\\n\\n        Args:\\n            sql (str): The SQL content to write.\\n\\n        Raises:\\n            RuntimeError: If the file cannot be written.\\n\\n        Returns:\\n            bool: True if the file was written successfully, False otherwise.\\n        \"\"\"\\n        try:\\n            result = self.notebookutils.fs.put(\\n                file=self.file_path,\\n                content=sql,\\n                overwrite=True\\n            )\\n            return result\\n        except Exception as e:\\n            raise RuntimeError(f\"Fehler beim Schreiben der Datei: {e}\")\\n\\n    def create_schema(self) -> DataFrame | None:\\n        \"\"\"Creates the schema in the lakehouse if it does not exist.\"\"\"\\n        create_schema = f\"CREATE SCHEMA IF NOT EXISTS {self.schema_path}\"\\n        logger.info(create_schema)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(create_schema)\\n\\n    def create(self, sql: str) -> DataFrame | None:\\n        \"\"\"Creates a Materialized Lake View (MLV) in the lakehouse with the given SQL query.\"\"\"\\n        self.create_schema()\\n\\n        create_mlv = f\"CREATE MATERIALIZED LAKE VIEW {self.table_path}\\\\nAS\\\\n{sql}\"\\n        logger.info(f\"CREATE MLV: {self.table_path}\")\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(create_mlv)\\n\\n    def drop(self) -> str:\\n        \"\"\"Drops the Materialized Lake View (MLV) if it exists.\"\"\"\\n        drop_mlv = f\"DROP MATERIALIZED LAKE VIEW IF EXISTS {self.table_path}\"\\n        logger.info(drop_mlv)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(drop_mlv)\\n\\n    def create_or_replace(self, sql: str, mock_is_existing: bool = None) -> DataFrame:\\n        \"\"\"Creates or replaces the Materialized Lake View (MLV) in the lakehouse.\\n\\n        Args:\\n            sql (str): The SQL query to create the MLV.\\n            mock_is_existing (bool, optional): If True, it simulates the existence of the MLV. Defaults to None.\\n\\n        Returns:\\n            DataFrame: The result of the create or replace operation.\\n        \"\"\"\\n        mlv_code_current = self.read_file()\\n        is_existing = (\\n            mock_is_existing\\n            if mock_is_existing is not None\\n            else self.spark.catalog.tableExists(self.table_path)\\n        )\\n\\n        if mlv_code_current is None and not is_existing:\\n            res = self.create(sql)\\n            self.write_file(sql)\\n            return res\\n\\n        elif mlv_code_current is None and is_existing:\\n            logger.warning(\"WARN: file=None, is_existing=True. RECREATE.\")\\n            self.drop()\\n            res = self.create(sql)\\n            self.write_file(sql)\\n            return res\\n\\n        elif sql == mlv_code_current and is_existing:\\n            logger.info(\"Nothing has changed.\")\\n            return None\\n\\n        logger.info(f\"REPLACE MLV: {self.table_path}\")\\n        self.drop()\\n        res = self.create(sql)\\n        self.write_file(sql)\\n        return res\\n\\n    def refresh(self, full_refresh: bool) -> DataFrame:\\n        \"\"\"Refreshes the Materialized Lake View (MLV) in the lakehouse.\"\"\"\\n        full_refresh_str = \"FULL\" if full_refresh else \"\"\\n        refresh_mlv = f\"REFRESH MATERIALIZED LAKE VIEW {self.table_path} {full_refresh_str}\"\\n        logger.info(refresh_mlv)\\n\\n        if self._is_testing_mock:\\n            return None\\n\\n        return self.spark.sql(refresh_mlv)\\n\\n    def to_dict(self) -> None:\\n        \"\"\"Returns a dictionary representation of the Materialized Lake View.\"\"\"\\n        return {\\n            \"lakehouse\": self.lakehouse,\\n            \"schema\": self.schema,\\n            \"table\": self.table,\\n            \"table_path\": self.table_path\\n        }\\n\\n\\nmlv = MaterializedLakeView()', '_i4': 'with open(\"./code.py\", \"w\") as f:\\n    f.write(code)', 'f': <_io.TextIOWrapper name='./code.py' mode='w' encoding='UTF-8'>, '_i5': 'exec(code)', 'datetime': <class 'datetime.datetime'>, 'Callable': typing.Callable, 'ABC': <class 'abc.ABC'>, 'abstractmethod': <function abstractmethod at 0x104cfe840>, 'dataclass': <function dataclass at 0x1058fbb00>, 'uuid4': <function uuid4 at 0x1061dad40>, 'DataFrame': <class 'pyspark.sql.dataframe.DataFrame'>, 'F': <module 'pyspark.sql.functions' from '/Users/enricogoerlitz/opt/miniconda3/envs/py312/lib/python3.12/site-packages/pyspark/sql/functions.py'>, 'T': <module 'pyspark.sql.types' from '/Users/enricogoerlitz/opt/miniconda3/envs/py312/lib/python3.12/site-packages/pyspark/sql/types.py'>, 'Window': <class 'pyspark.sql.window.Window'>, 'logger': <Logger fabricengineer (INFO)>, 'LakehouseTable': <class '__main__.LakehouseTable'>, 'generate_uuid': <function generate_uuid at 0x107e844a0>, 'ConstantColumn': <class '__main__.ConstantColumn'>, 'get_mock_table_path': <function get_mock_table_path at 0x10c33bce0>, 'BaseSilverIngestionService': <class '__main__.BaseSilverIngestionService'>, 'SilverIngestionInsertOnlyService': <class '__main__.SilverIngestionInsertOnlyService'>, 'etl': <__main__.SilverIngestionInsertOnlyService object at 0x107bbe780>, '_i6': 'etl', '_6': <__main__.SilverIngestionInsertOnlyService object at 0x107bbe780>, '_i7': 'table = LakehouseTable(\\n    lakehouse=\"test_lakehouse\",\\n    schema=\"test_schema\",\\n    table=\"test_table\"\\n)\\ntable', 'table': LakehouseTable(lakehouse='test_lakehouse', schema='test_schema', table='test_table'), '_7': LakehouseTable(lakehouse='test_lakehouse', schema='test_schema', table='test_table'), '_i8': 'code = import_module(\"transform.silver.mlv\", VERSION)\\n\\nprint(code, globals())'}\n"
     ]
    }
   ],
   "source": [
    "code = import_module(\"transform.silver.mlv\", VERSION)\n",
    "\n",
    "print(code, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "637aeca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(code, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7b8cfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[05.08.2025 13:04:02] [INFO] fabricengineer REPLACE MLV: TestLakehouse.schema.table_mlv\n",
      "[05.08.2025 13:04:02] [INFO] fabricengineer DROP MATERIALIZED LAKE VIEW IF EXISTS TestLakehouse.schema.table_mlv\n",
      "[05.08.2025 13:04:02] [INFO] fabricengineer CREATE SCHEMA IF NOT EXISTS TestLakehouse.schema\n",
      "[05.08.2025 13:04:02] [INFO] fabricengineer CREATE MLV: TestLakehouse.schema.table_mlv\n"
     ]
    }
   ],
   "source": [
    "mlv.init(\n",
    "    lakehouse=\"TestLakehouse\",\n",
    "    schema=\"schema\",\n",
    "    table=\"table\",\n",
    "    is_testing_mock=True\n",
    ")\n",
    "\n",
    "sql = \"SELECT * FROM TestLakehouse.schema.table2\"\n",
    "mlv.create_or_replace(sql, mock_is_existing=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
