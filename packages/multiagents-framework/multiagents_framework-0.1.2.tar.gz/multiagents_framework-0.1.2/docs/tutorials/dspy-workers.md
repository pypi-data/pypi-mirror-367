# Tutorial: Building DSPy-Powered Workers

Learn how to create intelligent workers powered by Large Language Models using DSPy integration.

## Learning Objectives

By the end of this tutorial, you will:
- Create DSPy-powered workers for LLM integration
- Understand structured input/output with DSPy signatures
- Build intelligent content generation workflows
- Handle LLM errors and edge cases
- Optimize LLM interactions for production use

## Prerequisites

- Completed [Basic Workflow Tutorial](basic-workflow.md)
- Understanding of Large Language Models
- OpenAI API key (or other LLM provider)
- Basic knowledge of prompt engineering

## What We're Building

An **Intelligent Content Processing Pipeline** that:
1. **Analyzes** customer feedback using sentiment analysis
2. **Summarizes** long documents with key insights
3. **Generates** personalized responses
4. **Translates** content to multiple languages
5. **Creates** structured reports

## Step 1: Set Up DSPy Environment

First, install and configure DSPy:

```bash
pip install dspy-ai openai
```

Create `dspy_content_pipeline.py`:

```python
import asyncio
import os
from datetime import datetime
from typing import Dict, Any, List

import dspy
from multiagents import worker, dspy_worker
from multiagents.core.factory import create_simple_framework
from multiagents.orchestrator.workflow import WorkflowBuilder

# Configure DSPy with OpenAI (replace with your preferred LLM)
openai_api_key = os.getenv("OPENAI_API_KEY", "your-api-key-here")

if not openai_api_key or openai_api_key == "your-api-key-here":
    print("âš ï¸  Please set your OPENAI_API_KEY environment variable")
    print("   export OPENAI_API_KEY='your-actual-api-key'")
    exit(1)

# Initialize DSPy with OpenAI
lm = dspy.OpenAI(
    model="gpt-3.5-turbo",
    api_key=openai_api_key,
    max_tokens=500
)
dspy.settings.configure(lm=lm)

print("âœ… DSPy configured with OpenAI GPT-3.5-turbo")
```

## Step 2: Basic DSPy Worker

Let's start with a simple sentiment analysis worker:

```python
@dspy_worker("analyze_sentiment")
async def analyze_sentiment_worker(context: Dict[str, Any]) -> Dict[str, Any]:
    """
    Analyze sentiment of customer feedback using DSPy.
    
    Input context:
        - feedback_text: text to analyze
        - customer_id: customer identifier
    
    Output (generated by DSPy):
        - sentiment: positive, negative, or neutral
        - confidence: confidence score (0-1)
        - key_emotions: list of detected emotions
        - summary: brief summary of the feedback
    """
    feedback_text = context.get("feedback_text", "")
    customer_id = context.get("customer_id", "unknown")
    
    if not feedback_text:
        return {
            "sentiment": "neutral",
            "confidence": 0.0,
            "key_emotions": [],
            "summary": "No feedback provided",
            "error": "Empty feedback text"
        }
    
    print(f"ðŸ§  Analyzing sentiment for customer {customer_id}")
    
    # DSPy will automatically process this context using the configured LLM
    # The framework handles the LLM interaction and returns structured results
    return {
        "feedback_text": feedback_text,
        "customer_id": customer_id,
        "analysis_type": "sentiment_analysis",
        "timestamp": datetime.utcnow().isoformat()
    }

# Post-processing hook for DSPy results
@analyze_sentiment_worker.post_process
async def post_process_sentiment(dspy_result: Dict[str, Any], original_context: Dict[str, Any]) -> Dict[str, Any]:
    """Post-process the DSPy-generated sentiment analysis."""
    
    print(f"ðŸ“Š Post-processing sentiment analysis results")
    
    # Validate and enhance DSPy results
    sentiment = dspy_result.get("sentiment", "neutral").lower()
    confidence = float(dspy_result.get("confidence", 0.5))
    
    # Ensure sentiment is one of the expected values
    if sentiment not in ["positive", "negative", "neutral"]:
        sentiment = "neutral"
        confidence = 0.0
    
    # Ensure confidence is within valid range
    confidence = max(0.0, min(1.0, confidence))
    
    # Add processing metadata
    enhanced_result = {
        **dspy_result,
        "sentiment": sentiment,
        "confidence": confidence,
        "requires_human_review": confidence < 0.7,
        "processing_timestamp": datetime.utcnow().isoformat(),
        "model_version": "gpt-3.5-turbo",
        "original_text_length": len(original_context.get("feedback_text", ""))
    }
    
    # Categorize for follow-up actions
    if sentiment == "negative" and confidence > 0.8:
        enhanced_result["requires_followup"] = True
        enhanced_result["priority"] = "high"
    elif sentiment == "positive" and confidence > 0.9:
        enhanced_result["requires_followup"] = False
        enhanced_result["priority"] = "low"
    else:
        enhanced_result["requires_followup"] = False
        enhanced_result["priority"] = "medium"
    
    return enhanced_result
```

## Step 3: Advanced DSPy Worker with Custom Signature

For more structured output, we can define custom DSPy signatures:

```python
class DocumentSummarySignature(dspy.Signature):
    """Summarize a document and extract key insights."""
    
    document_text = dspy.InputField(desc="The full document text to summarize")
    document_type = dspy.InputField(desc="Type of document (email, report, article, etc.)")
    target_audience = dspy.InputField(desc="Target audience for the summary")
    
    summary = dspy.OutputField(desc="Concise summary of the document (2-3 sentences)")
    key_points = dspy.OutputField(desc="List of 3-5 key points from the document")
    action_items = dspy.OutputField(desc="Any action items or next steps mentioned")
    urgency_level = dspy.OutputField(desc="Urgency level: low, medium, high, critical")
    estimated_read_time = dspy.OutputField(desc="Estimated reading time in minutes")

@dspy_worker("summarize_document", signature=DocumentSummarySignature)
async def summarize_document_worker(context: Dict[str, Any]) -> Dict[str, Any]:
    """
    Summarize documents with structured output using custom DSPy signature.
    
    Input context:
        - document_text: full document content
        - document_type: type of document
        - target_audience: who will read the summary
    
    Output (structured by DSPy signature):
        - summary: concise summary
        - key_points: list of key points
        - action_items: actionable items
        - urgency_level: urgency assessment
        - estimated_read_time: reading time estimate
    """
    document_text = context.get("document_text", "")
    document_type = context.get("document_type", "general")
    target_audience = context.get("target_audience", "general audience")
    
    if not document_text:
        return {
            "summary": "No document provided",
            "key_points": [],
            "action_items": [],
            "urgency_level": "low",
            "estimated_read_time": "0",
            "error": "Empty document text"
        }
    
    if len(document_text) < 100:
        return {
            "summary": "Document too short for meaningful summary",
            "key_points": ["Document is very brief"],
            "action_items": [],
            "urgency_level": "low",
            "estimated_read_time": "1",
            "warning": "Document shorter than 100 characters"
        }
    
    print(f"ðŸ“„ Summarizing {document_type} document ({len(document_text)} chars)")
    
    # Return context for DSPy processing with signature
    return {
        "document_text": document_text,
        "document_type": document_type,
        "target_audience": target_audience,
        "original_length": len(document_text),
        "word_count": len(document_text.split())
    }

@summarize_document_worker.post_process
async def post_process_summary(dspy_result: Dict[str, Any], original_context: Dict[str, Any]) -> Dict[str, Any]:
    """Post-process document summary results."""
    
    print(f"ðŸ“ Post-processing document summary")
    
    # Parse key_points if it's a string
    key_points = dspy_result.get("key_points", [])
    if isinstance(key_points, str):
        # Split by common delimiters
        key_points = [point.strip() for point in key_points.replace("- ", "").split("\n") if point.strip()]
    
    # Parse action_items if it's a string  
    action_items = dspy_result.get("action_items", [])
    if isinstance(action_items, str):
        action_items = [item.strip() for item in action_items.replace("- ", "").split("\n") if item.strip()]
    
    # Validate urgency level
    urgency = dspy_result.get("urgency_level", "medium").lower()
    if urgency not in ["low", "medium", "high", "critical"]:
        urgency = "medium"
    
    # Calculate compression ratio
    original_length = original_context.get("original_length", 0)
    summary_length = len(dspy_result.get("summary", ""))
    compression_ratio = summary_length / original_length if original_length > 0 else 0
    
    return {
        **dspy_result,
        "key_points": key_points,
        "action_items": action_items,
        "urgency_level": urgency,
        "key_points_count": len(key_points),
        "action_items_count": len(action_items),
        "compression_ratio": round(compression_ratio, 3),
        "processing_timestamp": datetime.utcnow().isoformat()
    }
```

## Step 4: Response Generation Worker

Create personalized responses based on analysis:

```python
class ResponseGenerationSignature(dspy.Signature):
    """Generate a personalized response based on sentiment analysis."""
    
    original_feedback = dspy.InputField(desc="Original customer feedback")
    sentiment = dspy.InputField(desc="Detected sentiment (positive, negative, neutral)")
    customer_name = dspy.InputField(desc="Customer's name")
    issue_category = dspy.InputField(desc="Category of the issue or feedback")
    
    response_tone = dspy.OutputField(desc="Appropriate tone for response (formal, friendly, apologetic, grateful)")
    response_text = dspy.OutputField(desc="Personalized response text")
    suggested_actions = dspy.OutputField(desc="Suggested follow-up actions")
    escalation_needed = dspy.OutputField(desc="Whether this needs escalation (yes/no)")

@dspy_worker("generate_response", signature=ResponseGenerationSignature)
async def generate_response_worker(context: Dict[str, Any]) -> Dict[str, Any]:
    """
    Generate personalized customer responses based on sentiment analysis.
    
    Input context:
        - original_feedback: customer's feedback
        - sentiment: analyzed sentiment
        - customer_name: customer's name
        - issue_category: category of feedback
    
    Output (structured by DSPy signature):
        - response_tone: appropriate tone
        - response_text: generated response
        - suggested_actions: follow-up actions
        - escalation_needed: escalation flag
    """
    original_feedback = context.get("original_feedback", "")
    sentiment = context.get("sentiment", "neutral")
    customer_name = context.get("customer_name", "Valued Customer")
    issue_category = context.get("issue_category", "general")
    
    print(f"âœï¸  Generating response for {customer_name} ({sentiment} sentiment)")
    
    return {
        "original_feedback": original_feedback,
        "sentiment": sentiment,
        "customer_name": customer_name,
        "issue_category": issue_category,
        "response_context": f"{sentiment} feedback about {issue_category}"
    }

@generate_response_worker.post_process
async def post_process_response(dspy_result: Dict[str, Any], original_context: Dict[str, Any]) -> Dict[str, Any]:
    """Post-process generated response."""
    
    print(f"ðŸ“§ Post-processing generated response")
    
    # Parse suggested_actions if it's a string
    suggested_actions = dspy_result.get("suggested_actions", [])
    if isinstance(suggested_actions, str):
        suggested_actions = [action.strip() for action in suggested_actions.split("\n") if action.strip()]
    
    # Validate escalation_needed
    escalation = dspy_result.get("escalation_needed", "no").lower()
    escalation_needed = escalation in ["yes", "true", "1"]
    
    # Validate response tone
    tone = dspy_result.get("response_tone", "friendly").lower()
    valid_tones = ["formal", "friendly", "apologetic", "grateful", "professional"]
    if tone not in valid_tones:
        tone = "friendly"
    
    # Calculate response metrics
    response_text = dspy_result.get("response_text", "")
    
    return {
        **dspy_result,
        "suggested_actions": suggested_actions,
        "escalation_needed": escalation_needed,
        "response_tone": tone,
        "response_length": len(response_text),
        "word_count": len(response_text.split()),
        "actions_count": len(suggested_actions),
        "generated_at": datetime.utcnow().isoformat()
    }
```

## Step 5: Translation Worker

Add multi-language support:

```python
@dspy_worker("translate_content")
async def translate_content_worker(context: Dict[str, Any]) -> Dict[str, Any]:
    """
    Translate content to specified language using DSPy.
    
    Input context:
        - content: text to translate
        - target_language: target language for translation
        - source_language: source language (optional)
        - content_type: type of content for context
    
    Output (generated by DSPy):
        - translated_text: translated content
        - source_language_detected: detected source language
        - translation_quality: quality assessment
        - cultural_notes: any cultural adaptation notes
    """
    content = context.get("content", "")
    target_language = context.get("target_language", "Spanish")
    source_language = context.get("source_language", "auto-detect")
    content_type = context.get("content_type", "general")
    
    if not content:
        return {
            "translated_text": "",
            "source_language_detected": "unknown",
            "translation_quality": "low",
            "cultural_notes": "No content to translate",
            "error": "Empty content"
        }
    
    print(f"ðŸŒ Translating {content_type} to {target_language}")
    
    return {
        "content": content,
        "target_language": target_language,
        "source_language": source_language,
        "content_type": content_type,
        "original_length": len(content)
    }

@translate_content_worker.post_process  
async def post_process_translation(dspy_result: Dict[str, Any], original_context: Dict[str, Any]) -> Dict[str, Any]:
    """Post-process translation results."""
    
    print(f"ðŸ”„ Post-processing translation")
    
    translated_text = dspy_result.get("translated_text", "")
    original_length = original_context.get("original_length", 0)
    
    # Calculate translation metrics
    translation_ratio = len(translated_text) / original_length if original_length > 0 else 0
    
    # Validate quality assessment
    quality = dspy_result.get("translation_quality", "medium").lower()
    if quality not in ["low", "medium", "high", "excellent"]:
        quality = "medium"
    
    return {
        **dspy_result,
        "translation_quality": quality,
        "translation_ratio": round(translation_ratio, 3),
        "translated_length": len(translated_text),
        "translated_at": datetime.utcnow().isoformat()
    }
```

## Step 6: Report Generation Worker

Create comprehensive reports from all the analysis:

```python
@dspy_worker("generate_report")
async def generate_report_worker(context: Dict[str, Any]) -> Dict[str, Any]:
    """
    Generate a comprehensive report from content analysis results.
    
    Input context:
        - sentiment_analysis: sentiment analysis results
        - document_summary: document summary results
        - response_generated: generated response results
        - translation_results: translation results (optional)
    
    Output (generated by DSPy):
        - executive_summary: high-level summary
        - detailed_findings: detailed analysis findings
        - recommendations: actionable recommendations
        - risk_assessment: potential risks identified
        - next_steps: suggested next steps
    """
    sentiment_analysis = context.get("sentiment_analysis", {})
    document_summary = context.get("document_summary", {})
    response_generated = context.get("response_generated", {})
    translation_results = context.get("translation_results", {})
    
    print(f"ðŸ“Š Generating comprehensive analysis report")
    
    # Compile all analysis data for DSPy processing
    return {
        "sentiment_analysis": sentiment_analysis,
        "document_summary": document_summary,
        "response_generated": response_generated,
        "translation_results": translation_results,
        "report_type": "content_analysis",
        "analysis_timestamp": datetime.utcnow().isoformat()
    }

@generate_report_worker.post_process
async def post_process_report(dspy_result: Dict[str, Any], original_context: Dict[str, Any]) -> Dict[str, Any]:
    """Post-process generated report."""
    
    print(f"ðŸ“‹ Post-processing analysis report")
    
    # Parse recommendations if it's a string
    recommendations = dspy_result.get("recommendations", [])
    if isinstance(recommendations, str):
        recommendations = [rec.strip() for rec in recommendations.split("\n") if rec.strip()]
    
    # Parse next_steps if it's a string
    next_steps = dspy_result.get("next_steps", [])
    if isinstance(next_steps, str):
        next_steps = [step.strip() for step in next_steps.split("\n") if step.strip()]
    
    # Add report metadata
    return {
        **dspy_result,
        "recommendations": recommendations,
        "next_steps": next_steps,
        "recommendations_count": len(recommendations),
        "next_steps_count": len(next_steps),
        "report_generated_at": datetime.utcnow().isoformat(),
        "report_id": f"RPT-{int(datetime.utcnow().timestamp())}"
    }
```

## Step 7: Define the Content Processing Workflow

```python
def create_content_processing_workflow():
    """Create intelligent content processing workflow with DSPy workers."""
    return (WorkflowBuilder("intelligent_content_processing")
        .add_step("analyze_sentiment", "analyze_sentiment")
        .add_step("summarize_document", "summarize_document")
        .add_step("generate_response", "generate_response")
        .add_step("translate_content", "translate_content")
        .add_step("generate_report", "generate_report")
        .build())
```

## Step 8: Create Test Data and Main Function

```python
def create_test_data():
    """Create test data for the content processing pipeline."""
    return {
        "feedback_text": """
        I am extremely disappointed with the recent service I received. The product 
        arrived three days late, was damaged upon arrival, and when I called customer 
        service, I was put on hold for over an hour. The representative was unhelpful 
        and seemed annoyed with my concerns. This is not the quality of service I 
        expect from your company. I have been a loyal customer for over 5 years, 
        but I'm seriously considering taking my business elsewhere if this continues.
        """,
        "document_text": """
        QUARTERLY BUSINESS REVIEW - Q4 2023
        
        Executive Summary:
        Our Q4 performance exceeded expectations with a 15% revenue increase compared to Q3.
        Customer satisfaction scores improved to 4.2/5.0, though we identified key areas 
        for improvement in our support processes.
        
        Key Metrics:
        - Revenue: $2.4M (+15% from Q3)
        - New customers: 1,247 (+8% from Q3)
        - Customer retention: 92% (+3% from Q3)
        - Support ticket resolution time: Average 18 hours (-2 hours from Q3)
        
        Challenges Identified:
        1. Shipping delays affecting 12% of orders
        2. Support response times still above target
        3. Product quality issues in electronics category
        
        Recommended Actions:
        1. Implement new logistics partner by Q1 2024
        2. Hire 3 additional support representatives
        3. Conduct quality audit of electronics suppliers
        4. Launch customer feedback program
        
        The board meeting is scheduled for January 15th to discuss these findings and 
        approve the recommended budget allocations.
        """,
        "customer_id": "CUST-12345",
        "customer_name": "Sarah Johnson",
        "issue_category": "delivery_and_support",
        "document_type": "business_report",
        "target_audience": "executive_team",
        "target_language": "Spanish",
        "content_type": "customer_feedback"
    }

async def main():
    """Execute the intelligent content processing pipeline."""
    print("ðŸš€ Intelligent Content Processing with DSPy")
    print("=" * 60)
    
    # Create workflow
    workflow = create_content_processing_workflow()
    
    # Set up framework
    event_bus, worker_manager, orchestrator = await create_simple_framework(workflow)
    
    try:
        # Start framework
        print("ðŸ“¡ Starting framework...")
        await event_bus.start()
        
        # Register workers
        print("ðŸ‘· Registering DSPy workers...")
        workers = [
            analyze_sentiment_worker,
            summarize_document_worker,
            generate_response_worker,
            translate_content_worker,
            generate_report_worker
        ]
        
        for worker_func in workers:
            worker_manager.register(worker_func)
        
        await worker_manager.start()
        await orchestrator.start()
        
        print(f"âœ“ Registered {len(workers)} DSPy workers")
        
        # Create test data
        test_data = create_test_data()
        
        print("\nðŸ“„ Processing content with AI...")
        print(f"Feedback length: {len(test_data['feedback_text'])} characters")
        print(f"Document length: {len(test_data['document_text'])} characters")
        
        # Execute workflow
        transaction_id = await orchestrator.execute_workflow(
            "intelligent_content_processing",
            test_data
        )
        
        print(f"âœ“ Workflow started with ID: {transaction_id}")
        
        # Monitor progress with detailed output
        completed_states = {"completed", "failed", "compensated", "cancelled"}
        step_count = 0
        
        while True:
            status = await orchestrator.get_status(transaction_id)
            
            current_step = status.get('current_step')
            if current_step and step_count < len(status.get('step_results', {})):
                step_count = len(status.get('step_results', {}))
                print(f"  ðŸ”„ Processing: {current_step}")
            
            if status['state'] in completed_states:
                print(f"\nâœ… Pipeline {status['state'].upper()}")
                
                if status['step_results']:
                    print("\nðŸ“Š PROCESSING RESULTS")
                    print("=" * 60)
                    
                    # Display sentiment analysis
                    if 'analyze_sentiment' in status['step_results']:
                        sentiment_result = status['step_results']['analyze_sentiment']
                        print(f"\nðŸ§  SENTIMENT ANALYSIS:")
                        print(f"  Sentiment: {sentiment_result.get('sentiment', 'N/A').title()}")
                        print(f"  Confidence: {sentiment_result.get('confidence', 0):.2f}")
                        print(f"  Priority: {sentiment_result.get('priority', 'N/A')}")
                        print(f"  Requires Follow-up: {sentiment_result.get('requires_followup', False)}")
                    
                    # Display document summary
                    if 'summarize_document' in status['step_results']:
                        summary_result = status['step_results']['summarize_document']
                        print(f"\nðŸ“„ DOCUMENT SUMMARY:")
                        print(f"  Summary: {summary_result.get('summary', 'N/A')}")
                        print(f"  Key Points: {len(summary_result.get('key_points', []))}")
                        print(f"  Action Items: {len(summary_result.get('action_items', []))}")
                        print(f"  Urgency: {summary_result.get('urgency_level', 'N/A')}")
                        print(f"  Compression Ratio: {summary_result.get('compression_ratio', 0):.3f}")
                    
                    # Display response generation
                    if 'generate_response' in status['step_results']:
                        response_result = status['step_results']['generate_response']
                        print(f"\nâœï¸  GENERATED RESPONSE:")
                        print(f"  Tone: {response_result.get('response_tone', 'N/A')}")
                        print(f"  Length: {response_result.get('response_length', 0)} characters")
                        print(f"  Escalation Needed: {response_result.get('escalation_needed', False)}")
                        print(f"  Actions Count: {response_result.get('actions_count', 0)}")
                    
                    # Display translation
                    if 'translate_content' in status['step_results']:
                        translation_result = status['step_results']['translate_content']
                        print(f"\nðŸŒ TRANSLATION:")
                        print(f"  Target Language: {translation_result.get('target_language', 'N/A')}")
                        print(f"  Quality: {translation_result.get('translation_quality', 'N/A')}")
                        print(f"  Translation Ratio: {translation_result.get('translation_ratio', 0):.3f}")
                    
                    # Display report
                    if 'generate_report' in status['step_results']:
                        report_result = status['step_results']['generate_report']
                        print(f"\nðŸ“Š ANALYSIS REPORT:")
                        print(f"  Report ID: {report_result.get('report_id', 'N/A')}")
                        print(f"  Recommendations: {report_result.get('recommendations_count', 0)}")
                        print(f"  Next Steps: {report_result.get('next_steps_count', 0)}")
                
                break
            
            await asyncio.sleep(2)
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        print("\nðŸ§¹ Shutting down...")
        await worker_manager.stop()
        await orchestrator.stop()
        await event_bus.stop()
        print("âœ“ Shutdown complete")

if __name__ == "__main__":
    asyncio.run(main())
```

## Step 9: Run and Test

Execute the tutorial:

```bash
export OPENAI_API_KEY="your-actual-api-key"
python dspy_content_pipeline.py
```

Expected output:

```
ðŸš€ Intelligent Content Processing with DSPy
============================================================
âœ… DSPy configured with OpenAI GPT-3.5-turbo
ðŸ“¡ Starting framework...
ðŸ‘· Registering DSPy workers...
âœ“ Registered 5 DSPy workers

ðŸ“„ Processing content with AI...
Feedback length: 434 characters
Document length: 1247 characters
âœ“ Workflow started with ID: TX-20240103-123456-dspy

  ðŸ”„ Processing: analyze_sentiment
ðŸ§  Analyzing sentiment for customer CUST-12345
ðŸ“Š Post-processing sentiment analysis results
  ðŸ”„ Processing: summarize_document
ðŸ“„ Summarizing business_report document (1247 chars)
ðŸ“ Post-processing document summary
  ðŸ”„ Processing: generate_response
âœï¸  Generating response for Sarah Johnson (negative sentiment)
ðŸ“§ Post-processing generated response
  ðŸ”„ Processing: translate_content
ðŸŒ Translating customer_feedback to Spanish
ðŸ”„ Post-processing translation
  ðŸ”„ Processing: generate_report
ðŸ“Š Generating comprehensive analysis report
ðŸ“‹ Post-processing analysis report

âœ… Pipeline COMPLETED

ðŸ“Š PROCESSING RESULTS
============================================================

ðŸ§  SENTIMENT ANALYSIS:
  Sentiment: Negative
  Confidence: 0.92
  Priority: high
  Requires Follow-up: True

ðŸ“„ DOCUMENT SUMMARY:
  Summary: Q4 2023 exceeded expectations with 15% revenue growth and improved customer satisfaction, but shipping delays and support issues need addressing.
  Key Points: 5
  Action Items: 4
  Urgency: high
  Compression Ratio: 0.089

âœï¸  GENERATED RESPONSE:
  Tone: apologetic
  Length: 267 characters
  Escalation Needed: True
  Actions Count: 3

ðŸŒ TRANSLATION:
  Target Language: Spanish
  Quality: high
  Translation Ratio: 1.124

ðŸ“Š ANALYSIS REPORT:
  Report ID: RPT-1704123456
  Recommendations: 6
  Next Steps: 4
```

## Understanding DSPy Integration

### How DSPy Workers Differ from Regular Workers

1. **LLM Processing**: DSPy workers automatically send context to configured LLMs
2. **Structured Output**: Can use signatures to define expected output format
3. **Post-Processing**: Allow post-processing hooks to validate and enhance results
4. **Error Handling**: Framework handles LLM timeouts and API errors
5. **Optimization**: DSPy can optimize prompts for better results

### Best Practices for DSPy Workers

1. **Input Validation**: Always validate inputs before sending to LLM
2. **Post-Processing**: Use post-processing to validate and structure LLM outputs  
3. **Error Handling**: Handle empty responses and API failures gracefully
4. **Cost Management**: Monitor token usage and implement caching where appropriate
5. **Quality Control**: Add confidence scores and human review flags

## Verification

Test the pipeline with different types of content:

```python
# Test with positive feedback
positive_test = {
    "feedback_text": "Amazing service! The product arrived early and exceeded my expectations.",
    "customer_name": "John Smith",
    # ... other fields
}

# Test with neutral/informational content
neutral_test = {
    "feedback_text": "I have a question about my order status. Can you provide an update?",
    "customer_name": "Maria Garcia",
    # ... other fields
}
```

## Common Issues

### API Key Issues
```
Error: OpenAI API key not set
```
**Solution**: Set environment variable: `export OPENAI_API_KEY="your-key"`

### DSPy Configuration Errors
```
Error: No language model configured
```
**Solution**: Ensure `dspy.settings.configure(lm=lm)` is called before workers

### Empty LLM Responses
- Check input validation in workers
- Verify LLM model is available and responding
- Check token limits and request formatting

### Post-Processing Failures
- Ensure post-processing functions handle missing fields
- Validate LLM output structure before processing
- Use try/catch blocks in post-processing

## What You Learned

âœ… **DSPy Integration**: How to create LLM-powered workers  
âœ… **Structured Output**: Using DSPy signatures for consistent results  
âœ… **Post-Processing**: Validating and enhancing LLM outputs  
âœ… **Error Handling**: Managing LLM failures and edge cases  
âœ… **Complex Workflows**: Building multi-step AI processing pipelines  
âœ… **Production Patterns**: Best practices for LLM integration

## Next Steps

Continue with advanced topics:

1. **[Advanced Patterns Tutorial](advanced-patterns.md)** - Implement conditional logic and parallel processing
2. **[Production Deployment Tutorial](production-deployment.md)** - Scale DSPy workers for production
3. **[Custom Extensions Tutorial](custom-extensions.md)** - Build custom DSPy modules and optimizers

## Performance Optimization

For production use:

1. **Caching**: Cache LLM responses for repeated inputs
2. **Batching**: Process multiple requests in batches
3. **Model Selection**: Choose appropriate models for each task
4. **Prompt Optimization**: Use DSPy's automatic prompt optimization
5. **Error Recovery**: Implement fallback strategies for LLM failures

This tutorial demonstrates how to build sophisticated AI-powered workflows that can understand, process, and generate intelligent responses to complex content!