from pathlib import Path

from .tools import (
    ascend_tyche_data as ascend_tyche_data,
    resolve_p53_marker as resolve_p53_marker,
    verify_session_checksum as verify_session_checksum,
    generate_project_manifest as generate_project_manifest,
)
from .server import (
    Server as Server,
    JupyterJob as JupyterJob,
    generate_server_credentials as generate_server_credentials,
)
from .data_classes import (
    SessionData as SessionData,
    TrackerFileNames as TrackerFileNames,
    get_processing_tracker as get_processing_tracker,
)

def verify_session_integrity(
    session_path: Path,
    manager_id: int,
    create_processed_directories: bool,
    processed_data_root: Path | None,
    update_manifest: bool,
) -> None:
    """Checks the integrity of the target session's raw data (contents of the raw_data directory).

    This command assumes that the data has been checksummed during acquisition and contains an ax_checksum.txt file
    that stores the data checksum generated before transferring the data to the long-term storage destination. This
    function always verified the integrity of the 'raw_data' directory. It does not work with 'processed_data' or any
    other directories. If the session data was corrupted, the command removes the 'telomere.bin' file, marking the
    session as 'incomplete' and automatically excluding it from all further automated processing runtimes. If the
    session data is intact, it generates a 'verified.bin' marker file inside the session's raw_data folder.

    The command is also used by Sun lab data acquisition systems to generate the processed data hierarchy for each
    processed session. This use case is fully automated and should not be triggered manually by the user.
    """

def generate_project_manifest_file(
    project_path: Path, output_directory: Path, processed_data_root: Path | None
) -> None:
    """Generates the manifest .feather file that provides information about the data-processing state of all available
    project sessions.

    The manifest file is typically used when batch-processing session data on the remote compute server. It contains the
    comprehensive snapshot of the available project's data in a table-compatible format that can also be transferred
    between machines (as it is cached in a file).
    """

def generate_server_credentials_file(
    output_directory: Path,
    host: str,
    username: str,
    password: str,
    storage_root: str,
    working_root: str,
    shared_directory_name: str,
) -> None:
    """Generates a new server_credentials.yaml file under the specified directory, using input information.

    This command is used to set up access to compute servers and clusters on new machines (PCs). The data stored inside
    the server_credentials.yaml file generated by this command is used by the Server and Job classes used in many Sun
    lab data processing libraries.
    """

def ascend_tyche_directory(input_directory: Path) -> None:
    """Restructures old Tyche project data to use the modern Sun lab data structure and uploads them to the processing
    server.

    This command is used to convert ('ascend') the old Tyche project data to the modern Sun lab structure. After
    ascension, the data can be processed and analyzed using all modern Sun lab (sl-) tools and libraries. Note, this
    process expects the input data to be preprocessed using an old Sun lab mesoscope data preprocessing pipeline. It
    will not work for any other project or data. Also, this command will only work on a machine (PC) that belongs to a
    valid Sun lab data acquisition system, such as VRPC of the Mesoscope-VR system.
    """

def start_jupyter_server(
    credentials_path: Path, name: str, environment: str, directory: Path, cores: int, memory: int, time: int, port: int
) -> None:
    """Starts an interactive Jupyter session on the remote Sun lab server.

    This command should be used to run Jupyter lab and notebooks sessions on the remote Sun lab server. Since all lab
    data is stored on the server, this allows running light interactive analysis sessions on the same node as the data,
    while leveraging considerable compute resources of the server.

    Calling this command initializes a SLURM session that runs the interactive Jupyter server. Since this server
    directly competes for resources with all other headless jobs running on the server, it is imperative that each
    jupyter runtime uses only the minimum amount of resources and run-time as necessary. Do not use this command to run
    heavy data processing pipelines! Instead, consult with library documentation and use the headless Job class.
    """

def resolve_dataset_marker(
    session_path: Path,
    create_processed_directories: bool,
    processed_data_root: Path | None,
    remove: bool,
    update_manifest: bool,
) -> None:
    """Depending on configuration, either creates or removes the p53.bin marker from the target session.

    The p53.bin marker determines whether the session is ready for dataset integration. When the marker exists,
    processing pipelines are not allowed to work with the session data, ensuring that all processed data remains
    unchanged. If the marker does not exist, dataset integration pipelines are not allowed to work with the session
    data, enabling processing pipelines to safely modify the data at any time.
    """
