"""This module contains the high-level API for the multi-day suite2p processing pipeline. Primarily, it
exposes two ways of running the multi-day pipeline on data. The first way is to use the 'run_s2p_multiday' function,
which executes the entire pipeline on the input data. The second way is to call the 'discover_multiday_cells' and
'extract_multiday_fluorescence' functions to execute specific pipeline steps. The latter stepwise execution is helpful
when running suite2p on remote compute clusters to efficiently parallelize processing steps. Regardless of the runtime
mode, all functions require the 'ops_path' argument, which is generated by running the 'resolve_multiday_ops' function.
"""

import os
import shutil as sh
from typing import Any
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

from tqdm import tqdm
import numpy as np
from natsort import natsorted
from ataraxis_time import PrecisionTimer
from ataraxis_base_utilities import LogLevel, console, ensure_directory_exists

from .version import version, sl_version, python_version
from .multiday import (
    import_sessions,
    register_sessions,
    extract_session_traces,
    export_masks_and_images,
    generate_template_masks,
    backward_transform_masks,
    extract_unique_components,
)
from .configuration import MultiDayS2PConfiguration, SingleDayS2PConfiguration, generate_default_multiday_ops


def _resolve_session_data(session_data: tuple[int, str, str, Path, Path]) -> tuple[int, Path, dict[str, Any]]:
    """Resolves the multi-day configuration data and output data hierarchy for a given session.

    This worker function is used as part of the main 'resolve_multiday_ops' function to efficiently process multiple
    sessions in parallel.

    Args:
        session_data: The tuple of (index, session_folder, session_id, single_day_path, multi_day_path)

    Returns:
        The tuple of (session_index, combined_path, original_ops_dict)
    """
    # Unpacks the input data
    i, folder_str, session_id, single_day_path, multi_day_path = session_data

    # Ensures each folder is cast to Path before working with it
    folder = Path(folder_str)

    # There should be exactly one 'combined' folder for each session folder. If this is false, then the folder path
    # is not valid
    combined_path_list = [path for path in folder.rglob("combined")]
    if len(combined_path_list) != 1:
        message = (
            f"Unable to run multi-day processing using the input parameters. Specifically, expected each path in "
            f"'session_folders' configuration class field to point to a directory tree with exactly one 'combined' "
            f"folder nested under the input session directory. Instead, encountered at least one path pointing to "
            f"a directory tree with {len(combined_path_list)} 'combined' folders."
        )
        console.error(message=message, error=RuntimeError)
        raise RuntimeError(message)  # Fallback to appease mypy, should not be reachable
    else:
        combined_path = combined_path_list[0]

    # Extracts the path to the 'combined' folder parent: the root suite2p output folder. Uses the extracted path to
    # reconfigure both the root and the specific output folder paths fields of the processed session's single-day
    # configuration dictionary. This ensures that the single-day configuration parameters point to the correct
    # single-day session data location, in case the session data was moved between single-day and multi-day
    # runtimes.
    suite2p_path = combined_path.parent
    original_ops: dict[str, Any] = SingleDayS2PConfiguration.from_yaml(  # type: ignore
        combined_path.parent.joinpath("single_day_s2p_configuration.yaml")
    ).to_ops()
    original_ops["save_folder"] = str(suite2p_path.stem)
    original_ops["save_path0"] = str(suite2p_path.parent)

    # Copies every single-day output file from the 'combined' folder to the single-day session folder under the
    # multi-day output directory. Primarily, this is used to cache the single-day F, Fneu, and spks outputs.
    for path in combined_path.rglob("*"):
        if path.is_file() and path.suffix.lower() in [".npy", ".yaml"]:
            sh.copy2(src=path, dst=single_day_path.joinpath(path.name))

    # Copies the .yaml snapshot of the session's single-day configuration parameters to the single_day folder. This
    # ensures that the .yaml representation of the processing parameters is preserved with the data generated by
    # running the pipeline with these parameters.
    sh.copy2(
        src=suite2p_path.joinpath("single_day_s2p_configuration.yaml"),
        dst=single_day_path.joinpath("single_day_s2p_configuration.yaml"),
    )

    # Also saves the updated single-day config to the multi_day session output folder, as it is reused during
    # the multi-day pipeline
    single_day_config = SingleDayS2PConfiguration.from_ops(ops_dict=original_ops)
    single_day_config.to_config(output_directory=multi_day_path)

    return i, combined_path, original_ops


def resolve_multiday_ops(ops: dict[str, Any], db: dict[str, Any]) -> Path:
    """Generates the output directory hierarchy and the multiday 'ops.npy' files for the multi-day suite2p pipeline,
    using the configuration parameters from ops and db.

    This function should be used before the first step (multi-day registration) of each multi-day suite2p pipeline to
    generate the necessary directories and files used by all pipeline steps. During step-wise pipeline execution, this
    function can also be used between steps to update the 'ops.npy' file with new runtime parameters.

    Notes:
        All multi-day pipeline functions require the path generated by this function as the 'ops_path' argument.

        Since the multi-day pipeline uses the outputs of the single-day pipeline, this function merges the multi-day
        configuration parameters into the single-day configuration file of each processed session. Both 'ops' and 'db'
        can contain single-day and multi-day parameters to override them as necessary to support runtime.

        If both 'ops' and 'db' do not contain some of the expected multiday parameters, they will be automatically
        filled using the 'default' dictionary generated using MultiDayS2PConfiguration. All single-day parameters will
        be loaded from the existing single-day ops.npy file. Therefore, both 'ops' and 'db' only need to override the
        parameters that need to be different from default values.

        As part of its runtime, this function also creates a shared 'multi_day_s2p_configuration.yaml' file that stores
        the same multi-day parameters as the created ops.npy file. This file has a noticeable advantage of not relying
        on pickling to be loaded.

    Args:
        ops: A dictionary that contains suite2p multi-day runtime parameters. See MultiDayS2PConfiguration class for
            available configuration parameters, as it is used to generate the 'ops' dictionary during most runtimes.
            Also, this dictionary can contain keys from the SingleDayS2PConfiguration class if you want to override
            parameters inherited from each session's single-day configuration file.
        db: An optional dictionary that contains the same keys as 'ops'. This is used to override a subset of parameters
            from 'ops', for example, to specialize a shared suite2p configuration for specific sessions or projects.

    Returns:
        The path to the multi-day ops.npy for the first session in the list of all processed sessions. The file
        specified by the path contains the information about all other sessions processed as part of the same multiday
        runtime, so it can be used to discover all other sessions.
    """

    # Since this step takes a noticeable amount of time, notifies the user about the progress of this step.
    console.echo(f"Resolving multi-day ops dictionary for {len(ops['session_folders'])} sessions...")

    # Builds up the multiday 'ops' dictionary. Specifically, first fills the dictionary with the 'default' keys. Then
    # overwrites all default keys with keys from the input 'ops' dictionary. Finally, overwrites any keys from the
    # input 'db' dictionary with values from 'db'. This way, there is the following order of precedence:
    # db > ops > default.
    ops = {
        **generate_default_multiday_ops(as_dict=True),  # type: ignore
        **ops,
        **db,
        # Actualizes version information
        "base_suite2p_version": version,
        "sl_suite2p_version": sl_version,
        "python_version": python_version,
    }

    # If the user does not specify the maximum parallel worker limit, sets it based on the number of available
    # CPU cores
    if ops["parallel_workers"] < 1:
        # noinspection PyUnresolvedReferences
        ops["parallel_workers"] = os.process_cpu_count()

    # Ensures that the resolved 'ops' file contains at least two sessions to process.
    if len(ops["session_folders"]) < 2:
        message = (
            f"Unable to run multi-day processing using the input parameters. Specifically, expected at least two "
            f"items inside the 'session_folders' list, but instead encountered {len(ops['session_folders'])}."
        )
        console.error(message=message, error=RuntimeError)

    # Builds a list of unique path components for each session. Assuming all sessions use unique names, this generates
    # a list of unique session IDs from their paths.
    sessions = natsorted([Path(session) for session in ops["session_folders"]])
    ops["session_ids"] = extract_unique_components(paths=sessions)

    # Resolves and generates the output directory for the multi-day runtime
    output_folder = Path(ops["multiday_save_path"]).joinpath(ops["multiday_save_folder"])
    ensure_directory_exists(output_folder)

    # Pre-creates all session output directories
    session_folders = ops["session_folders"]
    session_tasks = []

    # Loops over sessions and builds the list of tasks to run in parallel.
    for i, folder in enumerate(session_folders):
        # Generates the session-specific output directory under the main multi-day output folder, using the session ID.
        # Under this directory, creates the single_day and multi_day output folders.
        output_path = output_folder.joinpath(ops["session_ids"][i])
        single_day_path = output_path.joinpath("single_day")
        ensure_directory_exists(single_day_path)
        multi_day_path = output_path.joinpath("multi_day")
        ensure_directory_exists(multi_day_path)

        session_tasks.append((i, folder, ops["session_ids"][i], single_day_path, multi_day_path))

    # Processes all sessions in parallel
    session_results = {}
    last_combined_path = Path()
    with ThreadPoolExecutor(max_workers=ops["parallel_workers"]) as executor:
        # Submits processing tasks
        future_to_session = {
            executor.submit(_resolve_session_data, task_data): task_data[0] for task_data in session_tasks
        }

        # Processes completed tasks with a progress bar if progress bars are enabled
        with tqdm(
            total=len(session_tasks), desc="Processing sessions", disable=not ops["progress_bars"], unit="session"
        ) as pbar:
            for future in as_completed(future_to_session):
                i, combined_path, original_ops = future.result()
                session_results[i] = (combined_path, original_ops)
                last_combined_path = combined_path  # Keeps track of the last processed path
                pbar.update(1)

    # Resolves the path to the 'combined' folder ops.npy file for the last session processed by the loop above. This
    # file contains all single-day processing parameters, some of which are reused at the end of the multi-day pipeline
    # (during fluorescence extraction). Then, loads the combined ops into memory as a dictionary. Note, this approach
    # EXPECTS all sessions to share the same single-day parameters and many of the descriptive characteristics (such as
    # imaging fields) to work as expected.
    ops_combined: dict[str, Any] = np.load(last_combined_path.joinpath("ops.npy"), allow_pickle=True).item()

    # Merges the multi-day runtime parameters into the single-day ops dictionary. This generates a
    # combined config that holds both the single-day and the multi-day processing parameters. Also, since 'ops'
    # overrides single-day 'ops_combined', users can overwrite the parameters inside the single-day 'ops' dictionary
    # as necessary to support their runtime.
    multiday_ops = {
        **ops_combined,
        **ops,
        "allow_overlap": True,  # Required for multi-day signal extraction to work as expected
    }

    # Saves the combined single-day and multi-day file to the root output folder. During the rest of the
    # runtime, this file is loaded back into memory for various processing steps.
    ops_path = output_folder.joinpath("ops.npy")
    np.save(ops_path, multiday_ops, allow_pickle=True)  # type: ignore

    # Also generates a 'yaml' snapshot of the multi-day configuration parameters. This does not use the 'combined' ops
    # as it stores many fields that are not compatible with YAML (unfortunate baggage from the original
    # suite2p codebase sl-suite2p is built on top of).
    multi_day_config = MultiDayS2PConfiguration.from_ops(ops_dict=ops)
    multi_day_config.to_config(output_directory=output_folder)

    # Returns the path to the final multi-day config.
    return ops_path


def run_s2p_multiday(ops_path: Path) -> None:
    """Executes the entire multi-day suite2p processing pipeline using the parameters inside the provided ops.npy file.

    This function is intended to run the suite2p pipeline on a local machine. It sequentially calls all steps of the
    suite2p multi-day processing pipeline, extracting fluorescence from cells tracked across multiple sessions
    processed with the single-day suite2p pipeline.

    Notes:
        This function works as a high-level wrapper over the two main pipeline functions: 'discover_multiday_cells' and
        'extract_multiday_fluorescence'. If you want to run suite2p on a remote compute server in the most efficient
        way, implement a call hierarchy similar to the one showcased in the example notebook, which additionally
        parallelizes all individual processing steps across sessions.

    Args:
        ops_path: The path to the ops.npy file used to store the suite2p processing parameters. This file can be from
            any session processed as part of the multi-day runtime. Compatible ops.npy files are generated by
            the resolve_multiday_ops () function.
    """

    # Guards against invalid inputs.
    if not ops_path.exists() or not ops_path.is_file() or ops_path.suffix != ".npy":
        message = (
            f"Unable to run the multi-day suite2p pipeline, as the 'ops.npy' file does not exist at the specified "
            f"path {ops_path}. Use 'resolve_multiday_ops' function to generate the 'ops.npy' file(s) before running "
            f"multi-day pipeline."
        )
        console.error(message=message, error=FileNotFoundError)

    # Instantiates and resets the execution timer
    timer = PrecisionTimer("s")
    timer.reset()

    console.echo(message=f"Initializing multi-day suite2p runtime...", level=LogLevel.INFO)

    # Loads the configuration data into memory
    ops = np.load(ops_path, allow_pickle=True).item()

    # Step 1: Tracks cells active across processed sessions and generates the ROI masks for the tracked cells in the
    # visual space of each processed session.
    discover_multiday_cells(ops_path)

    # Step 2: Extracts the across-day-tracked cell fluorescence for each processed session.
    for session in ops["session_ids"]:
        extract_multiday_fluorescence(ops_path, session)

    message = f"Multi-day suite2p runtime: Complete. Total time: {timer.elapsed} seconds."
    console.echo(message=message, level=LogLevel.SUCCESS)


def discover_multiday_cells(ops_path: Path) -> None:
    """Registers all processed sessions to the same visual space, discovers cells active across sessions, and extracts
    their ROI information from each processed session.

    This function executes the first step of the multi-day suite2p pipeline. As part of its runtime, it first registers
    all sessions to the same (deformed) visual space. Then, it clusters the cells from different sessions in this
    space and generates a set of 'template' ROI masks, one for each cluster, to track all active cells across sessions.
    Finally, it translates the 'template' masks back to the original visual space of every processed session and caches
    the data on disk to prepare for the second step of the pipeline (trace extraction).

    Notes:
        As part of its runtime, this function will fill the multi-day output folder of each session with the results of
        across-day cell tracking, including the masks used by the second multi-day pipeline step.

        This function will parallelize certain operations using the number of workers specified by the
        'parallel_workers' key of the input 'ops' dictionary.

    Args:
        ops_path: The path to the multi-day ops.npy file that stores multi-day suite2p processing parameters. This file
            is typically stored inside the root multi-day output directory, together with the
            'multi-day-s2p-configuration.yaml' file. Compatible ops.npy files are generated by the
            resolve_multiday_ops() function.
    """

    # Verifies the input ops.npy file.
    if not ops_path.exists() or not ops_path.is_file() or ops_path.suffix != ".npy":
        message = (
            f"Unable to run the multi-day suite2p pipeline, as the 'ops.npy' file does not exist at the specified "
            f"path {ops_path}. Use 'resolve_multiday_ops' function to generate the 'ops.npy' file(s) before running "
            f"multi-day pipeline."
        )
        console.error(message=message, error=FileNotFoundError)
    ops = np.load(ops_path, allow_pickle=True).item()

    # Initializes the runtime timer
    timer = PrecisionTimer("s")
    step_timer = PrecisionTimer("s")  # Times the whole registration processing step.
    step_timer.reset()

    # Imports the data required to carry out registration for each session.
    console.echo(message=f"Importing requested session data for multi-day registration...")
    timer.reset()
    sessions_data = import_sessions(ops=ops)
    console.echo(message=f"Session data: imported. Time taken: {timer.elapsed} seconds.", level=LogLevel.SUCCESS)

    # Registers all sessions across days (to the same deformed visual field) by generating and applying the deformation
    # offsets to all reference images and cell masks of every session.
    console.echo(message=f"Registering sessions across days...")
    timer.reset()
    sessions_data = register_sessions(ops=ops, data=sessions_data)
    console.echo(message=f"Registration: complete. Time taken: {timer.elapsed} seconds.", level=LogLevel.SUCCESS)

    # Tracks cells across sessions in the deformed visual space and computes the template masks for cells that can be
    # identified across sessions to apply to the original visual space of each session
    console.echo(message=f"Computing across-session cell masks...")
    timer.reset()
    sessions_data = generate_template_masks(ops=ops, data=sessions_data)
    console.echo(
        message=f"Across-session cell masks: computed. Time taken: {timer.elapsed} seconds.", level=LogLevel.SUCCESS
    )

    # Transforms the template cell masks from the shared multi-session (deformed) visual space to the original
    # (unregistered) visual space of each session. This is necessary to re-extract the fluorescence of multi-day-tracked
    # cells from each session during the second step of the multi-day pipeline.
    console.echo(message=f"Transforming template cell masks from multi-day visual space to single-day visual space...")
    timer.reset()
    sessions_data = backward_transform_masks(ops=ops, data=sessions_data)
    console.echo(message=f"Cell masks: transformed. Time taken: {timer.elapsed} seconds.", level=LogLevel.SUCCESS)

    # Exports all data generate during the first (registration) step to disk. The data is then reloaded as part of the
    # second (extraction) step.
    console.echo(message=f"Appending multi-day registration data to each session's suite2p (output) folder...")
    export_masks_and_images(ops=ops, data=sessions_data)
    console.echo(
        message=f"Multi-day registration: complete. Time taken: {step_timer.elapsed} seconds.", level=LogLevel.SUCCESS
    )


def extract_multiday_fluorescence(ops_path: Path, session_id: str) -> None:
    """Extracts the fluorescence from all cells tracked across sessions.

    This function executes the second step of the multi-day suite2p pipeline. It uses the 'template' cell masks created
    during the first step (discover_multiday_cells) to extract the fluorescence of all cells tracked across-sessions
    from each processed session. This includes sessions where the cell is active and sessions where the cell is not
    active.

    Notes:
        This function generates three trace files, F.npy, Fneu.npy, and spks.npy inside the multi-day output folder of
        each processed session.

    Args:
        ops_path: The path to the multi-day ops.npy file that stores multi-day suite2p processing parameters. This file
            is typically stored inside the root multi-day output directory, together with the
            'multi-day-s2p-configuration.yaml' file. Compatible ops.npy files are generated by the
            resolve_multiday_ops() function.
        session_id: The unique ID of the session for which to extract the across-day-tracked cell fluorescence. The ID
            has to match one of the unique session IDs stored under the 'session_ids' key of the dictionary loaded from
            the ops.npy file.
    """

    # Verifies the input ops.npy file.
    if not ops_path.exists() or not ops_path.is_file() or ops_path.suffix != ".npy":
        message = (
            f"Unable to run the multi-day suite2p pipeline, as the 'ops.npy' file does not exist at the specified "
            f"path {ops_path}. Use 'resolve_multiday_ops' function to generate the 'ops.npy' file(s) before running "
            f"multi-day pipeline."
        )
        console.error(message=message, error=FileNotFoundError)

    ops = np.load(ops_path, allow_pickle=True).item()

    # Reconstructs the path to the output folder from 'ops' parameters
    output_folder = Path(ops["multiday_save_path"]).joinpath(ops["multiday_save_folder"])

    # The output folder contains .npy and .yaml files and directories named after each processed session ID.
    # This re-generates the list of session IDs from the directories stored in the output folder.
    session_ids = [folder.stem for folder in output_folder.glob("*") if folder.is_dir()]

    # Sorts session IDs for consistency
    session_ids = natsorted(session_ids)

    if session_id not in session_ids:
        message = (
            f"Unable to extract across-day-tracked cell activity data for session '{session_id}' as this ID is not "
            f"inside the session_ids list loaded from the ops.npy file specified by the {ops_path} path."
        )
        console.error(message=message, error=ValueError)

    # Loops over the output session directories and reconstructs the path to the single-day output folder of each
    # session using the data inside the single-day s2p config from the multi-day folder.
    sessions = []
    for session in session_ids:
        configuration: SingleDayS2PConfiguration = SingleDayS2PConfiguration.from_yaml(  # type: ignore
            output_folder.joinpath(session, "multi_day", "single_day_s2p_configuration.yaml")
        )
        session_folder = Path(configuration.file_io.save_path0).joinpath(configuration.file_io.save_folder)
        sessions.append(session_folder)

    # Uses the input ID to resolve the target session folder
    session_index = session_ids.index(session_id)
    target_session_folder = sessions[session_index]

    # Calls the trace extraction function
    extract_session_traces(ops=ops, session_folder=target_session_folder, session_id=session_id)
