#!/usr/bin/env python3
"""
Agent Evolve Dashboard - Streamlit web interface for visualizing evolution results
"""
import streamlit as st
import json
import os
import pandas as pd
from pathlib import Path
from datetime import datetime
import difflib
import plotly.express as px
import plotly.graph_objects as go
from typing import Dict, List, Tuple, Optional

def load_tool_data(base_dir: str = ".agent_evolve") -> Dict:
    """Load data for all evolved tools."""
    base_path = Path(base_dir)
    tools_data = {}
    
    if not base_path.exists():
        return tools_data
    
    # Skip non-tool directories
    skip_dirs = {'db', 'data', '__pycache__', '.git', 'logs', 'output', 'checkpoints', 'temp', 'tmp'}
    
    for tool_dir in base_path.iterdir():
        if not tool_dir.is_dir() or tool_dir.name in skip_dirs:
            continue
        
        tool_name = tool_dir.name
        tool_data = {
            'name': tool_name,
            'path': tool_dir,
            'has_evolution': False,
            'original_code': None,
            'best_code': None,
            'score_comparison': None,
            'checkpoints': [],
            'best_info': None
        }
        
        # Load basic tool files (training data, evaluator) regardless of evolution status
        # Load training data
        training_data_file = tool_dir / "training_data.json"
        if training_data_file.exists():
            with open(training_data_file, 'r') as f:
                tool_data['training_data'] = json.load(f)
        else:
            tool_data['training_data'] = None
        
        # Load evaluator code
        evaluator_file = tool_dir / "evaluator.py"
        if evaluator_file.exists():
            try:
                with open(evaluator_file, 'r', encoding='utf-8') as f:
                    tool_data['evaluator_code'] = f.read()
                print(f"  ‚úÖ Loaded evaluator code for {tool_name}: {len(tool_data['evaluator_code'])} chars")
            except Exception as e:
                print(f"  ‚ùå Error loading evaluator for {tool_name}: {e}")
                tool_data['evaluator_code'] = None
        else:
            print(f"  ‚ö†Ô∏è  No evaluator.py found for {tool_name}")
            tool_data['evaluator_code'] = None
        
        # Load original code if available
        evolve_target = tool_dir / "evolve_target.py"
        if evolve_target.exists():
            with open(evolve_target, 'r') as f:
                tool_data['original_code'] = f.read()
        
        # Check if tool has evolution results
        openevolve_output = tool_dir / "openevolve_output"
        if openevolve_output.exists():
            tool_data['has_evolution'] = True
            
            # Load best code from highest checkpoint's best_program.py
            checkpoints_dir = openevolve_output / "checkpoints"
            if checkpoints_dir.exists():
                # Find the highest numbered checkpoint
                checkpoint_dirs = [d for d in checkpoints_dir.iterdir() if d.is_dir() and d.name.startswith('checkpoint_')]
                if checkpoint_dirs:
                    # Sort by checkpoint number (not alphabetically)
                    def get_checkpoint_number(checkpoint_dir):
                        try:
                            return int(checkpoint_dir.name.split('_')[1])
                        except (IndexError, ValueError):
                            return 0
                    
                    checkpoint_dirs = sorted(checkpoint_dirs, key=get_checkpoint_number)
                    # Get the highest checkpoint (last in sorted list)
                    highest_checkpoint = checkpoint_dirs[-1]
                    best_program = highest_checkpoint / "best_program.py"
                    
                    if best_program.exists():
                        with open(best_program, 'r') as f:
                            content = f.read()
                            # Skip only the evolution metadata header at the very beginning
                            lines = content.split('\n')
                            code_start = 0
                            
                            # Look for the first docstring that contains evolution metadata
                            if lines[0].strip().startswith('"""'):
                                # Find the end of the first docstring
                                for j in range(1, len(lines)):
                                    if lines[j].strip().endswith('"""'):
                                        # Check if this docstring contains evolution metadata
                                        docstring_content = '\n'.join(lines[0:j+1])
                                        if any(keyword in docstring_content for keyword in [
                                            'Best Evolved Version', 'Generated by OpenEvolve', 
                                            'Evolution Metrics:', 'Generation:', 'Iteration:'
                                        ]):
                                            code_start = j + 1
                                        break
                            
                            # Skip empty lines after header
                            while code_start < len(lines) and not lines[code_start].strip():
                                code_start += 1
                            tool_data['best_code'] = '\n'.join(lines[code_start:])
                    else:
                        # Fallback to best_version.py if best_program.py doesn't exist
                        best_version = tool_dir / "best_version.py"
                        if best_version.exists():
                            with open(best_version, 'r') as f:
                                content = f.read()
                                lines = content.split('\n')
                                code_start = 0
                                
                                if lines[0].strip().startswith('"""'):
                                    for j in range(1, len(lines)):
                                        if lines[j].strip().endswith('"""'):
                                            docstring_content = '\n'.join(lines[0:j+1])
                                            if any(keyword in docstring_content for keyword in [
                                                'Best Evolved Version', 'Generated by OpenEvolve', 
                                                'Evolution Metrics:', 'Generation:', 'Iteration:'
                                            ]):
                                                code_start = j + 1
                                            break
                                
                                while code_start < len(lines) and not lines[code_start].strip():
                                    code_start += 1
                                tool_data['best_code'] = '\n'.join(lines[code_start:])
            
            # Calculate score comparison from checkpoint data
            if tool_data['checkpoints']:
                best_scores = None
                best_checkpoint_num = 0
                
                # Find the best checkpoint scores (highest checkpoint number)
                for checkpoint_data in tool_data['checkpoints']:
                    if checkpoint_data['checkpoint'] > best_checkpoint_num:
                        best_checkpoint_num = checkpoint_data['checkpoint']
                        checkpoint_info = checkpoint_data['info']
                        if 'metrics' in checkpoint_info:
                            best_scores = checkpoint_info['metrics']
                
                # Try to get original scores from initial_score.json first
                original_scores = None
                initial_score_file = tool_dir / "initial_score.json"
                print(f"  üîç Looking for initial scores at: {initial_score_file}")
                print(f"  üîç File exists: {initial_score_file.exists()}")
                if initial_score_file.exists():
                    try:
                        with open(initial_score_file, 'r') as f:
                            original_scores = json.load(f)
                            print(f"  ‚úÖ Loaded initial scores for {tool_name}: {original_scores}")
                            # Remove combined_score if it exists (it's calculated, not a real metric)
                            if isinstance(original_scores, dict) and 'combined_score' in original_scores:
                                original_scores = {k: v for k, v in original_scores.items() if k != 'combined_score'}
                                print(f"  ‚úÖ Filtered initial scores for {tool_name}: {original_scores}")
                    except Exception as e:
                        print(f"  ‚ö†Ô∏è Could not load initial scores for {tool_name}: {e}")
                else:
                    print(f"  ‚ö†Ô∏è No initial_score.json found for {tool_name} at {initial_score_file}")
                
                # Fallback: try to get original scores by evaluating the original code
                if not original_scores and tool_data.get('evaluator_code') and tool_data.get('original_code'):
                    try:
                        # Create a temporary file with the original code
                        import tempfile
                        import os
                        import sys
                        
                        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as temp_file:
                            temp_file.write(tool_data['original_code'])
                            temp_file_path = temp_file.name
                        
                        try:
                            # Try to run the evaluator on the original code
                            # Create a safe namespace for evaluation
                            eval_namespace = {}
                            exec(tool_data['evaluator_code'], eval_namespace)
                            
                            if 'evaluate' in eval_namespace:
                                original_scores = eval_namespace['evaluate'](temp_file_path)
                                # Remove combined_score if it exists (it's calculated, not a real metric)
                                if isinstance(original_scores, dict) and 'combined_score' in original_scores:
                                    original_scores = {k: v for k, v in original_scores.items() if k != 'combined_score'}
                        except Exception as e:
                            print(f"  ‚ö†Ô∏è Could not evaluate original code for {tool_name}: {e}")
                        finally:
                            # Clean up temp file
                            try:
                                os.unlink(temp_file_path)
                            except:
                                pass
                    except Exception as e:
                        print(f"  ‚ö†Ô∏è Error setting up original evaluation for {tool_name}: {e}")
                
                # Create score comparison structure
                if best_scores:
                    # Remove combined_score from best_scores if it exists
                    if 'combined_score' in best_scores:
                        best_scores = {k: v for k, v in best_scores.items() if k != 'combined_score'}
                    
                    # Calculate averages
                    original_avg = sum(original_scores.values()) / len(original_scores) if original_scores else 0
                    best_avg = sum(best_scores.values()) / len(best_scores) if best_scores else 0
                    
                    tool_data['score_comparison'] = {
                        'original_version': {
                            'scores': original_scores or {},
                            'average': original_avg
                        },
                        'best_version': {
                            'scores': best_scores,
                            'average': best_avg
                        }
                    }
            
            # Load best program info from highest checkpoint
            if tool_data['checkpoints']:
                # Find the highest checkpoint
                highest_checkpoint_data = max(tool_data['checkpoints'], key=lambda x: x['checkpoint'])
                tool_data['best_info'] = highest_checkpoint_data['info']
            else:
                # Fallback to "best" directory if no checkpoints
                best_info_file = openevolve_output / "best" / "best_program_info.json"
                if best_info_file.exists():
                    with open(best_info_file, 'r') as f:
                        tool_data['best_info'] = json.load(f)
            
            # Load checkpoint data
            checkpoints_dir = openevolve_output / "checkpoints"
            if checkpoints_dir.exists():
                checkpoint_dirs = [d for d in checkpoints_dir.iterdir() if d.is_dir() and d.name.startswith('checkpoint_')]
                # Sort by checkpoint number
                def get_checkpoint_number(checkpoint_dir):
                    try:
                        return int(checkpoint_dir.name.split('_')[1])
                    except (IndexError, ValueError):
                        return 0
                
                for checkpoint_dir in sorted(checkpoint_dirs, key=get_checkpoint_number):
                    checkpoint_num = int(checkpoint_dir.name.split('_')[1])
                    
                    # Load best program info for this checkpoint
                    checkpoint_info_file = checkpoint_dir / "best_program_info.json"
                    if checkpoint_info_file.exists():
                        with open(checkpoint_info_file, 'r') as f:
                            checkpoint_info = json.load(f)
                            tool_data['checkpoints'].append({
                                    'checkpoint': checkpoint_num,
                                    'info': checkpoint_info
                                })
        
        tools_data[tool_name] = tool_data
    
    return tools_data

def create_code_diff_html(original: str, evolved: str) -> str:
    """Create an HTML diff with syntax highlighting for additions/deletions."""
    if not original or not evolved:
        return "<p>Code not available for comparison</p>"
    
    # Create HTML diff
    diff = difflib.unified_diff(
        original.splitlines(keepends=True),
        evolved.splitlines(keepends=True),
        fromfile="Original Version", 
        tofile="Best Evolved Version",
        lineterm=""
    )
    
    html_lines = []
    html_lines.append('<div style="font-family: monospace; font-size: 12px; line-height: 1.4;">')
    
    for line in diff:
        if line.startswith('+++') or line.startswith('---'):
            html_lines.append(f'<div style="color: #666; font-weight: bold;">{line.rstrip()}</div>')
        elif line.startswith('@@'):
            html_lines.append(f'<div style="color: #0969da; background-color: #f6f8fa; padding: 2px 4px; font-weight: bold;">{line.rstrip()}</div>')
        elif line.startswith('+'):
            html_lines.append(f'<div style="color: #1a7f37; background-color: #dafbe1; padding: 1px 4px;">{line.rstrip()}</div>')
        elif line.startswith('-'):
            html_lines.append(f'<div style="color: #cf222e; background-color: #ffebe9; padding: 1px 4px;">{line.rstrip()}</div>')
        else:
            html_lines.append(f'<div style="padding: 1px 4px;">{line.rstrip()}</div>')
    
    html_lines.append('</div>')
    return ''.join(html_lines)

def create_code_diff_html_for_display(original: str, evolved: str, show_original: bool = True) -> str:
    """Create HTML diff with red/green highlighting for side-by-side display"""
    if not original or not evolved:
        return "<p>Code not available</p>"
    
    original_lines = original.splitlines()
    evolved_lines = evolved.splitlines()
    
    # Use SequenceMatcher to find differences
    matcher = difflib.SequenceMatcher(None, original_lines, evolved_lines)
    
    html_lines = []
    html_lines.append('<div style="font-family: \'Courier New\', monospace; font-size: 12px; line-height: 1.4; background-color: #f8f9fa; padding: 10px; border-radius: 4px; max-height: 600px; overflow-y: auto;">')
    
    if show_original:
        # Show original code with deletions highlighted in red
        line_num = 1
        for tag, i1, i2, j1, j2 in matcher.get_opcodes():
            if tag == 'equal':
                for i in range(i1, i2):
                    html_lines.append(f'<div style="padding: 1px 4px;"><span style="color: #666; margin-right: 10px; user-select: none;">{line_num:3d}</span><span style="color: #24292e;">{original_lines[i]}</span></div>')
                    line_num += 1
            elif tag == 'delete':
                for i in range(i1, i2):
                    html_lines.append(f'<div style="background-color: #ffeef0; border-left: 3px solid #d73a49; padding: 1px 4px;"><span style="color: #666; margin-right: 10px; user-select: none;">{line_num:3d}</span><span style="color: #d73a49;"><del>{original_lines[i]}</del></span></div>')
                    line_num += 1
            elif tag == 'replace':
                for i in range(i1, i2):
                    html_lines.append(f'<div style="background-color: #ffeef0; border-left: 3px solid #d73a49; padding: 1px 4px;"><span style="color: #666; margin-right: 10px; user-select: none;">{line_num:3d}</span><span style="color: #d73a49;"><del>{original_lines[i]}</del></span></div>')
                    line_num += 1
            # Skip 'insert' for original view
    else:
        # Show evolved code with additions highlighted in green
        line_num = 1
        for tag, i1, i2, j1, j2 in matcher.get_opcodes():
            if tag == 'equal':
                for j in range(j1, j2):
                    html_lines.append(f'<div style="padding: 1px 4px;"><span style="color: #666; margin-right: 10px; user-select: none;">{line_num:3d}</span><span style="color: #24292e;">{evolved_lines[j]}</span></div>')
                    line_num += 1
            elif tag == 'insert':
                for j in range(j1, j2):
                    html_lines.append(f'<div style="background-color: #e6ffed; border-left: 3px solid #28a745; padding: 1px 4px;"><span style="color: #666; margin-right: 10px; user-select: none;">{line_num:3d}</span><span style="color: #28a745;"><strong>{evolved_lines[j]}</strong></span></div>')
                    line_num += 1
            elif tag == 'replace':
                for j in range(j1, j2):
                    html_lines.append(f'<div style="background-color: #e6ffed; border-left: 3px solid #28a745; padding: 1px 4px;"><span style="color: #666; margin-right: 10px; user-select: none;">{line_num:3d}</span><span style="color: #28a745;"><strong>{evolved_lines[j]}</strong></span></div>')
                    line_num += 1
            # Skip 'delete' for evolved view
    
    html_lines.append('</div>')
    return ''.join(html_lines)

def create_side_by_side_diff_html(original: str, evolved: str) -> str:
    """Create a side-by-side HTML diff with line-by-line comparison."""
    if not original or not evolved:
        return "<p>Code not available for comparison</p>"
    
    original_lines = original.splitlines()
    evolved_lines = evolved.splitlines()
    
    # Use SequenceMatcher to find differences
    matcher = difflib.SequenceMatcher(None, original_lines, evolved_lines)
    
    html_parts = []
    html_parts.append('''
    <div style="display: flex; font-family: monospace; font-size: 12px; line-height: 1.4;">
        <div style="flex: 1; border-right: 1px solid #ddd; padding-right: 10px;">
            <div style="background-color: #f6f8fa; padding: 5px; font-weight: bold; border-bottom: 1px solid #ddd;">Original Version</div>
            <div style="padding: 5px;">
    ''')
    
    # Left side (original)
    original_html = []
    evolved_html = []
    
    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        if tag == 'equal':
            for i in range(i1, i2):
                line_num = i + 1
                original_html.append(f'<div style="padding: 1px 4px;"><span style="color: #666; margin-right: 10px;">{line_num:3d}</span>{original_lines[i]}</div>')
            for j in range(j1, j2):
                line_num = j + 1  
                evolved_html.append(f'<div style="padding: 1px 4px;"><span style="color: #666; margin-right: 10px;">{line_num:3d}</span>{evolved_lines[j]}</div>')
        elif tag == 'delete':
            for i in range(i1, i2):
                line_num = i + 1
                original_html.append(f'<div style="color: #cf222e; background-color: #ffebe9; padding: 1px 4px;"><span style="color: #666; margin-right: 10px;">{line_num:3d}</span>{original_lines[i]}</div>')
        elif tag == 'insert':
            for j in range(j1, j2):
                line_num = j + 1
                evolved_html.append(f'<div style="color: #1a7f37; background-color: #dafbe1; padding: 1px 4px;"><span style="color: #666; margin-right: 10px;">{line_num:3d}</span>{evolved_lines[j]}</div>')
        elif tag == 'replace':
            for i in range(i1, i2):
                line_num = i + 1
                original_html.append(f'<div style="color: #cf222e; background-color: #ffebe9; padding: 1px 4px;"><span style="color: #666; margin-right: 10px;">{line_num:3d}</span>{original_lines[i]}</div>')
            for j in range(j1, j2):
                line_num = j + 1
                evolved_html.append(f'<div style="color: #1a7f37; background-color: #dafbe1; padding: 1px 4px;"><span style="color: #666; margin-right: 10px;">{line_num:3d}</span>{evolved_lines[j]}</div>')
    
    html_parts.append(''.join(original_html))
    html_parts.append('''
            </div>
        </div>
        <div style="flex: 1; padding-left: 10px;">
            <div style="background-color: #f6f8fa; padding: 5px; font-weight: bold; border-bottom: 1px solid #ddd;">Best Evolved Version</div>
            <div style="padding: 5px;">
    ''')
    html_parts.append(''.join(evolved_html))
    html_parts.append('''
            </div>
        </div>
    </div>
    ''')
    
    return ''.join(html_parts)

def display_metrics_chart(score_data: Dict) -> None:
    """Display a radar chart of metrics comparison."""
    if not score_data:
        st.warning("No score data available")
        return
    
    original_scores = score_data.get('original_version', {}).get('scores', {})
    best_scores = score_data.get('best_version', {}).get('scores', {})
    
    if not original_scores or not best_scores:
        st.warning("Incomplete score data")
        return
    
    # Create radar chart
    metrics = list(original_scores.keys())
    original_values = [original_scores[m] for m in metrics]
    best_values = [best_scores[m] for m in metrics]
    
    fig = go.Figure()
    
    fig.add_trace(go.Scatterpolar(
        r=original_values,
        theta=metrics,
        fill='toself',
        name='Original Version',
        line_color='red',
        fillcolor='rgba(255, 0, 0, 0.1)'
    ))
    
    fig.add_trace(go.Scatterpolar(
        r=best_values,
        theta=metrics,
        fill='toself',
        name='Best Evolved Version',
        line_color='green',
        fillcolor='rgba(0, 255, 0, 0.1)'
    ))
    
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 1.0]
            )),
        showlegend=True,
        title="Performance Metrics Comparison"
    )
    
    st.plotly_chart(fig, use_container_width=True)

@st.dialog("ü§ñ Improve Training Data with AI")
def show_training_data_improvement_modal(selected_tool: str, current_training_data: List[Dict], evaluator_code: str):
    """Show AI training data improvement modal dialog"""
    
    # Check if we have valid training data
    if not current_training_data:
        st.error("‚ùå No training data found. Please generate training data first.")
        if st.button("Close"):
            st.rerun()
        return
    
    st.markdown("Describe how you want to improve or expand the training data:")
    
    # Show current training data preview
    with st.expander("Current Training Data Preview", expanded=False):
        st.json(current_training_data[:3])  # Show first 3 samples
        st.caption(f"Total samples: {len(current_training_data)}")
    
    improvement_instructions = st.text_area(
        "Improvement instructions:",
        placeholder="e.g., Add more diverse examples, generate 5 additional samples, focus on edge cases, improve data quality, etc.",
        height=120,
        key=f"modal_training_improvement_{selected_tool}"
    )
    
    # Option to specify number of samples
    col1, col2 = st.columns([2, 1])
    with col1:
        action_type = st.radio(
            "Action:",
            ["Improve existing data", "Generate additional samples", "Replace with better data"],
            key=f"training_action_{selected_tool}"
        )
    
    with col2:
        if action_type == "Generate additional samples":
            num_additional = st.number_input(
                "Additional samples:",
                min_value=1,
                max_value=20,
                value=5,
                key=f"num_additional_{selected_tool}"
            )
    
    col1, col2 = st.columns([3, 1])
    
    with col2:
        if st.button("Cancel", key=f"modal_training_cancel_{selected_tool}"):
            st.rerun()
    
    with col1:
        if st.button("üöÄ Generate Improved Training Data", type="primary", key=f"modal_training_submit_{selected_tool}"):
            if improvement_instructions:
                with st.spinner("ü§ñ Generating improved training data..."):
                    try:
                        import openai
                        client = openai.OpenAI()
                        
                        # Create training data improvement prompt
                        if action_type == "Generate additional samples":
                            action_instruction = f"Generate {num_additional} additional high-quality training samples"
                        elif action_type == "Replace with better data":
                            action_instruction = f"Replace the current training data with {len(current_training_data)} better quality samples"
                        else:
                            action_instruction = "Improve the existing training data samples"
                        
                        training_prompt = f"""You are an expert at creating high-quality training data for AI systems.

CURRENT TRAINING DATA:
```json
{json.dumps(current_training_data, indent=2)}
```

EVALUATOR CODE CONTEXT:
```python
{evaluator_code[:1000]}...
```

USER REQUEST:
{improvement_instructions}

TASK: {action_instruction}

REQUIREMENTS:
1. Maintain the same JSON structure as existing data
2. Ensure data is diverse, realistic, and high-quality
3. Focus on variety in scenarios, contexts, and use cases
4. Avoid repetitive patterns or artificial examples
5. Make sure data is relevant to the evaluator's purpose

For "Generate additional samples": Return ONLY the new samples as a JSON array
For "Improve existing" or "Replace": Return the complete improved dataset as a JSON array

Return ONLY valid JSON, no explanations or markdown."""
                        
                        response = client.chat.completions.create(
                            model="gpt-4o",
                            messages=[{"role": "user", "content": training_prompt}],
                            temperature=0.7  # Higher temperature for more diverse training data
                        )
                        
                        improved_data_text = response.choices[0].message.content.strip()
                        
                        # Clean up any markdown formatting
                        if improved_data_text.startswith('```json'):
                            improved_data_text = improved_data_text[7:]
                        elif improved_data_text.startswith('```'):
                            improved_data_text = improved_data_text[3:]
                        if improved_data_text.endswith('```'):
                            improved_data_text = improved_data_text[:-3]
                        improved_data_text = improved_data_text.strip()
                        
                        # Parse the improved data
                        improved_data = json.loads(improved_data_text)
                        
                        # Handle different action types
                        if action_type == "Generate additional samples":
                            final_data = current_training_data + improved_data
                        else:
                            final_data = improved_data
                        
                        # Debug info
                        st.info(f"Generated {len(improved_data)} samples. Total: {len(final_data)}")
                        
                        # Show a preview of the changes
                        with st.expander("Preview of Improved Training Data", expanded=True):
                            if action_type == "Generate additional samples":
                                st.markdown("**New samples added:**")
                                st.json(improved_data[:3] if len(improved_data) > 3 else improved_data)
                            else:
                                st.markdown("**Improved training data:**")
                                st.json(final_data[:3] if len(final_data) > 3 else final_data)
                        
                        # Store the improved data for display in main tab
                        st.session_state[f'improved_training_data_{selected_tool}'] = final_data
                        st.session_state[f'show_training_diff_{selected_tool}'] = True
                        st.success("‚úÖ Improved training data generated! Check the Training Data tab to see the changes and apply them.")
                        
                        # Close modal automatically
                        st.rerun()
                        
                    except json.JSONDecodeError as e:
                        st.error(f"‚ùå Error parsing generated JSON: {e}")
                    except Exception as e:
                        st.error(f"‚ùå Error generating improved training data: {e}")
            else:
                st.warning("Please provide improvement instructions")

@st.dialog("ü§ñ Improve Evaluator with AI")
def show_ai_improvement_modal(selected_tool: str, current_code: str, training_data: List[Dict]):
    """Show AI improvement modal dialog"""
    
    # Check if we have valid code
    if not current_code:
        st.error("‚ùå No evaluator code found. Please generate an evaluator first.")
        if st.button("Close"):
            st.rerun()
        return
    
    st.markdown("Describe how you want to improve the evaluator:")
    
    # Debug info
    with st.expander("Current Code Preview (first 500 chars)", expanded=False):
        preview_code = current_code[:500] + "..." if len(current_code) > 500 else current_code
        st.code(preview_code, language="python")
    
    improvement_instructions = st.text_area(
        "Improvement instructions:",
        placeholder="e.g., Make the evaluation more strict, focus on specific aspects, add new metrics, fix errors, etc.",
        height=120,
        key=f"modal_improvement_instructions_{selected_tool}"
    )
    
    col1, col2 = st.columns([3, 1])
    
    with col2:
        if st.button("Cancel", key=f"modal_cancel_{selected_tool}"):
            st.rerun()
    
    with col1:
        if st.button("üöÄ Generate Improved Evaluator", type="primary", key=f"modal_submit_{selected_tool}"):
            if improvement_instructions:
                with st.spinner("ü§ñ Generating improved evaluator..."):
                    try:
                        import openai
                        client = openai.OpenAI()
                        
                        # Create improvement prompt
                        improvement_prompt = f"""You are an expert Python developer improving an OpenEvolve evaluator.

CURRENT EVALUATOR CODE:
```python
{current_code}
```

TRAINING DATA SAMPLES (first 3):
```json
{json.dumps(training_data[:3] if training_data else [], indent=2)}
```

USER IMPROVEMENT REQUEST:
{improvement_instructions}

REQUIREMENTS:
1. The evaluator MUST have this exact function signature: def evaluate(program) -> dict:
2. Use raw OpenAI API (openai.OpenAI()), NOT langchain
3. Include robust JSON parsing with error handling
4. Return a dictionary with metric scores between 0.0 and 1.0
5. Apply the user's improvement instructions while maintaining functionality
6. Include proper error handling and logging

Generate the complete improved evaluator code.
Return ONLY Python code, no explanations or markdown."""
                        
                        response = client.chat.completions.create(
                            model="gpt-4o",
                            messages=[{"role": "user", "content": improvement_prompt}],
                            temperature=0.3
                        )
                        
                        improved_code = response.choices[0].message.content.strip()
                        
                        # Clean up any markdown formatting
                        if improved_code.startswith('```python'):
                            improved_code = improved_code[9:]
                        elif improved_code.startswith('```'):
                            improved_code = improved_code[3:]
                        if improved_code.endswith('```'):
                            improved_code = improved_code[:-3]
                        improved_code = improved_code.strip()
                        
                        # Debug info
                        st.info(f"Generated {len(improved_code)} characters of improved code")
                        
                        # Show a preview of the changes
                        with st.expander("Preview of Improved Code (first 500 chars)", expanded=True):
                            st.code(improved_code[:500] + "..." if len(improved_code) > 500 else improved_code, language="python")
                        
                        # Store the improved code for display in main tab
                        st.session_state[f'improved_evaluator_{selected_tool}'] = improved_code
                        st.session_state[f'show_diff_{selected_tool}'] = True
                        st.success("‚úÖ Improved evaluator generated! Check the Evaluator tab to see the diff and apply changes.")
                        
                        # Close modal automatically
                        st.rerun()
                        
                    except Exception as e:
                        st.error(f"‚ùå Error generating improved evaluator: {e}")
            else:
                st.warning("Please provide improvement instructions")

def display_evolution_timeline(checkpoints: List[Dict]) -> None:
    """Display evolution progress over checkpoints."""
    if not checkpoints:
        st.warning("No checkpoint data available")
        return
    
    # Extract metrics data for timeline
    timeline_data = []
    for cp in checkpoints:
        checkpoint_num = cp['checkpoint']
        metrics = cp['info'].get('metrics', {})
        
        for metric_name, value in metrics.items():
            timeline_data.append({
                'Checkpoint': checkpoint_num,
                'Metric': metric_name,
                'Score': value
            })
    
    if not timeline_data:
        st.warning("No metrics data in checkpoints")
        return
    
    df = pd.DataFrame(timeline_data)
    
    fig = px.line(df, x='Checkpoint', y='Score', color='Metric',
                  title='Evolution Progress Over Checkpoints',
                  markers=True)
    
    fig.update_layout(
        xaxis_title="Checkpoint",
        yaxis_title="Score",
        yaxis=dict(range=[0, 1.0])
    )
    
    st.plotly_chart(fig, use_container_width=True)

def main():
    st.set_page_config(
        page_title="Agent Evolve Dashboard",
        page_icon="üß¨",
        layout="wide"
    )
    
    st.title("üß¨ Agent Evolve Dashboard")
    st.markdown("Evolution tracking and analysis for your AI agents")
    
    # Sidebar for configuration
    st.sidebar.header("Configuration")
    
    # Get base directory from environment variable or use default
    import os
    default_base_dir = os.getenv('AGENT_EVOLVE_BASE_DIR', '.agent_evolve')
    base_dir = st.sidebar.text_input("Base Directory", value=default_base_dir)
    
    # Store base directory in session state for modal access
    st.session_state['dashboard_base_dir'] = base_dir
    
    # Load data
    with st.spinner("Loading evolution data..."):
        tools_data = load_tool_data(base_dir)
    
    if not tools_data:
        st.error(f"No tools found in {base_dir}")
        st.info("Make sure you have run evolution experiments first")
        return
    
    # Show all tools (both evolved and non-evolved)
    evolved_tools = tools_data
    
    # Sidebar tool selection
    col1, col2 = st.sidebar.columns([3, 1])
    with col1:
        st.header("Evolution Targets")
    with col2:
        if st.button("üîÑ", help="Sync - Re-extract all tools", key="sync_tools"):
            with st.spinner("üîÑ Syncing tools..."):
                try:
                    from agent_evolve.extract_decorated_tools import DecoratedToolExtractor
                    import os
                    
                    # Find the source directory (go up from base_dir to find src)
                    base_path = Path(base_dir)
                    search_path = base_path.parent.parent / "src"
                    
                    if not search_path.exists():
                        # Try alternative paths
                        search_path = base_path.parent / "src"
                        if not search_path.exists():
                            search_path = Path.cwd() / "src"
                    
                    if search_path.exists():
                        # Initialize extractor
                        extractor = DecoratedToolExtractor(output_dir=str(base_path))
                        
                        # Find all Python files
                        python_files = list(search_path.rglob("*.py"))
                        total_extracted = 0
                        
                        for py_file in python_files:
                            try:
                                # Extract @evolve decorated functions
                                decorated_tools = extractor.extract_from_file(str(py_file))
                                
                                # Save extracted decorated tools
                                for tool in decorated_tools:
                                    extractor.save_extracted_tool(tool)
                                
                                # Extract commented #@evolve() targets
                                from agent_evolve.extract_commented_evolve import extract_commented_evolve_from_file
                                commented_tools = extract_commented_evolve_from_file(str(py_file), str(base_path))
                                
                                total_extracted += len(decorated_tools) + len(commented_tools)
                                
                            except Exception as e:
                                st.warning(f"‚ö†Ô∏è Error processing {py_file.name}: {e}")
                        
                        st.success(f"‚úÖ Synced {total_extracted} evolution targets!")
                        st.rerun()
                    else:
                        st.error(f"‚ùå Source directory not found. Searched: {search_path}")
                except Exception as e:
                    st.error(f"‚ùå Sync failed: {e}")
    
    if not evolved_tools:
        st.sidebar.info("No tools found")
        selected_tool = None
    else:
        # Create tool selection in sidebar
        tool_options = list(evolved_tools.keys())
        selected_tool = st.sidebar.radio(
            "Tools",
            options=tool_options,
            label_visibility="hidden"
        )
        
        # Show tool status in sidebar
        if selected_tool:
            st.sidebar.markdown("---")
            st.sidebar.subheader("Tool Status")
            if evolved_tools[selected_tool]['score_comparison']:
                avg_improvement = (
                    evolved_tools[selected_tool]['score_comparison']['best_version']['average'] - 
                    evolved_tools[selected_tool]['score_comparison']['original_version']['average']
                )
                improvement_color = "green" if avg_improvement > 0 else "red" if avg_improvement < 0 else "gray"
                st.sidebar.markdown(f"**Improvement:** <span style='color:{improvement_color}'>{avg_improvement:+.3f}</span>", 
                                   unsafe_allow_html=True)
            else:
                st.sidebar.markdown("**Improvement:** N/A")
            
            if evolved_tools[selected_tool]['best_info']:
                generation = evolved_tools[selected_tool]['best_info'].get('generation', 'N/A')
                st.sidebar.markdown(f"**Best Generation:** {generation}")
    
    # Main content header
    if selected_tool:
        st.header(f"üìä {selected_tool}")
    else:
        st.header("üìä Evolution Results")
        if not evolved_tools:
            return
    
    if not selected_tool:
        st.info("üëà Select a tool from the sidebar to view its evolution results")
        return
    
    tool_data = evolved_tools[selected_tool]
    
    # Create tabs for different views
    overview_tab, training_tab, evolution_tab, evaluator_tab, evolved_code_tab, metrics_tab = st.tabs([
        "üìà Overview", "üìã Training Data", "üß¨ Evolution", "üîß Evaluator", "üîç Evolved Code", "üìä Metrics"
    ])
    
    with overview_tab:
        st.subheader("Evolution Pipeline")
        
        # Step 1: Training Data
        st.markdown("### 1Ô∏è‚É£ Training Data")
        if tool_data.get('training_data'):
            st.success(f"‚úÖ Training data available ({len(tool_data['training_data'])} samples)")
        else:
            st.warning("‚ùå No training data")
            if st.button("üöÄ Generate Training Data", key=f"gen_training_{selected_tool}"):
                with st.spinner("ü§ñ Generating training data..."):
                    try:
                        from agent_evolve.generate_training_data import TrainingDataGenerator
                        generator = TrainingDataGenerator(num_samples=10)
                        generator.generate_training_data(str(Path(tool_data['path']).parent), force=False, specific_tool=selected_tool)
                        st.success("‚úÖ Training data generated!")
                        st.rerun()
                    except Exception as e:
                        st.error(f"‚ùå Error: {e}")
        
        # Step 2: Evaluator
        st.markdown("### 2Ô∏è‚É£ Evaluator")
        if tool_data.get('evaluator_code'):
            st.success("‚úÖ Evaluator exists")
        else:
            if tool_data.get('training_data'):
                st.warning("‚ùå No evaluator")
                if st.button("üöÄ Generate Evaluator", key=f"gen_eval_{selected_tool}"):
                    with st.spinner("ü§ñ Generating evaluator..."):
                        try:
                            from agent_evolve.generate_evaluators import EvaluatorGenerator
                            if not os.getenv('OPENAI_API_KEY'):
                                st.error("‚ùå OPENAI_API_KEY required")
                            else:
                                generator = EvaluatorGenerator(model_name="gpt-4o")
                                tool_path = Path(tool_data['path'])
                                
                                # Force regeneration by removing existing evaluator
                                evaluator_file = tool_path / "evaluator.py"
                                if evaluator_file.exists():
                                    evaluator_file.unlink()
                                    st.info("üîÑ Removing existing evaluator to regenerate...")
                                
                                generator._generate_single_evaluator(tool_path)
                                
                                # Check if evaluator was actually created
                                if evaluator_file.exists():
                                    # Also regenerate config to use the correct metrics from the evaluator
                                    try:
                                        from agent_evolve.generate_openevolve_configs import OpenEvolveConfigGenerator
                                        config_generator = OpenEvolveConfigGenerator(str(tool_path.parent))
                                        config_generator._generate_single_config(tool_path, selected_tool)
                                        st.info("üîÑ Updated config with evaluator metrics")
                                    except Exception as config_e:
                                        st.warning(f"‚ö†Ô∏è Config update failed: {config_e}")
                                    
                                    st.success("‚úÖ Evaluator generated!")
                                    st.rerun()
                                else:
                                    st.error("‚ùå Evaluator generation failed - file not created")
                        except Exception as e:
                            st.error(f"‚ùå Error: {e}")
                            import traceback
                            st.code(traceback.format_exc())
            else:
                st.info("‚è∏Ô∏è Generate training data first")
        
        # Step 3: Config
        st.markdown("### 3Ô∏è‚É£ OpenEvolve Config")
        config_file = Path(tool_data['path']) / "openevolve_config.yaml"
        if config_file.exists():
            st.success("‚úÖ Config file exists")
        else:
            if tool_data.get('evaluator_code'):
                st.warning("‚ùå No config file")
                if st.button("üöÄ Generate Config", key=f"gen_config_{selected_tool}"):
                    with st.spinner("ü§ñ Generating config..."):
                        try:
                            from agent_evolve.generate_openevolve_configs import OpenEvolveConfigGenerator
                            generator = OpenEvolveConfigGenerator(str(Path(tool_data['path']).parent))
                            generator._generate_single_config(Path(tool_data['path']), selected_tool)
                            st.success("‚úÖ Config generated!")
                            st.rerun()
                        except Exception as e:
                            st.error(f"‚ùå Error: {e}")
            else:
                st.info("‚è∏Ô∏è Generate evaluator first")
        
        # Step 4: Evolution
        st.markdown("### 4Ô∏è‚É£ Run Evolution")
        if tool_data.get('has_evolution'):
            st.success("‚úÖ Evolution has been run")
            # Show metrics if available
            if tool_data['score_comparison']:
                col1, col2 = st.columns(2)
                with col1:
                    avg_improvement = (
                        tool_data['score_comparison']['best_version']['average'] - 
                        tool_data['score_comparison']['original_version']['average']
                    )
                    st.metric("Average Improvement", f"{avg_improvement:+.3f}")
                with col2:
                    if tool_data['best_info']:
                        generation = tool_data['best_info'].get('generation', 'N/A')
                        st.metric("Best Generation", generation)
        else:
            if config_file.exists():
                st.warning("‚ùå Evolution not run yet")
                st.info("üí° Go to the Evolution tab to run evolution")
            else:
                st.info("‚è∏Ô∏è Generate config first")
        
        # Score comparison table
        if tool_data.get('score_comparison'):
            st.markdown("---")
            st.subheader("Score Comparison")
            
            scores_df = pd.DataFrame([
                {'Version': 'Original', **tool_data['score_comparison']['original_version']['scores']},
                {'Version': 'Best Evolved', **tool_data['score_comparison']['best_version']['scores']}
            ])
            
            st.dataframe(scores_df, use_container_width=True)
    
    with training_tab:
        # Header with improve button
        col1, col2 = st.columns([3, 1])
        with col1:
            st.subheader("Training Data")
        with col2:
            if st.button("ü§ñ Improve with AI", key=f"improve_training_top_{selected_tool}"):
                # Get current training data and evaluator code
                current_training_data = tool_data.get('training_data', [])
                evaluator_code = tool_data.get('evaluator_code', '')
                # Show the modal
                show_training_data_improvement_modal(selected_tool, current_training_data, evaluator_code)
        
        # Check if we should show training data diff view
        if st.session_state.get(f'show_training_diff_{selected_tool}', False):
            st.subheader("üîÑ AI Improved Training Data - Review Changes")
            
            # Get the improved training data
            improved_training_data = st.session_state.get(f'improved_training_data_{selected_tool}', [])
            original_training_data = tool_data.get('training_data', [])
            
            if not improved_training_data:
                st.error("‚ùå No improved training data found in session state. Please try generating again.")
                del st.session_state[f'show_training_diff_{selected_tool}']
                st.rerun()
                return
            
            # Show side-by-side comparison
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**Original Training Data**")
                st.json(original_training_data)
                st.caption(f"Total samples: {len(original_training_data)}")
            
            with col2:
                st.markdown("**Improved Training Data**") 
                st.json(improved_training_data)
                st.caption(f"Total samples: {len(improved_training_data)}")
                
                # Apply button at bottom right
                st.markdown("")  # Add some space
                col_left, col_right = st.columns([3, 1])
                with col_right:
                    if st.button("‚úÖ Apply Changes", type="primary", key=f"apply_training_diff_{selected_tool}"):
                        # Save the improved training data directly to file
                        training_data_path = Path(tool_data['path']) / "training_data.json"
                        
                        print(f"[APPLY TRAINING DIFF] Saving to: {training_data_path}")
                        print(f"[APPLY TRAINING DIFF] Data samples: {len(improved_training_data)}")
                        
                        try:
                            with open(training_data_path, 'w') as f:
                                json.dump(improved_training_data, f, indent=2)
                            
                            print(f"[APPLY TRAINING DIFF] File saved successfully")
                            st.success(f"‚úÖ Training data changes applied and saved to {training_data_path}")
                            
                            # Update the in-memory data
                            tool_data['training_data'] = improved_training_data
                            
                            # Clear the diff flags to return to regular view
                            del st.session_state[f'show_training_diff_{selected_tool}']
                            del st.session_state[f'improved_training_data_{selected_tool}']
                            
                            # Refresh the page
                            st.rerun()
                            
                        except Exception as e:
                            print(f"[APPLY TRAINING DIFF] Error: {e}")
                            st.error(f"‚ùå Error saving: {e}")
                
                with col_left:
                    if st.button("‚ùå Discard Changes", key=f"discard_training_diff_{selected_tool}"):
                        # Clear the diff flags without saving
                        del st.session_state[f'show_training_diff_{selected_tool}']
                        del st.session_state[f'improved_training_data_{selected_tool}']
                        st.rerun()
            
            return  # Don't show the regular training data view when in diff mode
        
        if tool_data['training_data']:
            # Display training data as a table
            training_df = pd.DataFrame(tool_data['training_data'])
            
            # Flatten nested dictionaries for better display
            flattened_data = []
            for i, item in enumerate(tool_data['training_data']):
                flattened_item = {'Index': i + 1}
                
                def flatten_dict(d, prefix=''):
                    for key, value in d.items():
                        if isinstance(value, dict):
                            flatten_dict(value, f"{prefix}{key}.")
                        else:
                            flattened_item[f"{prefix}{key}"] = value
                
                flatten_dict(item)
                flattened_data.append(flattened_item)
            
            if flattened_data:
                flattened_df = pd.DataFrame(flattened_data)
                st.dataframe(flattened_df, use_container_width=True)
                
                # Show summary statistics
                st.subheader("Training Data Summary")
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("Total Samples", len(tool_data['training_data']))
                with col2:
                    st.metric("Columns", len(flattened_df.columns) - 1)  # -1 for Index column
            else:
                st.warning("Could not parse training data structure")
        else:
            st.warning("No training data available")
    
    with evolution_tab:
        st.subheader("üß¨ Run Evolution")
        
        # Evolution parameters
        col1, col2 = st.columns(2)
        
        with col1:
            # Get available checkpoints from the openevolve_output directory
            checkpoints_dir = Path(tool_data['path']) / "openevolve_output" / "checkpoints"
            available_checkpoints = []
            
            if checkpoints_dir.exists():
                # Find all checkpoint directories
                checkpoint_dirs = [d for d in checkpoints_dir.iterdir() if d.is_dir() and d.name.startswith('checkpoint_')]
                for checkpoint_dir in checkpoint_dirs:
                    try:
                        checkpoint_num = int(checkpoint_dir.name.split('_')[1])
                        available_checkpoints.append(checkpoint_num)
                    except (IndexError, ValueError):
                        continue
            
            # Remove duplicates and sort
            available_checkpoints = sorted(list(set(available_checkpoints)))
            
            # Create selectbox for checkpoints
            if available_checkpoints:
                checkpoint_options = ["None (start from beginning)"]
                for cp in available_checkpoints:
                    checkpoint_options.append(f"{cp}")
                
                selected_checkpoint_str = st.selectbox(
                    "Start from checkpoint:",
                    options=checkpoint_options,
                    index=0,
                    help="Choose a checkpoint to resume from, or None to start from beginning"
                )
                
                # Extract the numeric value (None means no checkpoint)
                if selected_checkpoint_str.startswith("None"):
                    start_checkpoint = None
                else:
                    start_checkpoint = int(selected_checkpoint_str)
            else:
                # No checkpoints available
                st.selectbox(
                    "Start from checkpoint:",
                    options=["None (no checkpoints available)"],
                    index=0,
                    disabled=True,
                    help="No checkpoints found - will start from beginning"
                )
                start_checkpoint = None
        
        with col2:
            num_iterations = st.number_input(
                "Number of iterations:",
                min_value=1,
                max_value=100,
                value=20,
                help="How many evolution iterations to run (modifies the OpenEvolve config)"
            )
        
        # Build command for user to copy
        cmd_parts = ["agent-evolve", "evolve", selected_tool]
        
        # Only add checkpoint argument if one is specified
        if start_checkpoint is not None:
            cmd_parts.extend(["--checkpoint", str(start_checkpoint)])
        
        # Add iterations parameter if different from default
        if num_iterations != 20:
            cmd_parts.extend(["--iterations", str(num_iterations)])
        
        command = " ".join(cmd_parts)
        
        # Show the command to run
        st.subheader("üíª Run Evolution Command")
        st.markdown("Copy and paste these commands into your terminal to run evolution:")
        
        full_command = f"cd {Path.cwd()}\n{command}"
        st.code(full_command, language="bash")
        
        
        st.markdown("---")
        
        # Show additional info
        col1, col2 = st.columns(2)
        
        with col1:
            st.info(f"**Tool**: {selected_tool}")
            if start_checkpoint is not None:
                st.info(f"**Starting from checkpoint**: {start_checkpoint}")
            else:
                st.info("**Starting from**: Beginning (no checkpoint)")
        
        with col2:
            st.info(f"**Iterations**: {num_iterations}")
            st.info(f"**Working directory**: {Path.cwd()}")
        
        # Show current evolution status if available
        openevolve_output = Path(tool_data['path']) / "openevolve_output"
        if openevolve_output.exists():
            st.subheader("üìä Current Evolution Status")
            
            # Show latest checkpoint info
            checkpoints_dir = openevolve_output / "checkpoints"
            if checkpoints_dir.exists():
                checkpoint_dirs = [d for d in checkpoints_dir.iterdir() if d.is_dir() and d.name.startswith('checkpoint_')]
                if checkpoint_dirs:
                    # Sort by checkpoint number
                    def get_checkpoint_number(checkpoint_dir):
                        try:
                            return int(checkpoint_dir.name.split('_')[1])
                        except (IndexError, ValueError):
                            return 0
                    
                    checkpoint_dirs = sorted(checkpoint_dirs, key=get_checkpoint_number)
                    latest_checkpoint = checkpoint_dirs[-1]
                    checkpoint_num = latest_checkpoint.name.split('_')[1]
                    
                    best_info_file = latest_checkpoint / "best_program_info.json"
                    if best_info_file.exists():
                        try:
                            with open(best_info_file, 'r') as f:
                                best_info = json.load(f)
                            
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("Latest Checkpoint", checkpoint_num)
                            with col2:
                                st.metric("Generation", best_info.get('generation', 'N/A'))
                            with col3:
                                metrics = best_info.get('metrics', {})
                                if metrics:
                                    avg_score = sum(metrics.values()) / len(metrics)
                                    st.metric("Avg Score", f"{avg_score:.3f}")
                        except Exception as e:
                            st.error(f"Error reading checkpoint info: {e}")
    
    with evaluator_tab:
        # Header with improve button
        col1, col2 = st.columns([3, 1])
        with col1:
            st.subheader("Evaluator Code")
        with col2:
            if st.button("ü§ñ Improve with AI", key=f"improve_ai_top_{selected_tool}"):
                # Store tool data in session state for modal access
                st.session_state['current_tool_data'] = tool_data
                
                # Get current evaluator code - prioritize what's in the editor (session state)
                current_code = None
                
                # First try to get from session state (what's currently in the editor)
                if f'evaluator_editor_{selected_tool}' in st.session_state:
                    current_code = st.session_state[f'evaluator_editor_{selected_tool}']
                
                # If not in session state, try to get from tool_data
                if not current_code:
                    current_code = tool_data.get('evaluator_code')
                
                # If still not found, try to reload from file
                if not current_code:
                    evaluator_file = Path(tool_data['path']) / "evaluator.py"
                    if evaluator_file.exists():
                        try:
                            with open(evaluator_file, 'r', encoding='utf-8') as f:
                                current_code = f.read()
                        except Exception as e:
                            print(f"[ERROR] Failed to read evaluator file: {e}")
                
                # Debug logging
                print(f"[DEBUG] Tool: {selected_tool}")
                print(f"[DEBUG] current_code source: {'session_state' if f'evaluator_editor_{selected_tool}' in st.session_state else 'tool_data' if tool_data.get('evaluator_code') else 'file' if current_code else 'none'}")
                print(f"[DEBUG] current_code length: {len(current_code) if current_code else 'None'}")
                
                # Load training data
                training_data = tool_data.get('training_data', [])
                # Show the modal
                show_ai_improvement_modal(selected_tool, current_code, training_data)
        
        # Check if there's a pending apply from the modal
        if st.session_state.get(f'pending_apply_{selected_tool}'):
            print(f"[MAIN] Processing pending apply for {selected_tool}")
            
            # Get the improved code
            improved_code = st.session_state[f'pending_apply_{selected_tool}']
            print(f"[MAIN] Applying {len(improved_code)} chars to editor")
            
            # Update the editor session state
            st.session_state[f'evaluator_editor_{selected_tool}'] = improved_code
            
            # Clear the pending flag
            del st.session_state[f'pending_apply_{selected_tool}']
            
            # Show success message
            st.success("‚úÖ Evaluator has been improved with AI! The code has been updated in the editor below. Click 'Save Evaluator' to save to file.")
            
            print(f"[MAIN] Applied successfully")
        
        # Check if AI improvement was just completed (legacy)
        if st.session_state.get(f'ai_improvement_success_{selected_tool}', False):
            st.success("‚úÖ Evaluator has been improved with AI! Review the updated code below.")
            # Clear the flag
            del st.session_state[f'ai_improvement_success_{selected_tool}']
        
        if tool_data['evaluator_code']:
            # Check if we should show diff view
            if st.session_state.get(f'show_diff_{selected_tool}', False):
                st.subheader("üîÑ AI Improved Code - Review Changes")
                
                # Get the improved code with debugging
                improved_code = st.session_state.get(f'improved_evaluator_{selected_tool}', '')
                original_code = tool_data['evaluator_code']
                
                # DEBUG: Show what we actually have
                print(f"[DIFF VIEW] improved_code type: {type(improved_code)}")
                print(f"[DIFF VIEW] improved_code length: {len(improved_code) if improved_code else 'None/Empty'}")
                print(f"[DIFF VIEW] Session state keys: {[k for k in st.session_state.keys() if selected_tool in k]}")
                
                if not improved_code:
                    st.error("‚ùå No improved code found in session state. Please try generating again.")
                    # Clear the flag and return to regular view
                    del st.session_state[f'show_diff_{selected_tool}']
                    st.rerun()
                    return
                
                # Show side-by-side diff with additions/deletions highlighted
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**Original Code**")
                    # Create diff HTML for original (deletions in red)
                    diff_html_original = create_code_diff_html_for_display(original_code, improved_code, show_original=True)
                    st.markdown(diff_html_original, unsafe_allow_html=True)
                
                with col2:
                    st.markdown("**Improved Code**")
                    # Create diff HTML for improved (additions in green)
                    diff_html_improved = create_code_diff_html_for_display(original_code, improved_code, show_original=False)
                    st.markdown(diff_html_improved, unsafe_allow_html=True)
                    
                    # Apply button at bottom right
                    st.markdown("")  # Add some space
                    col_left, col_right = st.columns([3, 1])
                    with col_right:
                        if st.button("‚úÖ Apply Changes", type="primary", key=f"apply_diff_{selected_tool}"):
                            # Save the improved code directly to file
                            evaluator_path = Path(tool_data['path']) / "evaluator.py"
                            
                            print(f"[APPLY DIFF] Saving to: {evaluator_path}")
                            print(f"[APPLY DIFF] Code length: {len(improved_code)}")
                            print(f"[APPLY DIFF] First 100 chars: {improved_code[:100]}")
                            
                            try:
                                with open(evaluator_path, 'w') as f:
                                    f.write(improved_code)
                                
                                print(f"[APPLY DIFF] File saved successfully")
                                st.success(f"‚úÖ Changes applied and saved to {evaluator_path}")
                                
                                # Update the in-memory data so the regular editor shows the new code
                                tool_data['evaluator_code'] = improved_code
                                
                                # Update the editor session state with the new code
                                st.session_state[f'evaluator_editor_{selected_tool}'] = improved_code
                                
                                # Clear the diff flags to return to regular editor
                                del st.session_state[f'show_diff_{selected_tool}']
                                del st.session_state[f'improved_evaluator_{selected_tool}']
                                
                                # Refresh the page
                                st.rerun()
                                
                            except Exception as e:
                                print(f"[APPLY DIFF] Error: {e}")
                                st.error(f"‚ùå Error saving: {e}")
                    
                    with col_left:
                        if st.button("‚ùå Discard Changes", key=f"discard_diff_{selected_tool}"):
                            # Clear the diff flags without saving
                            del st.session_state[f'show_diff_{selected_tool}']
                            del st.session_state[f'improved_evaluator_{selected_tool}']
                            st.rerun()
                
                return  # Don't show the regular editor when in diff mode
            
            # Regular code editor
            try:
                # Try to use streamlit-ace for better code editing experience
                from streamlit_ace import st_ace
                
                # Get the current value - check for improved code first
                if st.session_state.get(f'improved_evaluator_{selected_tool}'):
                    current_value = st.session_state[f'improved_evaluator_{selected_tool}']
                    # Clear the improved code after using it
                    del st.session_state[f'improved_evaluator_{selected_tool}']
                else:
                    current_value = st.session_state.get(f'evaluator_editor_{selected_tool}', tool_data['evaluator_code'])
                
                edited_code = st_ace(
                    value=current_value,
                    language='python',
                    theme='github',  # Changed to lighter theme
                    key=f"evaluator_editor_{selected_tool}",
                    font_size=14,
                    show_gutter=True,
                    show_print_margin=True,
                    wrap=False,
                    auto_update=True,  # Changed to True for immediate updates
                    annotations=None,
                    height=600  # Set explicit height for more space
                )
                
                # Note about the Apply button
                st.caption("üí° Use the 'Save Evaluator' button below to save changes to file (Apply button only updates the editor)")
            except ImportError:
                # Fallback to text_area if ace editor not installed
                st.info("üí° Install streamlit-ace for a better code editing experience: pip install streamlit-ace")
                
                # Get the current value - check for improved code first
                if st.session_state.get(f'improved_evaluator_{selected_tool}'):
                    current_value = st.session_state[f'improved_evaluator_{selected_tool}']
                    # Clear the improved code after using it
                    del st.session_state[f'improved_evaluator_{selected_tool}']
                else:
                    current_value = st.session_state.get(f'evaluator_editor_{selected_tool}', tool_data['evaluator_code'])
                
                edited_code = st.text_area(
                    "Edit evaluator code:",
                    value=current_value,
                    height=600,
                    key=f"evaluator_editor_{selected_tool}"
                )
            
            # Action buttons
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if st.button("üíæ Save Evaluator", type="primary"):
                    evaluator_path = Path(tool_data['path']) / "evaluator.py"
                    
                    # DEBUG THE EDITOR STATE
                    print(f"[SAVE] Tool: {selected_tool}")
                    print(f"[SAVE] edited_code type: {type(edited_code)}")
                    print(f"[SAVE] edited_code value: '{edited_code}'")
                    print(f"[SAVE] edited_code length: {len(edited_code) if edited_code else 'None'}")
                    print(f"[SAVE] Session state editor key exists: {f'evaluator_editor_{selected_tool}' in st.session_state}")
                    if f'evaluator_editor_{selected_tool}' in st.session_state:
                        session_value = st.session_state[f'evaluator_editor_{selected_tool}']
                        print(f"[SAVE] Session state value type: {type(session_value)}")
                        print(f"[SAVE] Session state value length: {len(session_value) if session_value else 'None'}")
                        print(f"[SAVE] Session state first 100 chars: {session_value[:100] if session_value else 'None'}")
                    
                    try:
                        with open(evaluator_path, 'w') as f:
                            f.write(edited_code)
                        print(f"[SAVE] SUCCESS: File written")
                        st.success(f"‚úÖ Evaluator saved to {evaluator_path}")
                        # Update the in-memory data
                        tool_data['evaluator_code'] = edited_code
                    except Exception as e:
                        print(f"[SAVE] ERROR: {e}")
                        st.error(f"‚ùå Error saving evaluator: {e}")
            
            with col2:
                if st.button("üîç Validate Evaluator"):
                    try:
                        # Try to compile the code
                        compile(edited_code, '<string>', 'exec')
                        
                        # Check for required function
                        if 'def evaluate(' in edited_code:
                            st.success("‚úÖ Evaluator syntax is valid and contains evaluate() function")
                        else:
                            st.error("‚ùå Missing required 'def evaluate(' function")
                            
                    except SyntaxError as e:
                        st.error(f"‚ùå Syntax error: {e}")
                    except Exception as e:
                        st.error(f"‚ùå Validation error: {e}")
            
            with col3:
                if st.button("ü§ñ Improve with AI"):
                    # Store tool data in session state for modal access
                    st.session_state['current_tool_data'] = tool_data
                    
                    # Get current evaluator code - prioritize what's in the editor (session state)
                    current_code = None
                    
                    # First try to get from session state (what's currently in the editor)
                    if f'evaluator_editor_{selected_tool}' in st.session_state:
                        current_code = st.session_state[f'evaluator_editor_{selected_tool}']
                    
                    # If not in session state, try to get from tool_data
                    if not current_code:
                        current_code = tool_data.get('evaluator_code')
                    
                    # If still not found, try to reload from file
                    if not current_code:
                        evaluator_file = Path(tool_data['path']) / "evaluator.py"
                        if evaluator_file.exists():
                            try:
                                with open(evaluator_file, 'r', encoding='utf-8') as f:
                                    current_code = f.read()
                            except Exception as e:
                                print(f"[ERROR] Failed to read evaluator file: {e}")
                    
                    # Debug logging
                    print(f"[DEBUG BOTTOM] Tool: {selected_tool}")
                    print(f"[DEBUG BOTTOM] current_code length: {len(current_code) if current_code else 'None'}")
                    
                    # Load training data
                    training_data = tool_data.get('training_data', [])
                    # Show the modal
                    show_ai_improvement_modal(selected_tool, current_code, training_data)
            
            # Show code statistics at the bottom
            st.markdown("---")
            st.subheader("Code Statistics")
            
            col1, col2, col3, col4 = st.columns(4)
            
            lines = edited_code.split('\n')
            with col1:
                st.metric("Lines of Code", len(lines))
            with col2:
                st.metric("Characters", len(edited_code))
            
            # Count functions
            import re
            functions = re.findall(r'def\s+(\w+)', edited_code)
            with col3:
                st.metric("Functions", len(functions))
            
            # Show function list
            with col4:
                if functions:
                    st.markdown("**Functions found:**")
                    for func in functions:
                        st.markdown(f"- `def {func}()`")
                    
        else:
            st.warning("No evaluator code available")
            
            # Check if training data exists
            if tool_data.get('training_data'):
                st.info("‚úÖ Training data is available. You can now generate an evaluator.")
                
                if st.button("üöÄ Generate Evaluator", type="primary", key=f"generate_evaluator_{selected_tool}"):
                    with st.spinner("ü§ñ Generating evaluator..."):
                        try:
                            # Import the evaluator generator
                            from agent_evolve.generate_evaluators import EvaluatorGenerator
                            
                            # Check for OpenAI API key
                            if not os.getenv('OPENAI_API_KEY'):
                                st.error("‚ùå Error: OPENAI_API_KEY environment variable is required")
                                st.info("üí° Please set your OpenAI API key: export OPENAI_API_KEY=your_key_here")
                            else:
                                # Initialize generator
                                generator = EvaluatorGenerator(model_name="gpt-4o")
                                
                                # Generate evaluator for just this tool
                                tool_path = Path(tool_data['path'])
                                
                                # Force regeneration by removing existing evaluator
                                evaluator_file = tool_path / "evaluator.py"
                                if evaluator_file.exists():
                                    evaluator_file.unlink()
                                    st.info("üîÑ Removing existing evaluator to regenerate...")
                                
                                generator._generate_single_evaluator(tool_path)
                                
                                # Check if evaluator was actually created
                                if evaluator_file.exists():
                                    # Also regenerate config to use the correct metrics from the evaluator
                                    try:
                                        from agent_evolve.generate_openevolve_configs import OpenEvolveConfigGenerator
                                        config_generator = OpenEvolveConfigGenerator(str(tool_path.parent))
                                        config_generator._generate_single_config(tool_path, selected_tool)
                                        st.info("üîÑ Updated config with evaluator metrics")
                                    except Exception as config_e:
                                        st.warning(f"‚ö†Ô∏è Config update failed: {config_e}")
                                    
                                    with open(evaluator_file, 'r', encoding='utf-8') as f:
                                        tool_data['evaluator_code'] = f.read()
                                    st.success("‚úÖ Evaluator generated successfully!")
                                    st.info("Refreshing page to show the new evaluator...")
                                    st.rerun()
                                else:
                                    st.error("‚ùå Evaluator generation failed - file not created")
                                
                        except Exception as e:
                            st.error(f"‚ùå Error generating evaluator: {e}")
                            import traceback
                            st.code(traceback.format_exc())
            else:
                st.info("üí° Generate training data first, then you can create an evaluator")
                st.markdown("Run: `agent-evolve generate-training-data <tool_name>`")
    
    with evolved_code_tab:
        st.subheader("Code Comparison")
        
        # Build version options
        version_options = {}
        
        # Add original code
        if tool_data.get('original_code'):
            # Try to load original scores from initial_score.json
            original_scores = {}
            initial_score_file = Path(tool_data['path']) / "initial_score.json"
            if initial_score_file.exists():
                try:
                    with open(initial_score_file, 'r') as f:
                        original_scores = json.load(f)
                        # Remove combined_score if it exists
                        if 'combined_score' in original_scores:
                            original_scores = {k: v for k, v in original_scores.items() if k != 'combined_score'}
                except Exception as e:
                    print(f"Error loading initial scores: {e}")
            
            # Fallback to score_comparison if initial_score.json not found
            if not original_scores:
                score_comparison = tool_data.get('score_comparison') or {}
                original_scores = score_comparison.get('original_version', {}).get('scores', {}) if score_comparison else {}
            
            version_options["Original Code"] = {
                'code': tool_data['original_code'],
                'metrics': original_scores,
                'type': 'original'
            }
        
        # Add checkpoint versions
        if tool_data.get('checkpoints'):
            for checkpoint_data in sorted(tool_data['checkpoints'], key=lambda x: x['checkpoint']):
                checkpoint_num = checkpoint_data['checkpoint']
                checkpoint_info = checkpoint_data['info']
                
                # Try to load the checkpoint's best_program.py
                checkpoint_dir = Path(tool_data['path']) / "openevolve_output" / "checkpoints" / f"checkpoint_{checkpoint_num}"
                best_program_file = checkpoint_dir / "best_program.py"
                
                if best_program_file.exists():
                    try:
                        with open(best_program_file, 'r') as f:
                            content = f.read()
                            # Skip evolution metadata header
                            lines = content.split('\n')
                            code_start = 0
                            
                            if lines[0].strip().startswith('"""'):
                                for j in range(1, len(lines)):
                                    if lines[j].strip().endswith('"""'):
                                        docstring_content = '\n'.join(lines[0:j+1])
                                        if any(keyword in docstring_content for keyword in [
                                            'Best Evolved Version', 'Generated by OpenEvolve', 
                                            'Evolution Metrics:', 'Generation:', 'Iteration:'
                                        ]):
                                            code_start = j + 1
                                        break
                            
                            while code_start < len(lines) and not lines[code_start].strip():
                                code_start += 1
                            
                            checkpoint_code = '\n'.join(lines[code_start:])
                            checkpoint_metrics = checkpoint_info.get('metrics', {})
                            # Remove combined_score if present
                            if 'combined_score' in checkpoint_metrics:
                                checkpoint_metrics = {k: v for k, v in checkpoint_metrics.items() if k != 'combined_score'}
                            
                            version_options[f"Checkpoint {checkpoint_num}"] = {
                                'code': checkpoint_code,
                                'metrics': checkpoint_metrics,
                                'type': 'checkpoint',
                                'checkpoint': checkpoint_num
                            }
                    except Exception as e:
                        print(f"Error loading checkpoint {checkpoint_num}: {e}")
        
        if len(version_options) < 2:
            st.warning("Not enough code versions available for comparison")
            return
        
        # Version selectors
        col1, col2 = st.columns(2)
        version_keys = list(version_options.keys())
        
        # Default selections
        default_left = "Original Code" if "Original Code" in version_keys else version_keys[0]
        # Find highest checkpoint for default right selection
        highest_checkpoint = None
        for key in version_keys:
            if version_options[key]['type'] == 'checkpoint':
                if highest_checkpoint is None or version_options[key]['checkpoint'] > version_options[highest_checkpoint]['checkpoint']:
                    highest_checkpoint = key
        default_right = highest_checkpoint if highest_checkpoint else (version_keys[1] if len(version_keys) > 1 else version_keys[0])
        
        with col1:
            left_version = st.selectbox(
                "Left Version:",
                options=version_keys,
                index=version_keys.index(default_left) if default_left in version_keys else 0,
                key="left_version_select"
            )
        
        with col2:
            right_version = st.selectbox(
                "Right Version:", 
                options=version_keys,
                index=version_keys.index(default_right) if default_right in version_keys else 1,
                key="right_version_select"
            )
        
        # Show metrics comparison
        st.subheader("üìä Metrics Comparison")
        
        left_data = version_options[left_version]
        right_data = version_options[right_version]
        
        metrics_col1, metrics_col2, metrics_col3 = st.columns(3)
        
        with metrics_col1:
            st.markdown(f"**{left_version}**")
            if left_data['metrics']:
                for metric, value in left_data['metrics'].items():
                    st.metric(metric.title(), f"{value:.3f}")
            else:
                st.info("No metrics available")
        
        with metrics_col3:
            st.markdown(f"**{right_version}**")
            if right_data['metrics']:
                for metric, value in right_data['metrics'].items():
                    st.metric(metric.title(), f"{value:.3f}")
            else:
                st.info("No metrics available")
        
        with metrics_col2:
            st.markdown("**Difference**")
            if left_data['metrics'] and right_data['metrics']:
                for metric in left_data['metrics'].keys():
                    if metric in right_data['metrics']:
                        diff = right_data['metrics'][metric] - left_data['metrics'][metric]
                        st.metric(
                            metric.title(),
                            f"{diff:+.3f}",
                            delta=f"{diff:+.3f}",
                            delta_color="normal" if diff >= 0 else "inverse"
                        )
            else:
                st.info("Cannot calculate difference")
        
        # Code comparison
        st.subheader("üîç Code Comparison")
        
        # Diff view selector
        diff_view = st.radio(
            "Choose comparison view:",
            ["Unified Diff", "Side-by-Side Diff", "Code Blocks"],
            horizontal=True
        )
        
        left_code = left_data['code']
        right_code = right_data['code']
        
        if diff_view == "Unified Diff":
            diff_html = create_code_diff_html(left_code, right_code)
            st.markdown(diff_html, unsafe_allow_html=True)
            
        elif diff_view == "Side-by-Side Diff":
            side_by_side_html = create_side_by_side_diff_html(left_code, right_code)
            st.markdown(side_by_side_html, unsafe_allow_html=True)
            
        else:  # Code Blocks
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader(left_version)
                st.code(left_code, language='python')
            
            with col2:
                st.subheader(right_version)
                st.code(right_code, language='python')
    
    with metrics_tab:
        st.subheader("Metrics & Evolution Timeline")
        
        # Load initial scores for timeline
        initial_scores = {}
        initial_score_file = Path(tool_data['path']) / "initial_score.json"
        if initial_score_file.exists():
            try:
                with open(initial_score_file, 'r') as f:
                    initial_scores = json.load(f)
                    if 'combined_score' in initial_scores:
                        initial_scores = {k: v for k, v in initial_scores.items() if k != 'combined_score'}
            except Exception as e:
                st.error(f"Error loading initial scores: {e}")
        
        # Create timeline data including initial point
        timeline_data = []
        
        # Add initial checkpoint (checkpoint 0)
        if initial_scores:
            timeline_data.append({
                'checkpoint': 0,
                'scores': initial_scores,
                'is_initial': True
            })
        
        # Add all checkpoints
        if tool_data.get('checkpoints'):
            for checkpoint_data in sorted(tool_data['checkpoints'], key=lambda x: x['checkpoint']):
                checkpoint_info = checkpoint_data['info']
                metrics = checkpoint_info.get('metrics', {})
                if 'combined_score' in metrics:
                    metrics = {k: v for k, v in metrics.items() if k != 'combined_score'}
                
                if metrics:
                    timeline_data.append({
                        'checkpoint': checkpoint_data['checkpoint'],
                        'scores': metrics,
                        'info': checkpoint_info,
                        'is_initial': False
                    })
        
        if timeline_data:
            # Create timeline visualization
            import plotly.graph_objects as go
            fig = go.Figure()
            
            # Get all unique metrics
            all_metrics = set()
            for item in timeline_data:
                all_metrics.update(item['scores'].keys())
            
            # Create timeline chart
            for metric in sorted(all_metrics):
                x_values = []
                y_values = []
                colors = []
                
                for item in timeline_data:
                    if metric in item['scores']:
                        x_values.append(item['checkpoint'])
                        y_values.append(item['scores'][metric])
                        colors.append('red' if item['is_initial'] else 'blue')
                
                if x_values:
                    fig.add_trace(go.Scatter(
                        x=x_values,
                        y=y_values,
                        mode='lines+markers',
                        name=metric.title(),
                        line=dict(width=3),
                        marker=dict(size=10)
                    ))
            
            # Add annotations for key points
            if len(timeline_data) > 1:
                initial_point = timeline_data[0]
                final_point = timeline_data[-1]
                
                fig.add_annotation(
                    x=initial_point['checkpoint'],
                    y=max(initial_point['scores'].values()) if initial_point['scores'] else 0,
                    text="Initial",
                    showarrow=True,
                    arrowhead=2,
                    arrowcolor="red",
                    arrowwidth=2
                )
                
                fig.add_annotation(
                    x=final_point['checkpoint'],
                    y=max(final_point['scores'].values()) if final_point['scores'] else 0,
                    text="Latest",
                    showarrow=True,
                    arrowhead=2,
                    arrowcolor="green",
                    arrowwidth=2
                )
            
            fig.update_layout(
                title="Evolution Timeline",
                xaxis_title="Checkpoint",
                yaxis_title="Score",
                yaxis=dict(range=[0, 1.0]),
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            # Improvement Analysis
            if len(timeline_data) > 1:
                st.subheader("Improvement Analysis")
                best_checkpoint_data = max([item for item in timeline_data if not item['is_initial']], key=lambda x: x['checkpoint'])
                best_metrics = best_checkpoint_data['scores']
                
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.markdown("**Initial Scores**")
                    for metric, score in initial_scores.items():
                        st.metric(metric.title(), f"{score:.3f}")
                
                with col2:
                    st.markdown(f"**Best Scores (Checkpoint {best_checkpoint_data['checkpoint']})**")
                    for metric, score in best_metrics.items():
                        if metric in initial_scores:
                            delta = score - initial_scores[metric]
                            st.metric(metric.title(), f"{score:.3f}", delta=f"{delta:+.3f}")
                        else:
                            st.metric(metric.title(), f"{score:.3f}")
                
                with col3:
                    st.markdown("**Improvements**")
                    for metric in best_metrics.keys():
                        if metric in initial_scores:
                            improvement = best_metrics[metric] - initial_scores[metric]
                            color = "green" if improvement > 0 else "red" if improvement < 0 else "gray"
                            st.markdown(f"**{metric.title()}**: <span style='color:{color}'>{improvement:+.3f}</span>", 
                                       unsafe_allow_html=True)
            
            # Checkpoint selection and details
            st.subheader("Checkpoint Details")
            
            checkpoint_options = []
            if initial_scores:
                checkpoint_options.append("Initial (Checkpoint 0)")
            checkpoint_options.extend([f"Checkpoint {item['checkpoint']}" for item in timeline_data if not item['is_initial']])
            
            if checkpoint_options:
                selected_checkpoint = st.selectbox("Select checkpoint:", checkpoint_options)
                
                if selected_checkpoint:
                    if selected_checkpoint == "Initial (Checkpoint 0)":
                        st.subheader("Initial Program Evaluation")
                        col1, col2 = st.columns(2)
                        with col1:
                            st.markdown("**Scores:**")
                            for metric, score in initial_scores.items():
                                st.metric(metric.title(), f"{score:.3f}")
                        with col2:
                            st.markdown("**Details:**")
                            st.info("This is the baseline evaluation of the original program before any evolution.")
                    else:
                        checkpoint_num = int(selected_checkpoint.split()[-1])
                        checkpoint_item = next(item for item in timeline_data if item['checkpoint'] == checkpoint_num and not item['is_initial'])
                        
                        if checkpoint_item:
                            col1, col2 = st.columns(2)
                            with col1:
                                st.markdown("**Scores:**")
                                for metric, score in checkpoint_item['scores'].items():
                                    # Calculate improvement from initial
                                    if initial_scores and metric in initial_scores:
                                        delta = score - initial_scores[metric]
                                        st.metric(metric.title(), f"{score:.3f}", delta=f"{delta:+.3f}")
                                    else:
                                        st.metric(metric.title(), f"{score:.3f}")
                            
                            with col2:
                                st.markdown("**Checkpoint Info:**")
                                if 'info' in checkpoint_item:
                                    info = checkpoint_item['info']
                                    st.write(f"**Generation:** {info.get('generation', 'N/A')}")
                                    st.write(f"**Iteration:** {info.get('iteration', 'N/A')}")
                                    if 'timestamp' in info:
                                        try:
                                            from datetime import datetime
                                            # Parse the timestamp and format it as human readable
                                            timestamp = info['timestamp']
                                            
                                            if isinstance(timestamp, (int, float)):
                                                # Unix timestamp (seconds since epoch)
                                                dt = datetime.fromtimestamp(timestamp)
                                                formatted_time = dt.strftime('%B %d, %Y at %I:%M %p')
                                                st.write(f"**Created:** {formatted_time}")
                                            elif isinstance(timestamp, str):
                                                # Try common string timestamp formats
                                                try:
                                                    dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                                                except ValueError:
                                                    try:
                                                        dt = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')
                                                    except ValueError:
                                                        dt = datetime.strptime(timestamp, '%Y-%m-%dT%H:%M:%S')
                                                
                                                formatted_time = dt.strftime('%B %d, %Y at %I:%M %p')
                                                st.write(f"**Created:** {formatted_time}")
                                            else:
                                                st.write(f"**Created:** {timestamp}")
                                        except Exception as e:
                                            st.write(f"**Created:** {info['timestamp']}")
        else:
            st.warning("No timeline data available")

if __name__ == "__main__":
    main()