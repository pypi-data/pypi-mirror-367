{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e6cb5d9c8ae57b4",
   "metadata": {},
   "source": [
    "This notebook was used to compile all of the available data from the Utah Flux Network stations.  It should only need to be used once, as other notebooks are used to comile the newer data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e587dad2",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d356450d",
   "metadata": {},
   "source": [
    "## Import Standard Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T14:34:16.125093Z",
     "start_time": "2025-05-26T14:34:13.956898Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import sys\n",
    "import pathlib\n",
    "import glob\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "#from urllib.parse import quote\n",
    "#from sqlalchemy import create_engine\n",
    "#import configparser\n",
    "\n",
    "\n",
    "#import statsmodels.api as sm\n",
    "#import pingouin as pg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dca4ca2",
   "metadata": {},
   "source": [
    "## Import Micromet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61e3a29165852b08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T15:37:03.296515Z",
     "start_time": "2025-05-26T15:37:03.290259Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"../../src/\")\n",
    "import micromet\n",
    "import micromet.add_header_from_peer as ahp \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeadfcdb",
   "metadata": {},
   "source": [
    "## Initialize Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c940913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.WARNING)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setFormatter(\n",
    "    logging.Formatter(\n",
    "        fmt=\"%(levelname)s [%(asctime)s] %(name)s – %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    ")\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb5eef9",
   "metadata": {},
   "source": [
    "## Define the root folder for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f391f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_fold = pathlib.Path(f'G:/Shared drives/UGS_Flux/Data_Downloads/compiled')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d046c00c",
   "metadata": {},
   "source": [
    "# Run Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7fe141",
   "metadata": {},
   "source": [
    "Define the site folders and stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b045bef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_folders = {'US-UTD':'Dugout_Ranch',\n",
    "                'US-UTB':'BSF',\n",
    "                'US-UTJ':'Bluff',\n",
    "                'US-UTW':'Wellington',\n",
    "                'US-UTE':'Escalante',\n",
    "                'US-UTM':'Matheson',\n",
    "                'US-UTP':'Phrag',\n",
    "                'US-CdM':'Cedar_mesa',\n",
    "                'US-UTV':'Desert_View_Myton',\n",
    "                'US-UTN':'Juab',\n",
    "                'US-UTG':'Green_River',\n",
    "                'US-UTL':'Pelican_Lake',\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2245a878",
   "metadata": {},
   "source": [
    "## Compile Met Statistics Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc11ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ All possible files have been checked.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING [2025-08-06 13:00:40] __main__ – No TIMESTAMP column in dataframe\n",
      "WARNING [2025-08-06 13:00:40] __main__ – No valid files found in G:\\Shared drives\\UGS_Flux\\Data_Downloads\\compiled\\US-UTB\\Statistics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ All possible files have been checked.\n",
      "\n",
      "✔ All possible files have been checked.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING [2025-08-06 13:00:40] __main__ – No TIMESTAMP column in dataframe\n",
      "WARNING [2025-08-06 13:00:41] __main__ – No valid files found in G:\\Shared drives\\UGS_Flux\\Data_Downloads\\compiled\\US-UTW\\Statistics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ All possible files have been checked.\n",
      "\n",
      "✔ All possible files have been checked.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING [2025-08-06 13:00:41] __main__ – No TIMESTAMP column in dataframe\n",
      "WARNING [2025-08-06 13:00:41] __main__ – No valid files found in G:\\Shared drives\\UGS_Flux\\Data_Downloads\\compiled\\US-UTM\\Statistics\n",
      "WARNING [2025-08-06 13:00:41] __main__ – No valid files found in G:\\Shared drives\\UGS_Flux\\Data_Downloads\\compiled\\US-UTP\\Statistics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ All possible files have been checked.\n",
      "\n",
      "✔ All possible files have been checked.\n",
      "\n",
      "✔ All possible files have been checked.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING [2025-08-06 13:00:42] __main__ – No TIMESTAMP column in dataframe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ All possible files have been checked.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING [2025-08-06 13:00:43] __main__ – No TIMESTAMP column in dataframe\n",
      "WARNING [2025-08-06 13:00:43] __main__ – No valid files found in G:\\Shared drives\\UGS_Flux\\Data_Downloads\\compiled\\US-UTN\\Statistics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ All possible files have been checked.\n",
      "\n",
      "✔ All possible files have been checked.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING [2025-08-06 13:00:44] __main__ – No TIMESTAMP column in dataframe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ All possible files have been checked.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING [2025-08-06 13:00:45] __main__ – No TIMESTAMP column in dataframe\n"
     ]
    }
   ],
   "source": [
    "comp_edd_df = {}\n",
    "outlier_report = {}\n",
    "\n",
    "am = micromet.AmerifluxDataProcessor(logger=logger)\n",
    "\n",
    "for key, value in site_folders.items():\n",
    "\n",
    "    parent_fold = raw_fold / f\"{key}\" / \"Statistics\"\n",
    "    ahp.scan(parent_fold, min_sim=0.3, backup=False)\n",
    "    pths = micromet.fix_all_in_parent(parent_fold)\n",
    "    raw_data = am.raw_file_compile(raw_fold, parent_fold, search_str = \"TOA5*Statistics*.dat\")\n",
    "    if raw_data is not None:\n",
    "        am_data = micromet.Reformatter(drop_soil=False,\n",
    "                                       logger=logger,\n",
    "                                       )\n",
    "        #raw_data = raw_data.drop([0], axis=0)\n",
    "        am_df, report = am_data.prepare(raw_data, data_type=\"met\")\n",
    "        comp_edd_df[key] = am_df\n",
    "        outlier_report[key] = report\n",
    "\n",
    "        am_df.to_csv(raw_fold / f\"{key}\" / f\"{key}_metStat.csv\")\n",
    "\n",
    "comp_edd = pd.concat(comp_edd_df)\n",
    "outlier_report = pd.concat(outlier_report)\n",
    "comp_edd.to_parquet(raw_fold / \"comp_met_stat.parquet\")\n",
    "outlier_report.to_csv(raw_fold / \"outlier_report_metstat.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ef6a1c",
   "metadata": {},
   "source": [
    "## Compile Downloaded Eddy Data from EasyFluxWeb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b346535",
   "metadata": {},
   "outputs": [],
   "source": [
    "easyfluxdf = {}\n",
    "ef_reports = {}\n",
    "\n",
    "for key, value in site_folders.items():\n",
    "    site_dir = raw_fold / key\n",
    "    for file in site_dir.glob(\"*_Flux_AmeriFluxFormat.dat\"):\n",
    "\n",
    "        am_data = micromet.Reformatter(drop_soil=False,\n",
    "                                            logger=logger,\n",
    "                                            )\n",
    "        df = pd.read_csv(file,skiprows=[0,2,3],\n",
    "                        na_values=[-9999,\"NAN\",\"NaN\",\"nan\"])\n",
    "        \n",
    "        df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'])\n",
    "\n",
    "        am_df, report = am_data.prepare(df, data_type=\"eddy\")\n",
    "        easyfluxdf[key] = am_df\n",
    "        ef_reports[key] = report\n",
    "\n",
    "easyflux = pd.concat(easyfluxdf)\n",
    "ef_report = pd.concat(ef_reports, axis=1).T\n",
    "\n",
    "easyflux.to_parquet(raw_fold / \"easyflux.parquet\")\n",
    "ef_report.to_csv(raw_fold / \"easyflux_report.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d651509",
   "metadata": {},
   "source": [
    "## Compile Ameriflux Format dat files from Dataloggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46aac744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ All possible files have been checked.\n",
      "\n",
      "✔ All possible files have been checked.\n",
      "\n",
      "✔ All possible files have been checked.\n",
      "\n",
      "✔ All possible files have been checked.\n",
      "\n",
      "✔ All possible files have been checked.\n",
      "\n",
      "✔ All possible files have been checked.\n",
      "\n",
      "✔ All possible files have been checked.\n",
      "\n",
      "✔ All possible files have been checked.\n",
      "\n",
      "✔ All possible files have been checked.\n",
      "\n",
      "✔ All possible files have been checked.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paulinkenbrandt\\.conda\\envs\\py313\\Lib\\site-packages\\pandas\\core\\dtypes\\cast.py:377: RuntimeWarning: invalid value encountered in cast\n",
      "  new_result = trans(result).astype(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ All possible files have been checked.\n",
      "\n",
      "✔ All possible files have been checked.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "comp_edd_df = {}\n",
    "outlier_report = {}\n",
    "\n",
    "am = micromet.AmerifluxDataProcessor(logger=logger)\n",
    "\n",
    "for key, value in site_folders.items():\n",
    "\n",
    "    parent_fold = raw_fold / f\"{key}\" / \"AmeriFluxFormat\"\n",
    "    ahp.scan(parent_fold, min_sim=0.3, backup=False)\n",
    "    pths = micromet.fix_all_in_parent(parent_fold)\n",
    "    raw_data = am.raw_file_compile(raw_fold, parent_fold, search_str = \"*Flux_AmeriFluxFormat*.dat\")\n",
    "    if raw_data is not None:\n",
    "        am_data = micromet.Reformatter(drop_soil=False,\n",
    "                                       logger=logger,\n",
    "                                       )\n",
    "        #raw_data = raw_data.drop([0], axis=0)\n",
    "        am_df, report = am_data.prepare(raw_data, data_type=\"eddy\")\n",
    "        comp_edd_df[key] = am_df\n",
    "        outlier_report[key] = report\n",
    "\n",
    "        timestart = am_df['TIMESTAMP_START'].values[0]\n",
    "        timeend = am_df['TIMESTAMP_END'].values[-1]\n",
    "\n",
    "        am_df.to_csv(raw_fold / f\"{key}\" / f\"{key}_HH_{timestart:}_{timeend:}.csv\")\n",
    "\n",
    "comp_edd = pd.concat(comp_edd_df)\n",
    "outlier_report = pd.concat(outlier_report)\n",
    "comp_edd.to_parquet(raw_fold / \"comp_edd.parquet\")\n",
    "outlier_report.to_csv(raw_fold / \"outlier_report_edd.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1720acb",
   "metadata": {},
   "source": [
    "## Compile Met Ameriflux Format .dat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad20f401f01ce92f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T16:13:42.754030Z",
     "start_time": "2025-05-17T16:05:51.349043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ All possible files have been checked.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING [2025-08-06 13:12:31] __main__ – No valid files found in G:\\Shared drives\\UGS_Flux\\Data_Downloads\\compiled\\US-UTB\\Statistics_Ameriflux\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ All possible files have been checked.\n",
      "\n",
      "✔ All possible files have been checked.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paulinkenbrandt\\.conda\\envs\\py313\\Lib\\site-packages\\pandas\\core\\dtypes\\cast.py:377: RuntimeWarning: invalid value encountered in cast\n",
      "  new_result = trans(result).astype(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ All possible files have been checked.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paulinkenbrandt\\.conda\\envs\\py313\\Lib\\site-packages\\pandas\\core\\dtypes\\cast.py:377: RuntimeWarning: invalid value encountered in cast\n",
      "  new_result = trans(result).astype(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ All possible files have been checked.\n",
      "\n",
      "✔ All possible files have been checked.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paulinkenbrandt\\.conda\\envs\\py313\\Lib\\site-packages\\pandas\\core\\dtypes\\cast.py:377: RuntimeWarning: invalid value encountered in cast\n",
      "  new_result = trans(result).astype(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ All possible files have been checked.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paulinkenbrandt\\.conda\\envs\\py313\\Lib\\site-packages\\pandas\\core\\dtypes\\cast.py:377: RuntimeWarning: invalid value encountered in cast\n",
      "  new_result = trans(result).astype(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ All possible files have been checked.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paulinkenbrandt\\.conda\\envs\\py313\\Lib\\site-packages\\pandas\\core\\dtypes\\cast.py:377: RuntimeWarning: invalid value encountered in cast\n",
      "  new_result = trans(result).astype(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ All possible files have been checked.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING [2025-08-06 13:19:21] __main__ – No valid files found in G:\\Shared drives\\UGS_Flux\\Data_Downloads\\compiled\\US-UTN\\Statistics_Ameriflux\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ All possible files have been checked.\n",
      "\n",
      "✔ All possible files have been checked.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paulinkenbrandt\\.conda\\envs\\py313\\Lib\\site-packages\\pandas\\core\\dtypes\\cast.py:377: RuntimeWarning: invalid value encountered in cast\n",
      "  new_result = trans(result).astype(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔ All possible files have been checked.\n"
     ]
    }
   ],
   "source": [
    "comp_edd_df = {}\n",
    "outlier_report = {}\n",
    "\n",
    "am = micromet.AmerifluxDataProcessor(logger=logger)\n",
    "\n",
    "for key, value in site_folders.items():\n",
    "\n",
    "    parent_fold = raw_fold / f\"{key}\" / \"Statistics_Ameriflux\"\n",
    "    ahp.scan(parent_fold, min_sim=0.3, backup=False)\n",
    "    pths = micromet.fix_all_in_parent(parent_fold)\n",
    "    raw_data = am.raw_file_compile(raw_fold, parent_fold, search_str = \"*Statistics_AmeriFlux*.dat\")\n",
    "    if raw_data is not None:\n",
    "        am_data = micromet.Reformatter(drop_soil=False,\n",
    "                                       logger=logger,\n",
    "                                       )\n",
    "        #raw_data = raw_data.drop([0], axis=0)\n",
    "        am_df, report = am_data.prepare(raw_data, data_type=\"met\")\n",
    "        comp_edd_df[key] = am_df\n",
    "        outlier_report[key] = report\n",
    "\n",
    "        timestart = am_df['TIMESTAMP_START'].values[0]\n",
    "        timeend = am_df['TIMESTAMP_END'].values[-1]\n",
    "\n",
    "        am_df.to_csv(raw_fold / f\"{key}\" / f\"{key}-met_HH_{timestart:}_{timeend:}.csv\")\n",
    "\n",
    "comp_met = pd.concat(comp_edd_df)\n",
    "out_report_met = pd.concat(outlier_report)\n",
    "comp_met.to_parquet(raw_fold / \"comp_met.parquet\")\n",
    "out_report_met.to_csv(raw_fold / \"outlier_report_met.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8792cc9c",
   "metadata": {},
   "source": [
    "Compile files from each station into a a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d61bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.concat(comp_edd_df, axis=0)\n",
    "cdf.index.set_names(['stationid','datetime_start'],inplace=True)\n",
    "#cdf.rename(columns={'level_0':'stationid'},inplace=True)\n",
    "#cdf.to_parquet('../station_data/all_data.parquet')\n",
    "for col in cdf.columns:\n",
    "    cdf.rename(columns={col:col.lower()},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf106f08",
   "metadata": {},
   "source": [
    "Save to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d86dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.to_parquet('../../station_data/all_eddy_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f62bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577962db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:18:32.176313Z",
     "start_time": "2025-05-26T21:18:31.858982Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "comp_met_df = {}\n",
    "root_dir = \"C:/Users/paulinkenbrandt/Documents/GitHub/MicroMet/src/micromet/data/\"\n",
    "config_path = root_dir + \"reformatter_vars.yml\"\n",
    "var_limits_csv = root_dir + \"extreme_values.csv\"\n",
    "am = micromet.AmerifluxDataProcessor(config_path, logger)\n",
    "\n",
    "\n",
    "for key, value in site_folders.items():\n",
    "\n",
    "    print(key)\n",
    "    raw_fold = pathlib.Path('G:/Shared drives/UGS_Flux/Data_Downloads/')\n",
    "    raw_data = am.raw_file_compile(raw_fold, value, search_str = \"*Statistics_AmeriFlux*.dat\")\n",
    "    if raw_data is not None:\n",
    "        am_data = micromet.Reformatter(\n",
    "                                       config_path=config_path,\n",
    "                                       var_limits_csv= var_limits_csv,\n",
    "                                       drop_soil=False,\n",
    "                                       logger=logger,\n",
    "                                       )\n",
    "        am_df = am_data.prepare(raw_data, data_type=\"met\")\n",
    "        #am_df = am_data.et_data\n",
    "        comp_met_df[key] = am_df\n",
    "\n",
    "        #am_df.to_csv(f\"../../station_data/{key}_HH_{am_df['TIMESTAMP_START'].values[0]:}_{am_df['TIMESTAMP_END'].values[-1]:}.csv\")\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64217e3c28f06cc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:18:41.983531Z",
     "start_time": "2025-05-26T21:18:41.964155Z"
    }
   },
   "outputs": [],
   "source": [
    "ddf.columns = ddf.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d44a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "soildfs\n",
    "\n",
    "for old_col, new_col in mapping.items():\n",
    "    if str(old_col).lower() in soildfs.columns.str.lower():\n",
    "        if str(new_col).lower() in soildfs.columns.str.lower():\n",
    "            soildfs[new_col.lower()] = soildfs[[old_col.lower(), new_col.lower()]].max(axis=1)\n",
    "            soildfs = soildfs.drop(old_col.lower(), axis=1)\n",
    "        else:\n",
    "            soildfs = soildfs.rename(columns={old_col.lower(): new_col.lower()})\n",
    "    elif str(old_col).lower()+\"_eddy\" in soildfs.columns.str.lower():\n",
    "        print(f\"Found {old_col} eddy column\")\n",
    "        if str(new_col).lower()+\"_eddy\" in soildfs.columns.str.lower():\n",
    "            soildfs[new_col.lower()] = soildfs[[old_col.lower()+\"_eddy\", new_col.lower()+\"_eddy\"]].max(axis=1)\n",
    "            soildfs = soildfs.drop(old_col.lower()+\"_eddy\", axis=1)\n",
    "        else:\n",
    "            soildfs = soildfs.rename(columns={old_col.lower()+\"_eddy\": new_col.lower()})\n",
    "    elif str(new_col).lower()+\"_eddy\" in soildfs.columns.str.lower():\n",
    "        if str(new_col).lower() in soildfs.columns.str.lower():\n",
    "            soildfs[new_col.lower()] = soildfs[[new_col.lower()+\"_eddy\", new_col.lower()+\"_eddy\"]].max(axis=1)\n",
    "            soildfs = soildfs.drop(new_col.lower()+\"_eddy\", axis=1)\n",
    "            print(f\"Found {new_col} eddy column\")\n",
    "        else:\n",
    "            print(f\"Found {new_col} eddy column\")\n",
    "            soildfs = soildfs.rename(columns={new_col.lower()+\"_eddy\": new_col.lower()})\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = pd.concat(comp_met_df, axis=0)\n",
    "ddf.index.set_names(['stationid','datetime_start'],inplace=True)\n",
    "#cdf.rename(columns={'level_0':'stationid'},inplace=True)\n",
    "#cdf.to_parquet('../station_data/all_data.parquet')\n",
    "for col in ddf.columns:\n",
    "    ddf.rename(columns={col:col.lower()},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf042fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[~ddf['vwc_2_7_1'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef3316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.iloc[0:1,:].to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f082e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "soilcols = [col.lower() for col in am_data.MATH_SOILS_V2]\n",
    "pattern = re.compile(r\"2_1_1|1_2_1|1_1_2\")\n",
    "# Print matching columns\n",
    "matching_cols = [col for col in soilcols if pattern.search(col)]\n",
    "# Remove them from the original list\n",
    "soilcols = [col for col in soilcols if not pattern.search(col)]\n",
    "\n",
    "        \n",
    "soildfs = pd.merge(ddf,cdf[soilcols],how='left',on=['stationid','datetime_start'],suffixes=(None,'_eddy'))\n",
    "soildfs\n",
    "\n",
    "for col in cdf.columns:\n",
    "    if col in soilcols:\n",
    "        cdf.drop(columns=col,inplace=True)  # drop the soil columns from the main dataframe\n",
    "\n",
    "cdf.to_parquet('../../station_data/all_eddy_data.parquet')\n",
    "\n",
    "soildfs.to_parquet('../../station_data/all_soil_data.parquet')\n",
    "\n",
    "ddf.to_parquet('../../station_data/all_met_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f9e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.read_parquet('../../station_data/all_eddy_data.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84295da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cc9ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "soildfs = pd.read_parquet('../../station_data/all_soil_data.parquet')\n",
    "utd_soilt = soildfs.loc['US-UTD'][['ts_3_1_1','ts_3_2_1','ts_3_3_1']].replace(-9999,np.nan)\n",
    "utd_soilt = utd_soilt[utd_soilt.index >= '2024-07-01']#.resample('30T').mean()\n",
    "utd_soilt['ts_3_1_1'].plot()\n",
    "utd_soilt['ts_3_2_1'].shift(-1).plot()\n",
    "utd_soilt['ts_3_3_1'].shift(-5).plot()\n",
    "plt.axvline('2024-07-04 15:00',color='r')\n",
    "#plt.xlim('2024-07-01','2024-07-08')\n",
    "#plt.ylim(10,35)\n",
    "plt.grid(True, which='minor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy.signal import correlate\n",
    "\n",
    "# Function to decompose the seasonal component\n",
    "def extract_seasonal(ts, period):\n",
    "    decomposition = seasonal_decompose(ts, model='additive', period=period)\n",
    "    return decomposition.seasonal\n",
    "\n",
    "# Function to calculate lag between two seasonal series using cross-correlation\n",
    "def calculate_lag(seasonal1, seasonal2):\n",
    "    n = len(seasonal1)\n",
    "    correlation = correlate(seasonal1 - np.mean(seasonal1), seasonal2 - np.mean(seasonal2), mode='full')\n",
    "    lags = np.arange(-n + 1, n)\n",
    "    lag = lags[np.argmax(correlation)]\n",
    "    return lag, correlation, lags\n",
    "\n",
    "ts1 = utd_soilt['ts_3_2_1']\n",
    "ts2 = utd_soilt['ts_3_3_1']\n",
    "#utd_soilt['ts_3_3_1'].shift(-5).plot()\n",
    "\n",
    "\n",
    "# Extract seasonal components\n",
    "seasonal1 = extract_seasonal(ts1, period=48)\n",
    "seasonal2 = extract_seasonal(ts2, period=48)\n",
    "\n",
    "# Calculate lag\n",
    "lag, correlation, lags = calculate_lag(seasonal1.dropna(), seasonal2.dropna())\n",
    "\n",
    "# Output\n",
    "print(f\"Calculated lag: {lag/2} hours\")\n",
    "\n",
    "# Plot seasonal components and correlation\n",
    "fig, ax = plt.subplots(3, 1, figsize=(10, 8))\n",
    "\n",
    "seasonal1.plot(ax=ax[0], label='Seasonal Component 1')\n",
    "seasonal2.plot(ax=ax[0], label='Seasonal Component 2')\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Seasonal Components')\n",
    "ax[0].set_xlim(pd.to_datetime('2024-07-01'),pd.to_datetime('2024-07-08'))\n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].plot(lags, correlation)\n",
    "ax[1].set_title('Cross-Correlation')\n",
    "ax[1].set_xlabel('Lag (hours)')\n",
    "ax[1].set_ylabel('Correlation')\n",
    "ax[1].set_xlim(-10, 10)\n",
    "ax[1].grid(True)\n",
    "\n",
    "ax[2].plot(seasonal1.index, seasonal1, label='Series 1')\n",
    "ax[2].plot(seasonal2.index + pd.Timedelta(hours=lag/2), seasonal2, label='Series 2 (Shifted)')\n",
    "ax[2].legend()\n",
    "ax[2].set_title(f'Series alignment (Lag: {lag/2} hours)')\n",
    "ax[2].set_xlim(pd.to_datetime('2024-07-01'),pd.to_datetime('2024-07-08'))\n",
    "ax[2].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04ee994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc536c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.read_parquet('../../station_data/all_eddy_data.parquet')\n",
    "ddf = pd.read_parquet('../../station_data/all_met_data.parquet')\n",
    "\n",
    "for col in cdf.columns:\n",
    "    if col in ddf.columns:\n",
    "        print(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head(10).to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549094d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = ddf.loc['US-UTD','t_si111_body'].replace(-9999,np.nan)\n",
    "series.plot()\n",
    "series.diff().plot()\n",
    "new_series = series[series.diff()<2].diff().cumsum()\n",
    "new_series.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddcebdfd6a7b51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "config.read('../../secrets/config.ini')\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import urllib.parse\n",
    "host = config['DEFAULT']['ip']\n",
    "pw = config['DEFAULT']['pw']\n",
    "user = config['DEFAULT']['login']\n",
    "\n",
    "encoded_password = urllib.parse.quote_plus(pw)\n",
    "\n",
    "def postconn_et(encoded_password, host='localhost',user='postgres',port='5432',db='groundwater', schema = 'groundwater'):\n",
    "    connection_text = \"postgresql+psycopg2://{:}:{:}@{:}:{:}/{:}?gssencmode=disable\".format(user,encoded_password,host,port,db)\n",
    "    return create_engine(connection_text, connect_args={'options': '-csearch_path={}'.format(schema)})\n",
    "\n",
    "\n",
    "engine = postconn_et(encoded_password, host=host, user=user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ce788442a9680",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.to_sql(name = 'amfluxeddy',\n",
    "           schema='groundwater',\n",
    "           con=engine,\n",
    "           if_exists='replace',\n",
    "           chunksize=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f92cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in soildfs.columns:\n",
    "    print(f\"amfluxmet.{col},\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5819ddd94230e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "soildfs.to_sql(name = 'amfluxmet',\n",
    "           schema='groundwater',\n",
    "           con=engine,\n",
    "           if_exists='replace',\n",
    "           chunksize=2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
