# EfficientLLM Benchmark Suite for Qwen2.5

è¿™ä¸ªåŸºå‡†æµ‹è¯•å¥—ä»¶å®ç°äº†è®ºæ–‡ä¸­æåˆ°çš„å„ç§é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶ã€ä½ç½®ç¼–ç æ–¹æ³•å’ŒMoEå˜ä½“ï¼Œç”¨äºç”ŸæˆEfficientLLMè¯„ä¼°è¡¨æ ¼ã€‚

## ğŸ“Š å®ç°çš„å˜ä½“

### ğŸ” æ³¨æ„åŠ›æœºåˆ¶
- **MQA (Multi-Query Attention)** - å•ä¸€K/Vå¤´ï¼Œå¤šä¸ªQå¤´
- **GQA (Grouped-Query Attention)** - Qwen2.5é»˜è®¤ï¼Œåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›
- **MLA (Multi-Head Latent Attention)** - åŸºäºDeepSeek-V2çš„æ½œåœ¨æ³¨æ„åŠ›
- **NSA (Native Sparse Attention)** - åŸç”Ÿç¨€ç–æ³¨æ„åŠ›

### ğŸ“ ä½ç½®ç¼–ç 
- **RoPE** - æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆé»˜è®¤ï¼‰
- **Absolute** - ç»å¯¹ä½ç½®ç¼–ç 
- **Learnable Absolute** - å¯å­¦ä¹ ç»å¯¹ä½ç½®ç¼–ç 
- **Relative** - ç›¸å¯¹ä½ç½®ç¼–ç 
- **None** - æ— ä½ç½®ç¼–ç 

### ğŸ¯ MoEå˜ä½“
- **Dense** - å¯†é›†æ¨¡å‹åŸºçº¿
- **MoE** - ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆ8ä¸ªä¸“å®¶ï¼ŒTop-2è·¯ç”±ï¼‰

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç”Ÿæˆé…ç½®æ–‡ä»¶
```bash
cd /work/nvme/bemy/zyuan2/code/efficientllm/pretrain/Pai-Megatron-Patch
bash scripts/generate_efficientllm_configs.sh
```

### 2. å¿«é€Ÿæµ‹è¯•ï¼ˆéªŒè¯æ‰€æœ‰å˜ä½“æ˜¯å¦å·¥ä½œï¼‰
```bash
bash scripts/test_efficientllm_variants.sh
```

### 3. è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•
```bash
# è®¾ç½®æ•°æ®é›†è·¯å¾„
export DATASET_PATH=/path/to/your/dataset
export VALID_DATASET_PATH=/path/to/your/valid_dataset

# è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•
bash scripts/run_efficientllm_benchmark.sh

# æˆ–è€…è¿è¡Œç‰¹å®šç±»å‹
bash scripts/run_efficientllm_benchmark.sh attention    # æ³¨æ„åŠ›æœºåˆ¶
bash scripts/run_efficientllm_benchmark.sh positional  # ä½ç½®ç¼–ç 
bash scripts/run_efficientllm_benchmark.sh moe         # MoEå˜ä½“
```

## ğŸ“ ç›®å½•ç»“æ„

```
examples/efficientllm_benchmark/
â”œâ”€â”€ attention_variants/          # æ³¨æ„åŠ›æœºåˆ¶å˜ä½“
â”‚   â”œâ”€â”€ run_mqa_0.5B.sh         # MQA 0.5B
â”‚   â”œâ”€â”€ run_mqa_1.5B.sh         # MQA 1.5B
â”‚   â”œâ”€â”€ run_mqa_3B.sh           # MQA 3B
â”‚   â”œâ”€â”€ run_gqa_*.sh            # GQAå˜ä½“
â”‚   â”œâ”€â”€ run_mla_*.sh            # MLAå˜ä½“
â”‚   â””â”€â”€ run_nsa_*.sh            # NSAå˜ä½“
â”œâ”€â”€ positional_encoding_variants/ # ä½ç½®ç¼–ç å˜ä½“
â”‚   â”œâ”€â”€ run_gqa_rope_1.5B.sh    # RoPE
â”‚   â”œâ”€â”€ run_gqa_absolute_1.5B.sh # ç»å¯¹ä½ç½®ç¼–ç 
â”‚   â””â”€â”€ ...                     # å…¶ä»–ç¼–ç æ–¹å¼
â””â”€â”€ moe_variants/               # MoEå˜ä½“
    â”œâ”€â”€ run_dense_1.5B.sh       # å¯†é›†æ¨¡å‹
    â”œâ”€â”€ run_dense_3B.sh         # å¯†é›†æ¨¡å‹
    â”œâ”€â”€ run_moe_0_5Bx8.sh       # MoE 0.5BÃ—8
    â””â”€â”€ run_moe_1_5Bx8.sh       # MoE 1.5BÃ—8

results/efficientllm_benchmark/  # ç»“æœè¾“å‡º
â”œâ”€â”€ attention_results/           # æ³¨æ„åŠ›æœºåˆ¶ç»“æœ
â”œâ”€â”€ positional_encoding_results/ # ä½ç½®ç¼–ç ç»“æœ
â”œâ”€â”€ moe_results/                # MoEç»“æœ
â”œâ”€â”€ attention_mechanisms_table.txt    # æ³¨æ„åŠ›æœºåˆ¶æ±‡æ€»è¡¨
â”œâ”€â”€ positional_encoding_table.txt     # ä½ç½®ç¼–ç æ±‡æ€»è¡¨
â””â”€â”€ moe_table.txt                     # MoEæ±‡æ€»è¡¨
```

## ğŸ”§ æ ¸å¿ƒå®ç°æ–‡ä»¶

### æ³¨æ„åŠ›æœºåˆ¶å®ç°
- `megatron_patch/model/qwen2/transformer/mqa_attention.py` - MQAå®ç°
- `megatron_patch/model/qwen2/transformer/mla_attention.py` - MLAå®ç°
- `megatron_patch/model/qwen2/transformer/nsa_attention.py` - NSAå®ç°

### ä½ç½®ç¼–ç å®ç°
- `megatron_patch/model/qwen2/positional_encodings.py` - å„ç§ä½ç½®ç¼–ç 

### EfficientLLMæŒ‡æ ‡
- `megatron_patch/efficientllm_metrics.py` - æ•ˆç‡æŒ‡æ ‡æ”¶é›†
- è‡ªåŠ¨æ”¶é›†AMUã€PCUã€ALã€TTã€STã€AECç­‰æŒ‡æ ‡

## ğŸ“Š ç”Ÿæˆçš„æŒ‡æ ‡

æ¯ä¸ªåŸºå‡†æµ‹è¯•ä¼šè‡ªåŠ¨æ”¶é›†ä»¥ä¸‹EfficientLLMæŒ‡æ ‡ï¼š

1. **AMU (Average Memory Utilization)** - å¹³å‡å†…å­˜åˆ©ç”¨ç‡ (GB)
2. **PCU (Peak Compute Utilization)** - å³°å€¼è®¡ç®—åˆ©ç”¨ç‡ (æ¯”ç‡)
3. **AL (Average Latency)** - å¹³å‡å»¶è¿Ÿ (ç§’/è¿­ä»£)
4. **TT (Token Throughput)** - ä»¤ç‰Œååé‡ (ä»¤ç‰Œ/å‚æ•°/ç§’)
5. **ST (Sample Throughput)** - æ ·æœ¬ååé‡ (æ ·æœ¬/å‚æ•°/ç§’)
6. **AEC (Average Energy Consumption)** - å¹³å‡èƒ½è€— (ç“¦ç‰¹)
7. **PPL (Perplexity)** - å›°æƒ‘åº¦
8. **GPU Hours** - GPUè®­ç»ƒæ—¶é—´

## ğŸ¯ æ¨¡å‹é…ç½®

### æ³¨æ„åŠ›æœºåˆ¶åŸºå‡†æµ‹è¯•
| æ–¹æ³• | å‚æ•°é‡ | å¾®æ‰¹é‡å¤§å° | æè¿° |
|------|--------|------------|------|
| MQA  | 0.5B/1.5B/3B | 4/2/1 | å•ä¸€K/Vå¤´ |
| GQA  | 0.5B/1.5B/3B | 4/2/1 | åˆ†ç»„æŸ¥è¯¢ï¼ˆé»˜è®¤ï¼‰ |
| MLA  | 0.5B/1.5B/3B | 4/2/1 | æ½œåœ¨æ³¨æ„åŠ› |
| NSA  | 0.5B/1.5B/3B | 4/2/1 | ç¨€ç–æ³¨æ„åŠ› |

### ä½ç½®ç¼–ç åŸºå‡†æµ‹è¯•
æ‰€æœ‰ä½ç½®ç¼–ç å˜ä½“ä½¿ç”¨1.5B GQAæ¨¡å‹ï¼Œä¸Šä¸‹æ–‡é•¿åº¦8Kã€‚

### MoEåŸºå‡†æµ‹è¯•
| æ–¹æ³• | é…ç½® | Top-K | æè¿° |
|------|------|-------|------|
| Dense | 1.5B/3B | - | å¯†é›†åŸºçº¿ |
| MoE | 0.5BÃ—8/1.5BÃ—8 | 2 | 8ä¸ªä¸“å®¶ |

## âš™ï¸ è‡ªå®šä¹‰é…ç½®

### ä¿®æ”¹è®­ç»ƒå‚æ•°
ç¼–è¾‘ç”Ÿæˆçš„è„šæœ¬æ–‡ä»¶ï¼Œè°ƒæ•´ï¼š
- `TRAIN_TOKENS` - è®­ç»ƒä»¤ç‰Œæ•°
- `BATCH_SIZE` - æ‰¹é‡å¤§å°
- `SEQ_LEN` - åºåˆ—é•¿åº¦
- `LR` - å­¦ä¹ ç‡

### è‡ªå®šä¹‰æ³¨æ„åŠ›å‚æ•°
å¯¹äºNSAï¼š
```bash
--nsa-sliding-window-size 128
--nsa-compress-block-size 64
--nsa-num-selected-blocks 4
```

å¯¹äºMLAï¼š
```bash
--mla-latent-dim 192  # hidden_size/8
--mla-qk-head-dim 128
```

## ğŸ“ˆ ç»“æœåˆ†æ

è¿è¡Œå®Œæˆåï¼Œæ£€æŸ¥ï¼š

1. **æ§åˆ¶å°æ—¥å¿—** - å®æ—¶EfficientLLMæŒ‡æ ‡
2. **TensorBoard** - å¯è§†åŒ–æŒ‡æ ‡æ›²çº¿
3. **æ±‡æ€»è¡¨æ ¼** - æ‰€æœ‰å˜ä½“çš„å¯¹æ¯”ç»“æœ

```bash
# æŸ¥çœ‹TensorBoard
tensorboard --logdir results/efficientllm_benchmark/*/tensorboard/

# æŸ¥çœ‹æ±‡æ€»è¡¨æ ¼
cat results/efficientllm_benchmark/attention_mechanisms_table.txt
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **å†…å­˜ä¸è¶³**
   - å‡å°`BATCH_SIZE`
   - å¯ç”¨æ¿€æ´»æ£€æŸ¥ç‚¹ï¼š`AC=full`

2. **GPUåˆ©ç”¨ç‡ä½**
   - æ£€æŸ¥æ•°æ®åŠ è½½æ˜¯å¦ä¸ºç“¶é¢ˆ
   - è°ƒæ•´`--num-workers`

3. **æŒ‡æ ‡æ”¶é›†å¤±è´¥**
   - ç¡®ä¿nvidia-smiå¯ç”¨
   - æ£€æŸ¥`EFFICIENTLLM_METRICS_ENABLED=true`

### è°ƒè¯•æ¨¡å¼
```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export EFFICIENTLLM_LOG_INTERVAL=1
export EFFICIENTLLM_COLLECTION_INTERVAL=0.5

# è¿è¡Œå•ä¸ªå˜ä½“è¿›è¡Œè°ƒè¯•
cd examples/efficientllm_benchmark/attention_variants
./run_mqa_0.5B.sh dsw
```

## ğŸ“š å‚è€ƒ

- [EfficientLLMè®ºæ–‡](é“¾æ¥)
- [Qwen2.5æ¨¡å‹](https://github.com/QwenLM/Qwen2.5)
- [Pai-Megatron-Patch](https://github.com/alibaba/Pai-Megatron-Patch)

## ğŸ¤ è´¡çŒ®

å¦‚éœ€æ·»åŠ æ–°çš„æ³¨æ„åŠ›æœºåˆ¶æˆ–ä½ç½®ç¼–ç æ–¹æ³•ï¼š

1. åœ¨ç›¸åº”ç›®å½•ä¸‹å®ç°æ–°çš„ç±»
2. æ›´æ–°é…ç½®ç”Ÿæˆè„šæœ¬
3. æ·»åŠ å¯¹åº”çš„åŸºå‡†æµ‹è¯•é…ç½®
4. æ›´æ–°æ–‡æ¡£

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªApache 2.0è®¸å¯è¯ã€‚