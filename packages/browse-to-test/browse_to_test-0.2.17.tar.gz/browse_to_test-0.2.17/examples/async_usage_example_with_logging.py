#!/usr/bin/env python3
"""
Async Usage Example for Browse-to-Test - WITH ENHANCED LOGGING

This example demonstrates how to use the async API with detailed logging
to track performance and identify bottlenecks.
"""

import asyncio
import time
import os
import logging
from pathlib import Path
import browse_to_test as btt
from typing import List, Dict, Any

from dotenv import load_dotenv

# Configure detailed logging BEFORE any imports
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    datefmt='%H:%M:%S'
)

# Enable detailed logging for all browse_to_test modules
logging.getLogger('browse_to_test').setLevel(logging.INFO)

load_dotenv()

# Create output directory
OUTPUT_DIR = Path(__file__).parent / "output"
OUTPUT_DIR.mkdir(exist_ok=True)


def save_generated_script(script_content: str, filename: str, data_source: str = "") -> str:
    """Save generated script to output directory with metadata."""
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    extension = ".py"  # Default to Python
    
    full_filename = f"{filename}_{timestamp}{extension}"
    file_path = OUTPUT_DIR / full_filename
    
    metadata_header = f"""# Generated by Browse-to-Test Async Example with Enhanced Logging
# Timestamp: {time.strftime("%Y-%m-%d %H:%M:%S")}
# Data Source: {data_source or "Unknown"}
# Script Length: {len(script_content)} characters
# =====================================

"""
    
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(metadata_header + script_content)
    
    return str(file_path)


# Fixed automation data with proper Browse-to-Test format
REAL_AUTOMATION_STEPS = [
    {
        "model_output": {
            "thinking": 'Starting by navigating to the target homepage to begin verification process.',
            "action": [{"go_to_url": {"url": "https://debugg.ai"}}]
        },
        "result": [
            {
                "is_done": False,
                "success": True,
                "error": None,
                "long_term_memory": 'Successfully navigated to homepage and found Sandbox header text visible.'
            }
        ],
        "state": {
            "url": "https://debugg.ai",
            "title": "Debugg AI - AI-Powered Testing Platform",
            "interacted_element": []
        },
        "metadata": {
            "step_start_time": 1753997156.1953292,
            "step_end_time": 1753997203.220958,
            "step_number": 1,
        },
    },
    {
        "model_output": {
            "thinking": "Now I need to locate and click on the main navigation or header element.",
            "action": [{"click_element": {"index": 0}}]
        },
        "result": [
            {
                "is_done": False,
                "success": True,
                "error": None,
                "long_term_memory": "Clicked on header element to explore the page structure."
            }
        ],
        "state": {
            "url": "https://debugg.ai",
            "title": "Debugg AI - AI-Powered Testing Platform",
            "interacted_element": [{
                "xpath": "//header//h1",
                "css_selector": "header h1",
                "text_content": "Debugg AI",
                "attributes": {
                    "class": "text-2xl font-bold text-gray-900"
                }
            }]
        },
        "metadata": {
            "step_start_time": 1753997350.8411188,
            "step_end_time": 1753997369.5740314,
            "step_number": 2,
        },
    },
    {
        "model_output": {
            "thinking": "Let me wait a moment for any dynamic content to load completely.",
            "action": [{"wait": {"seconds": 2}}]
        },
        "result": [
            {
                "is_done": False,
                "success": True,
                "error": None,
                "long_term_memory": "Waited for page to fully load before proceeding."
            }
        ],
        "state": {
            "url": "https://debugg.ai",
            "title": "Debugg AI - AI-Powered Testing Platform",
            "interacted_element": []
        },
        "metadata": {
            "step_start_time": 1753997372.2532299,
            "step_end_time": 1753997391.3151274,
            "step_number": 3,
        },
    },
    {
        "model_output": {
            "thinking": "Let me scroll down to explore more content on the page.",
            "action": [{"scroll": {"direction": "down", "amount": 500}}]
        },
        "result": [
            {
                "is_done": False,
                "success": True,
                "error": None,
                "long_term_memory": "Scrolled down the page to view additional content."
            }
        ],
        "state": {
            "url": "https://debugg.ai",
            "title": "Debugg AI - AI-Powered Testing Platform",
            "interacted_element": []
        },
        "metadata": {
            "step_start_time": 1753997394.1183739,
            "step_end_time": 1753997414.787713,
            "step_number": 4,
        },
    },
    {
        "model_output": {
            "thinking": "Task completed successfully. I have explored the website structure and interactions.",
            "action": [{"done": {}}]
        },
        "result": [
            {
                "is_done": True,
                "success": True,
                "error": None,
                "long_term_memory": "Successfully completed website exploration and interaction testing."
            }
        ],
        "state": {
            "url": "https://debugg.ai",
            "title": "Debugg AI - AI-Powered Testing Platform",
            "interacted_element": []
        },
        "metadata": {
            "step_start_time": 1753997419.0800045,
            "step_end_time": 1753997442.0409794,
            "step_number": 5,
        },
    }
]

async def incremental_session_example():
    """
    Example: Async Incremental Session with enhanced logging and FORCED AI analysis
    """
    print("=== Async Incremental Session Example with Enhanced Logging ===")
    print(f"üìä Detailed logging enabled for performance analysis")

    # Choose and validate data
    automation_data = REAL_AUTOMATION_STEPS
    data_source = "REAL_AUTOMATION_STEPS"
    
    print(f"Using {data_source} ({len(automation_data)} steps)")

    # FORCE AI analysis to be enabled regardless of API key
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("‚ö†Ô∏è  WARNING: No OPENAI_API_KEY found, but AI analysis will be FORCED ON for logging demo.")
        print("    This will likely cause errors, but you'll see where they occur!")
    else:
        print(f"‚úÖ OpenAI API key found. AI analysis enabled.")

    # Create config with AI FORCED ON
    config = (
        btt.ConfigBuilder()
        .framework("playwright")
        .ai_provider("openai", model="gpt-4.1-mini")  # Use supported model
        .language("python")
        .enable_ai_analysis(True)  # FORCE AI analysis ON
        .debug(True)  # Enable debug mode to see AI provider issues
        .build()
    )
    
    # Debug: Print config state
    print(f"üîß Config debug - AI analysis enabled: {config.processing.analyze_actions_with_ai}")
    print(f"üîß Config debug - AI provider: {config.ai.provider}")
    print(f"üîß Config debug - AI model: {config.ai.model}")

    session = btt.AsyncIncrementalSession(config)
    
    overall_start_time = time.time()
    print(f"\n‚è∞ {time.strftime('%H:%M:%S')} - Starting overall session timer")

    try:
        # Start the session with timeout
        print(f"‚è∞ {time.strftime('%H:%M:%S')} - Starting session...")
        result = await asyncio.wait_for(
            session.start(target_url="https://debugg.ai"),
            timeout=30.0
        )
        print(f"‚úì Session started: {result.success}")

        if not result.success:
            print(f"‚úó Failed to start session: {result.validation_issues}")
            return

        # Queue multiple steps without waiting for completion
        task_ids = []
        successful_steps = 0
        
        for i, step_data in enumerate(automation_data):
            try:
                print(f"‚è∞ {time.strftime('%H:%M:%S')} - Queueing step {i + 1}...")
                result = await asyncio.wait_for(
                    session.add_step_async(step_data, wait_for_completion=False),
                    timeout=30.0
                )
                
                if result.success and "task_id" in result.metadata:
                    task_ids.append(result.metadata["task_id"])
                    successful_steps += 1
                    print(f"  ‚úì Step {i + 1} queued with task ID: {result.metadata['task_id']}")
                else:
                    print(f"  ‚úó Step {i + 1} failed to queue: {result.validation_issues}")
            
            except asyncio.TimeoutError:
                print(f"  ‚úó Step {i + 1} timed out while queueing")
            except Exception as e:
                print(f"  ‚úó Step {i + 1} failed: {e}")

        print(f"\n‚úì Successfully queued {successful_steps}/{len(automation_data)} steps")

        # Monitor processing
        print(f"\n‚è∞ {time.strftime('%H:%M:%S')} - Monitoring queue while steps are processing...")
        for i in range(10):  # Monitor for up to 10 seconds
            await asyncio.sleep(1)
            try:
                stats = session.get_queue_stats()
                print(f"  üìä Queue stats: {stats['pending_tasks']} pending, {stats['total_tasks']} total")
                
                if stats['pending_tasks'] == 0:
                    print("  üéâ All tasks completed early!")
                    break
            except Exception as e:
                print(f"  ‚ö†Ô∏è  Warning: Could not get queue stats: {e}")

        # Wait for all tasks to complete with timeout
        print(f"\n‚è∞ {time.strftime('%H:%M:%S')} - Waiting for all tasks to complete...")
        try:
            final_result = await asyncio.wait_for(
                session.wait_for_all_tasks(timeout=180),  # 3 minute timeout for tasks
                timeout=200  # Extra buffer for the wait itself
            )
            
            overall_end_time = time.time()
            total_duration = overall_end_time - overall_start_time

            if final_result.success:
                print(f"‚úÖ All steps completed in {total_duration:.2f}s! Final script has {len(final_result.current_script)} characters")
                
                # Save the generated script to output directory
                try:
                    saved_path = save_generated_script(
                        final_result.current_script,
                        "async_with_logging",
                        data_source
                    )
                    print(f"‚úì Script saved to: {saved_path}")
                except Exception as e:
                    print(f"‚ö†Ô∏è  Warning: Could not save script: {e}")
                
                print("‚úì Final script preview:")
                preview = (
                    final_result.current_script[:400] + "..."
                    if len(final_result.current_script) > 400
                    else final_result.current_script
                )
                print(preview)
                
                # Demonstrate optional script quality analysis
                print(f"\\nüéØ Optional: Demonstrate script quality analysis...")
                if api_key:
                    try:
                        qa_start_time = time.time()
                        print(f"‚è∞ {time.strftime('%H:%M:%S')} - Starting optional script quality analysis...")
                        
                        qa_result = await session.analyze_script_quality_async(timeout=120)
                        
                        qa_end_time = time.time()
                        qa_duration = qa_end_time - qa_start_time
                        
                        if qa_result.success:
                            print(f"‚úÖ Script quality analysis completed in {qa_duration:.2f}s!")
                            print(f"üìä Analysis results:")
                            
                            # Show improvement metrics
                            metadata = qa_result.metadata
                            if metadata.get('improvement_detected'):
                                print(f"   ‚Ä¢ Original script: {metadata['original_script_chars']:,} chars, {metadata['original_script_lines']} lines")
                                print(f"   ‚Ä¢ Analyzed script: {metadata['analyzed_script_chars']:,} chars, {metadata['analyzed_script_lines']} lines")
                                print(f"   ‚Ä¢ Improvements detected: Yes")
                                
                                # Save the analyzed/optimized script
                                try:
                                    qa_saved_path = save_generated_script(
                                        qa_result.current_script,
                                        "async_with_qa_analysis",
                                        f"{data_source}_QA"
                                    )
                                    print(f"‚úì Analyzed script saved to: {qa_saved_path}")
                                except Exception as e:
                                    print(f"‚ö†Ô∏è  Warning: Could not save analyzed script: {e}")
                            else:
                                print(f"   ‚Ä¢ No significant improvements suggested - original script quality is good")
                                
                            print(f"   ‚Ä¢ Total analysis time: {metadata.get('analysis_duration', qa_duration):.2f}s")
                        else:
                            print(f"‚ö†Ô∏è  Script quality analysis failed in {qa_duration:.2f}s: {qa_result.validation_issues}")
                    
                    except Exception as e:
                        qa_end_time = time.time()
                        qa_duration = qa_end_time - qa_start_time
                        print(f"‚ùå Script quality analysis failed after {qa_duration:.2f}s: {e}")
                else:
                    print("‚ö†Ô∏è  API key required for script quality analysis - skipping")
                    print("   Set OPENAI_API_KEY to enable comprehensive script analysis and grading")
            else:
                print(f"‚úó Session failed after {total_duration:.2f}s: {final_result.validation_issues}")

        except asyncio.TimeoutError:
            overall_end_time = time.time()
            total_duration = overall_end_time - overall_start_time
            print(f"‚úó Tasks timed out after {total_duration:.2f}s during processing")
        
        # Always try to finalize the session
        try:
            await asyncio.wait_for(session.finalize_async(), timeout=30.0)
            print("‚úì Session finalized")
        except Exception as e:
            print(f"Warning: Session finalization failed: {e}")

    except asyncio.TimeoutError:
        overall_end_time = time.time()
        total_duration = overall_end_time - overall_start_time
        print(f"‚úó Session startup timed out after {total_duration:.2f}s")
    except Exception as e:
        overall_end_time = time.time()
        total_duration = overall_end_time - overall_start_time
        print(f"‚úó Session failed after {total_duration:.2f}s: {e}")
        
    print()


async def main():
    """Main function to run the enhanced logging example."""
    print("üöÄ Browse-to-Test Async Usage Example - WITH ENHANCED LOGGING")
    print("=" * 70)
    print(f"üìä Detailed logging enabled to track API calls and performance")
    print(f"üóÇÔ∏è  Output Directory: {OUTPUT_DIR.absolute()}")
    print()

    # Check OpenAI API key status
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("‚ö†Ô∏è  WARNING: OPENAI_API_KEY not found in environment variables.")
        print("    AI analysis will be FORCED ON anyway to demonstrate logging.")
        print("    You will see errors, but also the detailed timing of where they occur!")
        print()
    else:
        print("‚úÖ OpenAI API key found - AI analysis will work normally.")
        print()

    try:
        # Run the enhanced example
        await incremental_session_example()

        # Show output directory contents
        try:
            output_files = list(OUTPUT_DIR.glob("*.py"))
            if output_files:
                print(f"üìÅ Generated files in {OUTPUT_DIR.relative_to(Path.cwd())}:")
                for file_path in sorted(output_files, key=lambda x: x.stat().st_mtime, reverse=True)[:5]:
                    file_size = file_path.stat().st_size
                    print(f"   ‚Ä¢ {file_path.name} ({file_size:,} bytes)")
            else:
                print(f"üìÅ No output files generated in {OUTPUT_DIR.relative_to(Path.cwd())}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not list output files: {e}")

        print("\nüèÅ Enhanced logging example completed!")

    except KeyboardInterrupt:
        print("\nüõë Example interrupted by user")
    except Exception as e:
        print(f"üí• Example failed with unexpected error: {e}")
        print("    Check the detailed logs above for timing information.")


if __name__ == "__main__":
    asyncio.run(main())