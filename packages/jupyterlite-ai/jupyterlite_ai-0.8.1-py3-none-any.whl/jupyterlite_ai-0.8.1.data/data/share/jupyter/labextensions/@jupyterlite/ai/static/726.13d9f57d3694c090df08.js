"use strict";(self.webpackChunk_jupyterlite_ai=self.webpackChunk_jupyterlite_ai||[]).push([[726,32,470,859,89],{59905:(e,t,a)=>{a.d(t,{O:()=>o});var n=a(146),s=a(5292),r=a(44112),i=a(26877);class o extends n.a7{static lc_name(){return"ChatWebLLM"}constructor(e){super(e),Object.defineProperty(this,"engine",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"appConfig",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"chatOptions",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"temperature",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"model",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),this.appConfig=e.appConfig,this.chatOptions=e.chatOptions,this.model=e.model,this.temperature=e.temperature,this.engine=new i.MLCEngine({appConfig:this.appConfig})}_llmType(){return"web-llm"}async initialize(e){void 0!==e&&this.engine.setInitProgressCallback(e),await this.reload(this.model,this.chatOptions)}async reload(e,t){await this.engine.reload(e,t)}async*_streamResponseChunks(e,t,a){const n=e.map((e=>{if("string"!=typeof e.content)throw new Error("ChatWebLLM does not support non-string message content in sessions.");const t=e._getType();let a;if("ai"===t)a="assistant";else if("human"===t)a="user";else{if("system"!==t)throw new Error("Function, tool, and generic messages are not supported.");a="system"}return{role:a,content:e.content}})),i=await this.engine.chat.completions.create({stream:!0,messages:n,stop:t.stop,logprobs:!0});for await(const e of i){const t=e.choices[0].delta.content??"";yield new r.Cf({text:t,message:new s.H({content:t,additional_kwargs:{logprobs:e.choices[0].logprobs,finish_reason:e.choices[0].finish_reason}})}),await(a?.handleLLMNewToken(t))}}async _call(e,t,a){const n=[];for await(const s of this._streamResponseChunks(e,t,a))n.push(s.text);return n.join("")}}},43780:(e,t,a)=>{a.d(t,{L:()=>l});var n,s,r=a(44112),i=a(50630),o=a(59470);!function(e){e[e.system=0]="system",e[e.user=1]="user",e[e.assistant=2]="assistant"}(n||(n={})),function(e){e[e.user=0]="user",e[e.assistant=1]="assistant"}(s||(s={}));class l extends o.I{static lc_name(){return"ChromeAI"}constructor(e){super({...e}),Object.defineProperty(this,"temperature",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"topK",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"systemPrompt",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),this.temperature=e?.temperature??this.temperature,this.topK=e?.topK??this.topK,this.systemPrompt=e?.systemPrompt}_llmType(){return"chrome_ai"}async createSession(){let e;try{e=LanguageModel}catch(e){throw new Error(`Could not initialize ChromeAI instance. Make sure you are running a version of Chrome with the proper experimental flags enabled.\n\nError message: ${e.message}`)}const t=await e.availability();if("no"===t)throw new Error("The AI model is not available.");if("after-download"===t)throw new Error("The AI model is not yet downloaded.");return await e.create({systemPrompt:this.systemPrompt,topK:this.topK,temperature:this.temperature})}async*_streamResponseChunks(e,t,a){let n;try{n=await this.createSession();const t=n.promptStreaming(e),s=i.bO.fromReadableStream(t);let o="";for await(const e of s){const t=e;o+=t,yield new r.mu({text:t}),await(a?.handleLLMNewToken(t))}}finally{n?.destroy()}}async _call(e,t,a){const n=[];for await(const s of this._streamResponseChunks(e,t,a))n.push(s.text);return n.join("")}}},59470:(e,t,a)=>{a.d(t,{P:()=>c,I:()=>u});var n=a(5779),s=a(4645),r=a(73190),i=a(4533),o=a(78145),l=a(65695);class c extends i.j_{constructor({concurrency:e,...t}){super(e?{maxConcurrency:e,...t}:t),Object.defineProperty(this,"lc_namespace",{enumerable:!0,configurable:!0,writable:!0,value:["langchain","llms",this._llmType()]})}async invoke(e,t){const a=c._convertInputToPromptValue(e);return(await this.generatePrompt([a],t,t?.callbacks)).generations[0][0].text}async*_streamResponseChunks(e,t,a){throw new Error("Not implemented.")}_separateRunnableConfigFromCallOptionsCompat(e){const[t,a]=super._separateRunnableConfigFromCallOptions(e);return a.signal=t.signal,[t,a]}async*_streamIterator(e,t){if(this._streamResponseChunks===c.prototype._streamResponseChunks)yield this.invoke(e,t);else{const a=c._convertInputToPromptValue(e),[n,i]=this._separateRunnableConfigFromCallOptionsCompat(t),o=await r.Td.configure(n.callbacks,this.callbacks,n.tags,this.tags,n.metadata,this.metadata,{verbose:this.verbose}),l={options:i,invocation_params:this?.invocationParams(i),batch_size:1},u=await(o?.handleLLMStart(this.toJSON(),[a.toString()],n.runId,void 0,l,void 0,void 0,n.runName));let p=new s.mu({text:""});try{for await(const e of this._streamResponseChunks(a.toString(),i,u?.[0]))p=p?p.concat(e):e,"string"==typeof e.text&&(yield e.text)}catch(e){throw await Promise.all((u??[]).map((t=>t?.handleLLMError(e)))),e}await Promise.all((u??[]).map((e=>e?.handleLLMEnd({generations:[[p]]}))))}}async generatePrompt(e,t,a){const n=e.map((e=>e.toString()));return this.generate(n,t,a)}invocationParams(e){return{}}_flattenLLMResult(e){const t=[];for(let a=0;a<e.generations.length;a+=1){const n=e.generations[a];if(0===a)t.push({generations:[n],llmOutput:e.llmOutput});else{const a=e.llmOutput?{...e.llmOutput,tokenUsage:{}}:void 0;t.push({generations:[n],llmOutput:a})}}return t}async _generateUncached(e,t,a,n){let i,u;if(void 0!==n&&n.length===e.length)i=n;else{const n=await r.Td.configure(a.callbacks,this.callbacks,a.tags,this.tags,a.metadata,this.metadata,{verbose:this.verbose}),s={options:t,invocation_params:this?.invocationParams(t),batch_size:e.length};i=await(n?.handleLLMStart(this.toJSON(),e,a.runId,void 0,s,void 0,void 0,a?.runName))}if(i?.[0].handlers.find(l.xL)&&1===e.length&&this._streamResponseChunks!==c.prototype._streamResponseChunks)try{const a=await this._streamResponseChunks(e[0],t,i?.[0]);let n;for await(const e of a)n=void 0===n?e:(0,o.xW)(n,e);if(void 0===n)throw new Error("Received empty response from chat model call.");u={generations:[[n]],llmOutput:{}},await(i?.[0].handleLLMEnd(u))}catch(e){throw await(i?.[0].handleLLMError(e)),e}else{try{u=await this._generate(e,t,i?.[0])}catch(e){throw await Promise.all((i??[]).map((t=>t?.handleLLMError(e)))),e}const a=this._flattenLLMResult(u);await Promise.all((i??[]).map(((e,t)=>e?.handleLLMEnd(a[t]))))}const p=i?.map((e=>e.runId))||void 0;return Object.defineProperty(u,s.SP,{value:p?{runIds:p}:void 0,configurable:!0}),u}async _generateCached({prompts:e,cache:t,llmStringKey:a,parsedOptions:n,handledOptions:i,runId:o}){const l=await r.Td.configure(i.callbacks,this.callbacks,i.tags,this.tags,i.metadata,this.metadata,{verbose:this.verbose}),c={options:n,invocation_params:this?.invocationParams(n),batch_size:e.length},u=await(l?.handleLLMStart(this.toJSON(),e,o,void 0,c,void 0,void 0,i?.runName)),p=[],h=(await Promise.allSettled(e.map((async(e,n)=>{const s=await t.lookup(e,a);return null==s&&p.push(n),s})))).map(((e,t)=>({result:e,runManager:u?.[t]}))).filter((({result:e})=>"fulfilled"===e.status&&null!=e.value||"rejected"===e.status)),m=[];await Promise.all(h.map((async({result:e,runManager:t},a)=>{if("fulfilled"===e.status){const n=e.value;return m[a]=n.map((e=>(e.generationInfo={...e.generationInfo,tokenUsage:{}},e))),n.length&&await(t?.handleLLMNewToken(n[0].text)),t?.handleLLMEnd({generations:[n]},void 0,void 0,void 0,{cached:!0})}return await(t?.handleLLMError(e.reason,void 0,void 0,void 0,{cached:!0})),Promise.reject(e.reason)})));const d={generations:m,missingPromptIndices:p,startedRunManagers:u};return Object.defineProperty(d,s.SP,{value:u?{runIds:u?.map((e=>e.runId))}:void 0,configurable:!0}),d}async generate(e,t,a){if(!Array.isArray(e))throw new Error("Argument 'prompts' is expected to be a string[]");let n;n=Array.isArray(t)?{stop:t}:t;const[s,r]=this._separateRunnableConfigFromCallOptionsCompat(n);if(s.callbacks=s.callbacks??a,!this.cache)return this._generateUncached(e,r,s);const{cache:i}=this,o=this._getSerializedCacheKeyParametersForCall(r),{generations:l,missingPromptIndices:c,startedRunManagers:u}=await this._generateCached({prompts:e,cache:i,llmStringKey:o,parsedOptions:r,handledOptions:s,runId:s.runId});let p={};if(c.length>0){const t=await this._generateUncached(c.map((t=>e[t])),r,s,void 0!==u?c.map((e=>u?.[e])):void 0);await Promise.all(t.generations.map((async(t,a)=>{const n=c[a];return l[n]=t,i.update(e[n],o,t)}))),p=t.llmOutput??{}}return{generations:l,llmOutput:p}}async call(e,t,a){const{generations:n}=await this.generate([e],t,a);return n[0][0].text}async predict(e,t,a){return this.call(e,t,a)}async predictMessages(e,t,a){const s=(0,n.Sw)(e),r=await this.call(s,t,a);return new n.Od(r)}_identifyingParams(){return{}}serialize(){return{...this._identifyingParams(),_type:this._llmType(),_model:this._modelType()}}_modelType(){return"base_llm"}}class u extends c{async _generate(e,t,a){return{generations:await Promise.all(e.map(((e,n)=>this._call(e,{...t,promptIndex:n},a).then((e=>[{text:e}])))))}}}},50630:(e,t,a)=>{a.d(t,{bO:()=>n.bO,xW:()=>n.xW});var n=a(78145)},59257:(e,t,a)=>{var n=a(95709);t.A=void 0;var s=n(a(42032)),r=a(74848);t.A=(0,s.default)((0,r.jsx)("path",{d:"M6 6h12v12H6z"}),"Stop")},42032:(e,t,a)=>{Object.defineProperty(t,"__esModule",{value:!0}),Object.defineProperty(t,"default",{enumerable:!0,get:function(){return n.createSvgIcon}});var n=a(83704)}}]);