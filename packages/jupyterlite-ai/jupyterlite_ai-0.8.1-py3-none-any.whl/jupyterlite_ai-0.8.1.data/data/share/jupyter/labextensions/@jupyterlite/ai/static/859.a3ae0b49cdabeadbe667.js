"use strict";(self.webpackChunk_jupyterlite_ai=self.webpackChunk_jupyterlite_ai||[]).push([[859,470,89],{59470:(t,e,a)=>{a.d(e,{P:()=>c,I:()=>u});var n=a(5779),s=a(4645),r=a(73190),i=a(4533),o=a(78145),l=a(65695);class c extends i.j_{constructor({concurrency:t,...e}){super(t?{maxConcurrency:t,...e}:e),Object.defineProperty(this,"lc_namespace",{enumerable:!0,configurable:!0,writable:!0,value:["langchain","llms",this._llmType()]})}async invoke(t,e){const a=c._convertInputToPromptValue(t);return(await this.generatePrompt([a],e,e?.callbacks)).generations[0][0].text}async*_streamResponseChunks(t,e,a){throw new Error("Not implemented.")}_separateRunnableConfigFromCallOptionsCompat(t){const[e,a]=super._separateRunnableConfigFromCallOptions(t);return a.signal=e.signal,[e,a]}async*_streamIterator(t,e){if(this._streamResponseChunks===c.prototype._streamResponseChunks)yield this.invoke(t,e);else{const a=c._convertInputToPromptValue(t),[n,i]=this._separateRunnableConfigFromCallOptionsCompat(e),o=await r.Td.configure(n.callbacks,this.callbacks,n.tags,this.tags,n.metadata,this.metadata,{verbose:this.verbose}),l={options:i,invocation_params:this?.invocationParams(i),batch_size:1},u=await(o?.handleLLMStart(this.toJSON(),[a.toString()],n.runId,void 0,l,void 0,void 0,n.runName));let h=new s.mu({text:""});try{for await(const t of this._streamResponseChunks(a.toString(),i,u?.[0]))h=h?h.concat(t):t,"string"==typeof t.text&&(yield t.text)}catch(t){throw await Promise.all((u??[]).map((e=>e?.handleLLMError(t)))),t}await Promise.all((u??[]).map((t=>t?.handleLLMEnd({generations:[[h]]}))))}}async generatePrompt(t,e,a){const n=t.map((t=>t.toString()));return this.generate(n,e,a)}invocationParams(t){return{}}_flattenLLMResult(t){const e=[];for(let a=0;a<t.generations.length;a+=1){const n=t.generations[a];if(0===a)e.push({generations:[n],llmOutput:t.llmOutput});else{const a=t.llmOutput?{...t.llmOutput,tokenUsage:{}}:void 0;e.push({generations:[n],llmOutput:a})}}return e}async _generateUncached(t,e,a,n){let i,u;if(void 0!==n&&n.length===t.length)i=n;else{const n=await r.Td.configure(a.callbacks,this.callbacks,a.tags,this.tags,a.metadata,this.metadata,{verbose:this.verbose}),s={options:e,invocation_params:this?.invocationParams(e),batch_size:t.length};i=await(n?.handleLLMStart(this.toJSON(),t,a.runId,void 0,s,void 0,void 0,a?.runName))}if(i?.[0].handlers.find(l.xL)&&1===t.length&&this._streamResponseChunks!==c.prototype._streamResponseChunks)try{const a=await this._streamResponseChunks(t[0],e,i?.[0]);let n;for await(const t of a)n=void 0===n?t:(0,o.xW)(n,t);if(void 0===n)throw new Error("Received empty response from chat model call.");u={generations:[[n]],llmOutput:{}},await(i?.[0].handleLLMEnd(u))}catch(t){throw await(i?.[0].handleLLMError(t)),t}else{try{u=await this._generate(t,e,i?.[0])}catch(t){throw await Promise.all((i??[]).map((e=>e?.handleLLMError(t)))),t}const a=this._flattenLLMResult(u);await Promise.all((i??[]).map(((t,e)=>t?.handleLLMEnd(a[e]))))}const h=i?.map((t=>t.runId))||void 0;return Object.defineProperty(u,s.SP,{value:h?{runIds:h}:void 0,configurable:!0}),u}async _generateCached({prompts:t,cache:e,llmStringKey:a,parsedOptions:n,handledOptions:i,runId:o}){const l=await r.Td.configure(i.callbacks,this.callbacks,i.tags,this.tags,i.metadata,this.metadata,{verbose:this.verbose}),c={options:n,invocation_params:this?.invocationParams(n),batch_size:t.length},u=await(l?.handleLLMStart(this.toJSON(),t,o,void 0,c,void 0,void 0,i?.runName)),h=[],d=(await Promise.allSettled(t.map((async(t,n)=>{const s=await e.lookup(t,a);return null==s&&h.push(n),s})))).map(((t,e)=>({result:t,runManager:u?.[e]}))).filter((({result:t})=>"fulfilled"===t.status&&null!=t.value||"rejected"===t.status)),p=[];await Promise.all(d.map((async({result:t,runManager:e},a)=>{if("fulfilled"===t.status){const n=t.value;return p[a]=n.map((t=>(t.generationInfo={...t.generationInfo,tokenUsage:{}},t))),n.length&&await(e?.handleLLMNewToken(n[0].text)),e?.handleLLMEnd({generations:[n]},void 0,void 0,void 0,{cached:!0})}return await(e?.handleLLMError(t.reason,void 0,void 0,void 0,{cached:!0})),Promise.reject(t.reason)})));const m={generations:p,missingPromptIndices:h,startedRunManagers:u};return Object.defineProperty(m,s.SP,{value:u?{runIds:u?.map((t=>t.runId))}:void 0,configurable:!0}),m}async generate(t,e,a){if(!Array.isArray(t))throw new Error("Argument 'prompts' is expected to be a string[]");let n;n=Array.isArray(e)?{stop:e}:e;const[s,r]=this._separateRunnableConfigFromCallOptionsCompat(n);if(s.callbacks=s.callbacks??a,!this.cache)return this._generateUncached(t,r,s);const{cache:i}=this,o=this._getSerializedCacheKeyParametersForCall(r),{generations:l,missingPromptIndices:c,startedRunManagers:u}=await this._generateCached({prompts:t,cache:i,llmStringKey:o,parsedOptions:r,handledOptions:s,runId:s.runId});let h={};if(c.length>0){const e=await this._generateUncached(c.map((e=>t[e])),r,s,void 0!==u?c.map((t=>u?.[t])):void 0);await Promise.all(e.generations.map((async(e,a)=>{const n=c[a];return l[n]=e,i.update(t[n],o,e)}))),h=e.llmOutput??{}}return{generations:l,llmOutput:h}}async call(t,e,a){const{generations:n}=await this.generate([t],e,a);return n[0][0].text}async predict(t,e,a){return this.call(t,e,a)}async predictMessages(t,e,a){const s=(0,n.Sw)(t),r=await this.call(s,e,a);return new n.Od(r)}_identifyingParams(){return{}}serialize(){return{...this._identifyingParams(),_type:this._llmType(),_model:this._modelType()}}_modelType(){return"base_llm"}}class u extends c{async _generate(t,e,a){return{generations:await Promise.all(t.map(((t,n)=>this._call(t,{...e,promptIndex:n},a).then((t=>[{text:t}])))))}}}},50630:(t,e,a)=>{a.d(e,{bO:()=>n.bO,xW:()=>n.xW});var n=a(78145)}}]);