{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SynDiffix Usage Tutorial\n",
    "\n",
    "This notebook demonstrates how to use __SynDiffix__, an open-source library for generating statistically-accurate\n",
    "and strongly anonymous synthetic data from structured data.\n",
    "\n",
    "We'll go through the process of loading and inspecting a dataset, creating synthetic datasets that mimics the original, and computing some statistical properties over the two datasets and comparing them.\n",
    "\n",
    "### Setup\n",
    "\n",
    "The `syndiffix` package requires Python 3.10 or later. Let's install it and other packages we'll need for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q syndiffix requests pandas scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "We'll use the `loan` dataset from the Czech banking dataset. A cleaned-up version is available at open-diffix.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  loan_id account_id       date  amount  duration  payments status\n",
      "0    5314       1787 1993-08-05   96396        12    8033.0      B\n",
      "1    5316       1801 1993-08-11  165960        36    4610.0      A\n",
      "2    6863       9188 1993-08-28  127080        60    2118.0      A\n",
      "3    5325       1843 1993-09-03  105804        36    2939.0      A\n",
      "4    7240      11013 1993-10-06  274740        60    4579.0      A\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bz2\n",
    "import pickle\n",
    "def download_and_load(url):\n",
    "    response = requests.get(url)\n",
    "    data = bz2.decompress(response.content)\n",
    "    df = pickle.loads(data)\n",
    "    return df\n",
    "\n",
    "# Usage\n",
    "df_loan = download_and_load('http://open-diffix.org/datasets/loan.pbz2')\n",
    "print(df_loan.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating synthetic datasets\n",
    "\n",
    "Before creating synthetic datasets, it may be necessary to identify if there is some entity in the data whose privacy must be protected. We call this the *protected entity*. The `loans` dataset has an `account_id` column. Since the account is related to individual persons, we want to ensure that the privacy of individual accounts are protected.\n",
    "\n",
    "To do this, we prepare a dataframe consisting of only the `account_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pid = df_loan[['account_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at the correlation between the `amount` attribute and the `duration` and `loan_id` (we expect strong correlation with `duration` and none with `loan_id`). To do this, we'll create two synthetic datasets of two columns each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syndiffix import Synthesizer\n",
    "\n",
    "df_amt_dur = Synthesizer(df_loan[['amount','duration']], pids=df_pid).sample()\n",
    "df_amt_lid = Synthesizer(df_loan[['amount','loan_id']], pids=df_pid).sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the Spearman rank-order correlation to measure the correlation, and compare the results for both the original and synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount <-> duration:\n",
      "Original SignificanceResult(statistic=0.6276759903171304, pvalue=5.408495176711555e-76)\n",
      "Synthetic SignificanceResult(statistic=0.6475724573499293, pvalue=4.374883861691819e-82)\n",
      "amount <-> loan_id:\n",
      "Original SignificanceResult(statistic=-0.037362151151157305, pvalue=0.32992360906471985)\n",
      "Synthetic SignificanceResult(statistic=-0.0379950086356936, pvalue=0.3221482798617139)\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "print(\"amount <-> duration:\")\n",
    "print(\"Original\",scipy.stats.spearmanr(df_loan['amount'], df_loan['duration']))\n",
    "print(\"Synthetic\",scipy.stats.spearmanr(df_amt_dur['amount'], df_amt_dur['duration']))\n",
    "print(\"amount <-> loan_id:\")\n",
    "print(\"Original\",scipy.stats.spearmanr(df_loan['amount'], df_loan['loan_id']))\n",
    "print(\"Synthetic\",scipy.stats.spearmanr(df_amt_lid['amount'], df_amt_lid['loan_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlations computed from the synthetic data are very close to those of the original data.  As expected, we see a strong correlation between loan amount and loan duration, and virtually no correlation between loan amount and the loan id.\n",
    "\n",
    "### A simpler (but less accurate) approach\n",
    "\n",
    "Having to create a separate synthetic dataset for each column pair is inconvenient. It would be easier to create one synthetic data containing all of the columns. This is how other synthetic data products work. Let's try that and look at the resulting correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount <-> duration:\n",
      "Original SignificanceResult(statistic=0.6276759903171304, pvalue=5.408495176711555e-76)\n",
      "Synthetic (2-col) SignificanceResult(statistic=0.6475724573499293, pvalue=4.374883861691819e-82)\n",
      "Synthetic (all) SignificanceResult(statistic=0.6524030556117626, pvalue=2.5062793995535786e-83)\n",
      "amount <-> loan_id:\n",
      "Original SignificanceResult(statistic=-0.037362151151157305, pvalue=0.32992360906471985)\n",
      "Synthetic (2-col) SignificanceResult(statistic=-0.0379950086356936, pvalue=0.3221482798617139)\n",
      "Synthetic (all) SignificanceResult(statistic=-0.0028815796809600258, pvalue=0.9403435522830113)\n"
     ]
    }
   ],
   "source": [
    "df_loan_syn = Synthesizer(df_loan, pids=df_pid).sample()\n",
    "\n",
    "print(\"amount <-> duration:\")\n",
    "print(\"Original\",scipy.stats.spearmanr(df_loan['amount'], df_loan['duration']))\n",
    "print(\"Synthetic (2-col)\",scipy.stats.spearmanr(df_amt_dur['amount'], df_amt_dur['duration']))\n",
    "print(\"Synthetic (all)\",scipy.stats.spearmanr(df_loan_syn['amount'], df_loan_syn['duration']))\n",
    "print(\"amount <-> loan_id:\")\n",
    "print(\"Original\",scipy.stats.spearmanr(df_loan['amount'], df_loan['loan_id']))\n",
    "print(\"Synthetic (2-col)\",scipy.stats.spearmanr(df_amt_lid['amount'], df_amt_lid['loan_id']))\n",
    "print(\"Synthetic (all)\",scipy.stats.spearmanr(df_loan_syn['amount'], df_loan_syn['loan_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the Spearman measures are only slightly less accurate when all columns are synthesized. This is the case here because there are relatively few columns in this dataset. As a rule, the more columns, the lower the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "\n",
    "Now we give an example of using **SynDiffix** to build an ML model. Here we want to build a model that predicts the `duration` of a loan. \n",
    "\n",
    "Here are the possible values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Durations (months): [12 36 60 24 48]\n"
     ]
    }
   ],
   "source": [
    "print(\"Load Durations (months):\", df_loan['duration'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use the `sklearn` DecisionTreeClassifier. Let's prepare the dataset for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Let's drop 'loan_id' because we know it is of no predictive value\n",
    "df = df_loan.drop(columns=[\"loan_id\"])\n",
    "# Change date to a float because DecisionTreeClassifier requires it\n",
    "df['date'] = df['date'].astype('int64') / 10**9\n",
    "# Make the PID dataframe (we did this before, but put it here again for completeness)\n",
    "df_pid = df[['account_id']]\n",
    "# Drop the PID from the dataset because it also has no predictive value\n",
    "df = df.drop(columns=[\"account_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the synthetic data. Setting `target_column` improves the quality of the synthetic data with respect to building an ML model for the target. If we were to build another ML model for another target, we'd make a new synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'duration'\n",
    "df_syn = Synthesizer(df, pids=df_pid, target_column=target).sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to build models from both the original and synthetic data so that we can compare the results. We need to run all of the modeling preperation steps on both the original and synthetic data.\n",
    "\n",
    "Note that there is a one-hot encoding step here (`pd.get_dummies()`). It is import to synthesize the data **before** one-hot encoding rather than after, especially if there are a lot of values to be encoded. This is because the quality of the synthetic data decreases with an increase in the number of columns.\n",
    "\n",
    "Note also that in order to test the quality of the synthetic data model, we must take the test data from the original data, not from the synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and the target variable (y)\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "X_syn = df_syn.drop(target, axis=1)\n",
    "y_syn = df_syn[target]\n",
    "\n",
    "# And we need to convert strings to one-hot encoding\n",
    "# (Important to do this after synthesis, not before)\n",
    "X = pd.get_dummies(X)\n",
    "X_syn = pd.get_dummies(X_syn)\n",
    "\n",
    "# Split the original dataset into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Do the same for the synthetic data, but noting that we'll use the original test set for testing both original and synthetic\n",
    "X_train_syn, _, y_train_syn, _ = train_test_split(X_syn, y_syn, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an run the models, and display prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Original is 0.8536585365853658\n",
      "Accuracy Synthetic Target is 0.7853658536585366\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def runModel(X_train, X_test, y_train, y_test, dataSource):\n",
    "    # Create a decision tree classifier and fit it to the training data\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Use the trained classifier to make predictions on the test data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy {dataSource} is {accuracy}\")\n",
    "\n",
    "runModel(X_train, X_test, y_train, y_test, \"Original\")\n",
    "runModel(X_train_syn, X_test, y_train_syn, y_test, \"Synthetic Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of the synthetic data model is almost 10% below that of the original data model. This is not horrible, but certainly we'd like to do better. We have some ideas in mind as to how to improve this, but may not get to them for a while. If you are interested in contributing, do let us know!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
