# ğŸ“‚ validation_tests

## ğŸ§  What This Module Does

- Implements statistical validation methods for evaluating the robustness and significance of mined rules.
- Provides multiple testing frameworks (train/test, bootstrap, null) for edge persistence and overfitting control.
- Central to determining whether discovered rules are real, stable, and out-of-sample viable.

## ğŸ§° Main Features

- âœ… **Train/Test Pipeline**: Random splits with independent preprocessing after the split.
- âœ… **Walk Forward Analysis (WFA)**: Expanding window validation for realistic temporal evaluation.
- âœ… **Bootstrap Resampling**: Non-parametric sampling to assess variability and confidence intervals.
- âœ… **Null Distribution Creation**: Generates synthetic null edges for comparison.
- âœ… **False Discovery Rate (FDR) Correction**: Applies multiple testing correction to filter spurious rules.
- âœ… Full override system for all validation routines (e.g. number of splits, samples, alpha thresholds)

## ğŸš€ How to Use

Each pipeline is callable independently. Use `feature_df` and `hloc` for raw-data-based validation (e.g. train/test), or pre-mined/preprocessed data for bootstrap/null/FDR.

### ğŸ”¹ Train/Test

```python
from edge_research.validation_tests.validation import train_test_pipeline

results, log, pipeline_log = train_test_pipeline(feature_df, hloc, cfg, logger)

# With override
results, log, pipeline_log = train_test_pipeline(
    feature_df, hloc, cfg, logger,
    **{"train_test_splits": 2}
)
```

### ğŸ”¹ Walk Forward Analysis (WFA)

```python
from edge_research.validation_tests.validation import wfa_pipeline

wfa_results, wfa_log, wfa_pipeline_log = wfa_pipeline(feature_df, hloc, cfg, logger)

# Override: number of folds
wfa_results, wfa_log, wfa_pipeline_log = wfa_pipeline(
    feature_df, hloc, cfg, logger,
    **{"wfa_splits": 4}
)
```

### ğŸ”¹ Bootstrap Resampling

```python
from edge_research.validation_tests.validation import bootstrap_pipeline

bootstrap_results, bootstrap_log = bootstrap_pipeline(df, cfg, logger)

# With override
bootstrap_results, bootstrap_log = bootstrap_pipeline(
    df, cfg, logger,
    **{"n_bootstrap": 1000}
)
```

### ğŸ”¹ Null Distribution

```python
from edge_research.validation_tests.validation import null_pipeline

null_df, null_log = null_pipeline(df, cfg, logger)

# Override number of null samples
null_df, null_log = null_pipeline(df, cfg, logger, **{"n_null": 1000})
```

### ğŸ”¹ False Discovery Rate (FDR) Correction

```python
from edge_research.validation_tests.validation import fdr_pipeline

fdr_df, fdr_log = fdr_pipeline(mining_res, null_df, cfg, logger)

# Override alpha threshold
fdr_df, fdr_log = fdr_pipeline(
    mining_res, null_df, cfg, logger,
    **{"correction_alpha": 0.05}
)
```

> Note: `bootstrap_pipeline`, `null_pipeline`, and `fdr_pipeline` assume the input `df` was generated by `data_prep_pipeline()` in `rules_mining/`.
> Or custom logic that returns the same format

## âš™ï¸ Configuration Reference

All routines are config-driven via YAML or Python objects. See:

ğŸ“˜ `docs/params.md`:

* `âš™ï¸ Config: Train Test`
* `âš™ï¸ Config: WFA (Walk Forward Analysis)`
* `âš™ï¸ Config: Bootstrap`
* `âš™ï¸ Config: Null Distribution`
* `âš™ï¸ Config: Multiple Corrections`

## âš ï¸ Design Notes / Caveats

* `train_test_pipeline()` and `wfa_pipeline()` **perform their own preprocessing** post-split.
* All inputs must have a **valid datetime column** and properly aligned HLOC data for target merging.
* `bootstrap_pipeline()` and `null_pipeline()` assume **already preprocessed and encoded** data.
* `fdr_pipeline()` requires both null results and original mining results, and is sensitive to alignment (index/columns).
* Override arguments allow fast iteration without changing persistent config.

## ğŸ§ª Testing Status

* Unit tested:

  * Splitting logic
  * Bootstrap reproducibility
  * FDR correction with synthetic test cases
* Partial test coverage for:

  * Complex override interactions
  * Real-world HLOC merging edge cases
* Full test suite in `tests/validation/`

## ğŸ”— Related Modules

* `rules_mining/`: Provides mining results to be validated.
* `statistics/`: Computes rule-level stats used in evaluation metrics.
* `preprocessing/`: Required before bootstrap/null validation.
* `examples/`: See `examples/validation_example.py` for full workflows.

