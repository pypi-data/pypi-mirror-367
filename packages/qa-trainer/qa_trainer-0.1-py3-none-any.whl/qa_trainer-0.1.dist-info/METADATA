Metadata-Version: 2.4
Name: qa_trainer
Version: 0.1
Summary: Train QA models from feedback with Optuna hyperparameter tuning
Author: Your Name
Description-Content-Type: text/markdown
Requires-Dist: simpletransformers>=0.64.3
Requires-Dist: optuna
Requires-Dist: torch
Requires-Dist: transformers
Dynamic: author
Dynamic: description
Dynamic: description-content-type
Dynamic: requires-dist
Dynamic: summary

# qa_trainer

A simple Python package to fine-tune **question-answering models** using user feedback data and **Optuna** for hyperparameter optimization.

This is built on top of [SimpleTransformers](https://github.com/ThilinaRajapakse/simpletransformers) and supports any HuggingFace-compatible QA model like BERT, RoBERTa, etc.

---

## ðŸš€ Features

- Accepts feedback-based QA data (`question`, `answer`, `feedback`)
- Converts data into SQuAD format
- Uses Optuna to optimize `learning_rate`, `batch_size`, and `epochs`
- Saves the fine-tuned model automatically
- Super simple API: just call a function with 3 parameters

---

## ðŸ§  Installation

```bash
pip install qa_trainer
