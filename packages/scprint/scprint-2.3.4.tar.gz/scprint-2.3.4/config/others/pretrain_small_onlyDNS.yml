trainer:
  gradient_clip_val: 10
  #strategy: ddp
  log_every_n_steps: 100
  limit_train_batches: 5000
  limit_val_batches: 1000
  max_time:
    hours: 24
  callbacks:
    - class_path: scprint.trainer.TrainingMode
      init_args:
        do_denoise: True
        noise: [0.3]
        do_cce: True
        do_ecs: True
        do_mvc: False
        do_generate: True
        do_adv_cls: False
        do_adv_batch: True
        do_next_tp: False
        run_full_forward: False
        do_cls: True
        class_scale: 1.5
        warmup_duration: 500
        fused_adam: True
        mask_ratio: []
    - class_path: lightning.pytorch.callbacks.StochasticWeightAveraging
      init_args:
        swa_lrs: 0.01
    #- class_path: lightning.pytorch.callbacks.LearningRateFinder
    #init_args:
    #  mode: exponential
  #plugins:
  #  - class_path: lightning.pytorch.plugins.environments.SLURMEnvironment
  #    requeue_signal: signal.SIGHUP
model:
  lr: 0.002
  optim: "adamW"
  weight_decay: 0.01
  nhead: 4
  nlayers: 4
  layers_cls: [128]
  dropout: 0.1
  d_model: 128
data:
  organisms:
    - NCBITaxon:9606
    - NCBITaxon:10090
  collection_name: all no zhang13M # preprocessed dataset
  max_len: 1200
  weight_scaler: 10
  train_oversampling_per_epoch: 0.2
  validation_split: 0.05
  test_split: 0.05
  batch_size: 64
  num_workers: 16
