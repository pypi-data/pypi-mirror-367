name: CI

on:
  pull_request:
    branches: [ main ]
  push:
    branches: [ main ]

jobs:
  test:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-22.04, ubuntu-24.04, windows-2022]
        # TODO(hughhan1): Add macOS runners back when budget allows
        # macos-13, macos-14 - currently disabled due to cost
        # As of July 2025, macOS runners cost 10x more than Linux runners
        python-version: ['3.9', '3.10', '3.11', '3.12']
    
    steps:
    - uses: actions/checkout@v4
      with:
        submodules: true
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Set up Rust
      uses: dtolnay/rust-toolchain@stable
    
    - name: Cache Rust dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target
        key: ${{ runner.os }}-${{ matrix.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
    
    - name: Create virtual environment
      run: python -m venv .venv
    
    - name: Install dependencies in virtual environment
      run: |
        if [ "${{ runner.os }}" = "Windows" ]; then
          .venv\\Scripts\\pip install maturin pytest
        else
          .venv/bin/pip install maturin[patchelf] pytest
        fi
      shell: bash
    
    - name: Install rtest with maturin develop
      run: |
        if [ "${{ runner.os }}" = "Windows" ]; then
          .venv\\Scripts\\maturin develop --release
        else
          .venv/bin/maturin develop --release
        fi
      shell: bash
    
    - name: Run Rust unit tests
      run: |
        cd rtest && cargo test --lib
    
    - name: Run Rust integration tests
      run: cargo test
    
    - name: Test import
      run: |
        if [ "${{ runner.os }}" = "Windows" ]; then
          .venv\\Scripts\\python -c "import rtest; print('Import successful')"
        else
          .venv/bin/python -c "import rtest; print('Import successful')"
        fi
      shell: bash
    
    - name: Run Python integration tests
      run: |
        if [ "${{ runner.os }}" = "Windows" ]; then
          .venv\\Scripts\\pytest tests/ -vvv --log-level=debug -s --cache-clear
        else
          .venv/bin/pytest tests/ -vvv --log-level=debug -s --cache-clear
        fi
      shell: bash

  lint:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
      with:
        submodules: true
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
    
    - name: Set up Rust
      uses: dtolnay/rust-toolchain@stable
      with:
        components: rustfmt, clippy
    
    - name: Install Python dev dependencies
      run: |
        pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Run ruff format check
      run: ruff format --check python/ tests/ scripts/
    
    - name: Run ruff lint
      run: ruff check python/ tests/ scripts/
    
    - name: Run mypy
      run: mypy python/ tests/ scripts/
    
    - name: Run vulture (dead code detection)
      run: vulture python/ tests/ scripts/ --min-confidence 80
    
    - name: Check Rust formatting
      run: cargo fmt -- --check

    - name: Run clippy
      run: cargo clippy -p rtest --bin rtest -- -D warnings

  # Setup job that installs dependencies once
  benchmark-setup:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
    - uses: actions/checkout@v4
      with:
        submodules: true
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
    
    - name: Install uv
      uses: astral-sh/setup-uv@v3
    
    - name: Cache uv dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/uv
          .venv
        key: ${{ runner.os }}-uv-${{ hashFiles('**/pyproject.toml', '**/uv.lock') }}
    
    - name: Install dependencies
      run: uv sync --dev
    
    - name: Get repository list
      id: set-matrix
      run: |
        # Extract repository names from YAML
        repos=$(uv run python -c "
        import yaml
        from pathlib import Path
        config_path = Path('scripts/benchmark/repositories.yml')
        with open(config_path) as f:
            data = yaml.safe_load(f)
        repo_names = [r['name'] for r in data['repositories']]
        print(' '.join(repo_names))
        ")
        # Create matrix JSON
        matrix_json="{\"repository\": ["
        first=true
        for repo in $repos; do
          if [ "$first" = true ]; then
            first=false
          else
            matrix_json+=", "
          fi
          matrix_json+="\"$repo\""
        done
        matrix_json+="]}"
        echo "matrix=$matrix_json" >> $GITHUB_OUTPUT
        echo "Repository matrix: $matrix_json"
    
    # Upload the environment for parallel jobs
    - name: Create benchmark environment archive
      run: |
        # Archive the virtual environment and dependencies
        tar -czf benchmark-env.tar.gz .venv uv.lock pyproject.toml
    
    - name: Upload benchmark environment
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-environment
        path: benchmark-env.tar.gz
        retention-days: 1

  # Parallel benchmark jobs
  benchmark:
    needs: benchmark-setup
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.benchmark-setup.outputs.matrix) }}
    steps:
    - uses: actions/checkout@v4
      with:
        submodules: true
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
    
    - name: Install uv
      uses: astral-sh/setup-uv@v3
    
    - name: Download benchmark environment
      uses: actions/download-artifact@v4
      with:
        name: benchmark-environment
    
    - name: Extract benchmark environment
      run: |
        tar -xzf benchmark-env.tar.gz
        rm benchmark-env.tar.gz
    
    - name: Install hyperfine
      run: |
        wget https://github.com/sharkdp/hyperfine/releases/download/v1.18.0/hyperfine_1.18.0_amd64.deb
        sudo dpkg -i hyperfine_1.18.0_amd64.deb
    
    - name: Run benchmark for ${{ matrix.repository }}
      run: |
        uv run python scripts/benchmark/benchmark_repositories.py --repositories ${{ matrix.repository }} --collect-only
    
    - name: Upload results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ matrix.repository }}
        path: /tmp/rtest_benchmark_results_*/
        retention-days: 7

  # Aggregate results
  benchmark-summary:
    needs: benchmark
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
    
    - name: Download all results
      uses: actions/download-artifact@v4
      with:
        pattern: benchmark-results-*
        path: all-results
    
    - name: Aggregate results
      id: aggregate
      run: |
        # Function to output to both stdout and GitHub step summary
        output_both() {
          echo "$@"
          echo "$@" >> $GITHUB_STEP_SUMMARY
        }
        
        output_both "## Benchmark Results Summary"
        output_both ""
        
        # Aggregate all JSON files into a single array
        python3 scripts/benchmark/aggregate_results.py all-results/ aggregated-results.json | while IFS= read -r line; do
          output_both "$line"
        done
        
        # Set output for artifact name
        if [ "${{ github.event_name }}" = "push" ] && [ "${{ github.ref }}" = "refs/heads/main" ]; then
          echo "artifact_name=benchmark-baseline-main" >> $GITHUB_OUTPUT
        else
          echo "artifact_name=benchmark-results-pr-${{ github.event.pull_request.number }}" >> $GITHUB_OUTPUT
        fi
    
    # Upload baseline when on main branch
    - name: Upload baseline artifact
      if: github.event_name == 'push' && github.ref == 'refs/heads/main'
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-baseline-main
        path: aggregated-results.json
        retention-days: 90
    
    # Download and compare with baseline when on PR
    - name: Download baseline for comparison
      if: github.event_name == 'pull_request'
      uses: dawidd6/action-download-artifact@v3
      with:
        workflow: ci.yml
        branch: main
        name: benchmark-baseline-main
        path: baseline
        if_no_artifact_found: warn
    
    - name: Compare with baseline
      if: github.event_name == 'pull_request' && success()
      run: |
        # Function to output to both stdout and GitHub step summary
        output_both() {
          echo "$@"
          echo "$@" >> $GITHUB_STEP_SUMMARY
        }
        
        if [ -f baseline/aggregated-results.json ]; then
          # Run comparison and capture output
          comparison_output=$(python3 scripts/benchmark/compare_results.py baseline/aggregated-results.json aggregated-results.json 2>&1) || true
          
          # Output comparison results line by line to handle multi-line output
          while IFS= read -r line; do
            output_both "$line"
          done <<< "$comparison_output"
          
          output_both ""
          output_both "Note: Comparison only includes benchmarks that exist in both baseline and current results."
        else
          output_both "No baseline found for comparison"
        fi