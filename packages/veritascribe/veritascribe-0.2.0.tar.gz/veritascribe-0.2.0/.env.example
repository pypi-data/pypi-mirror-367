# VeritaScribe Configuration Template
# Copy this file to .env and fill in your actual values

# =============================================================================
# LLM PROVIDER CONFIGURATION
# =============================================================================

# Provider Selection: openai, openrouter, anthropic, custom
LLM_PROVIDER=openai

# =============================================================================
# OPENAI CONFIGURATION
# =============================================================================

# Required for provider 'openai' or 'custom'
OPENAI_API_KEY=your_openai_api_key_here

# Optional: Custom OpenAI-compatible endpoint (for local models, Azure, etc.)
# OPENAI_BASE_URL=https://your-custom-endpoint.com/v1
# OPENAI_BASE_URL=http://localhost:11434/v1  # For Ollama
# OPENAI_BASE_URL=https://your-org.openai.azure.com/  # For Azure OpenAI

# =============================================================================
# OPENROUTER CONFIGURATION  
# =============================================================================

# Required for provider 'openrouter'
# Get your key from: https://openrouter.ai/keys
# OPENROUTER_API_KEY=sk-or-your_openrouter_api_key_here

# =============================================================================
# ANTHROPIC CONFIGURATION
# =============================================================================

# Required for provider 'anthropic' 
# Get your key from: https://console.anthropic.com/
# ANTHROPIC_API_KEY=sk-ant-your_anthropic_api_key_here

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================

# Default model to use (provider-specific format)
# OpenAI: gpt-4, gpt-4-turbo, gpt-4o, gpt-3.5-turbo
# OpenRouter: anthropic/claude-3.5-sonnet, openai/gpt-4, meta-llama/llama-3.1-70b-instruct
# Anthropic: claude-3-5-sonnet-20241022, claude-3-opus-20240229, claude-3-haiku-20240307
# Custom: depends on your endpoint (e.g., llama3.1:8b for Ollama)
DEFAULT_MODEL=gpt-4

# LLM Request Parameters
MAX_TOKENS=2000
TEMPERATURE=0.1

# =============================================================================
# ANALYSIS FEATURE TOGGLES
# =============================================================================

# Enable/disable analysis types
GRAMMAR_ANALYSIS_ENABLED=true
CONTENT_ANALYSIS_ENABLED=true
CITATION_ANALYSIS_ENABLED=true

# Error severity thresholds (0.0 to 1.0)
HIGH_SEVERITY_THRESHOLD=0.8
MEDIUM_SEVERITY_THRESHOLD=0.5

# =============================================================================
# PROCESSING CONFIGURATION
# =============================================================================

# Text processing settings
MAX_TEXT_BLOCK_SIZE=2000
MIN_TEXT_BLOCK_SIZE=50

# Parallel processing (recommended for most providers)
PARALLEL_PROCESSING=true
MAX_CONCURRENT_REQUESTS=5

# =============================================================================
# OUTPUT CONFIGURATION
# =============================================================================

# Output settings
OUTPUT_DIRECTORY=./analysis_output
GENERATE_VISUALIZATIONS=true
SAVE_DETAILED_REPORTS=true

# =============================================================================
# RETRY CONFIGURATION
# =============================================================================

# Error handling
MAX_RETRIES=3
RETRY_DELAY=1.0

# =============================================================================
# EXAMPLE CONFIGURATIONS
# =============================================================================

# Example 1: Standard OpenAI
# LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-your-key-here
# DEFAULT_MODEL=gpt-4

# Example 2: OpenRouter with Claude
# LLM_PROVIDER=openrouter  
# OPENROUTER_API_KEY=sk-or-your-key-here
# DEFAULT_MODEL=anthropic/claude-3.5-sonnet

# Example 3: Direct Anthropic Claude
# LLM_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-your-key-here
# DEFAULT_MODEL=claude-3-5-sonnet-20241022

# Example 4: Local Ollama
# LLM_PROVIDER=custom
# OPENAI_API_KEY=ollama  # Can be any value for local models
# OPENAI_BASE_URL=http://localhost:11434/v1
# DEFAULT_MODEL=llama3.1:8b

# Example 5: Azure OpenAI
# LLM_PROVIDER=custom
# OPENAI_API_KEY=your-azure-key
# OPENAI_BASE_URL=https://your-resource.openai.azure.com/
# DEFAULT_MODEL=gpt-4  # Your Azure deployment name