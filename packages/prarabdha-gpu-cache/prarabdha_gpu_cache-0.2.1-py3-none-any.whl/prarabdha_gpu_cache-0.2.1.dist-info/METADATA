Metadata-Version: 2.4
Name: prarabdha-gpu-cache
Version: 0.2.1
Summary: Advanced Multimodal AI Cache System for GPU-optimized LLM deployments
Author-email: Prarabdha Soni <prarabdha.soni@gmail.com>
Keywords: cache,ai,vector,rag,audio,video,gpu,multimodal
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: redis>=4.0
Requires-Dist: faiss-cpu
Requires-Dist: fastapi
Requires-Dist: uvicorn
Requires-Dist: numpy
Requires-Dist: aiohttp
Requires-Dist: pydantic
Requires-Dist: typer[all]
Requires-Dist: cryptography
Requires-Dist: pandas
Requires-Dist: pyarrow
Requires-Dist: psutil
Dynamic: license-file

# GPUCache - Advanced Multimodal AI Cache System

A cutting-edge multimodal caching system for AI applications featuring **token prefill reuse**, **adaptive TTL with ML-based prediction**, **semantic input caching**, and **heavy input optimization**. Built for production-scale LLM deployments with 8x performance improvements.

## Performance Benefits

![GPUCache Performance](prarabdha_performance.png)

**Key Performance Improvements:**
- **8.0x speedup** for long context processing (25K tokens)
- **3.9x speedup** for RAG processing (4 x 2K chunks)
- **3-10x delay savings** and GPU cycle reduction
- **Multi-layer caching** across GPU, RAM, Disk, and Redis

## üöÄ Advanced Features (2025-2027)

### 1. Token/Segment Prefill Reuse (Industry First)
Cross-session KV cache reuse with trie-based sequence matching for unprecedented performance gains.

```python
from gpucache import AdvancedMultimodalCache

# Initialize with prefill reuse
cache = AdvancedMultimodalCache(enable_prefill_reuse=True)

# Cache with token prefill
tokens = [1000, 1001, 1002, 1003, 1004]
kv_cache = {'layer_0': np.random.rand(32, 768).astype(np.float32)}

result = cache.cache_text_with_prefill(
    prompt="Explain quantum computing",
    response="Quantum computing uses quantum mechanical phenomena...",
    tokens=tokens,
    kv_cache=kv_cache,
    user_id="user_123",
    session_id="session_456"
)

# Retrieve with prefill reuse
retrieved = cache.get_text_with_prefill(
    prompt="Explain quantum computing",
    tokens=tokens,
    user_id="user_123"
)
```

### 2. Adaptive TTL with ML-based Prediction
Intelligent cache duration prediction using access patterns, semantic distance, and user priority.

```python
# Cache with adaptive TTL
cache = AdvancedMultimodalCache(enable_adaptive_ttl=True)

# High priority content (longer TTL)
cache.cache_text_with_prefill(
    prompt="Important system configuration",
    response="System configuration details...",
    tokens=[2000, 2001, 2002],
    kv_cache={'layer_0': np.random.rand(32, 768).astype(np.float32)},
    user_id="admin",
    user_priority=5  # High priority = longer TTL
)

# Low priority content (shorter TTL)
cache.cache_text_with_prefill(
    prompt="Temporary debug info",
    response="Debug information...",
    tokens=[3000, 3001],
    kv_cache={'layer_0': np.random.rand(32, 768).astype(np.float32)},
    user_id="developer",
    user_priority=1  # Low priority = shorter TTL
)
```

### 3. Enhanced Semantic Input Caching
Advanced prompt normalization and paraphrase detection for input-level caching.

```python
from gpucache import SemanticInputCache

cache = SemanticInputCache(threshold=0.85)

# Cache prompts with enhanced normalization
cache.put("Please explain quantum computing", "Quantum computing uses...")
cache.put("Can you tell me about machine learning?", "Machine learning is...")

# Retrieve with paraphrase detection
response = cache.get("I want to know about quantum computing")  # Will find cached response
```

### 4. Heavy Input Optimization
High-throughput caching optimized for heavy input systems with batch operations.

```python
from gpucache import HeavyInputCache

cache = HeavyInputCache(max_size=50000, similarity_threshold=0.85)

# Batch operations for high throughput
batch_data = [
    ("Prompt 1", "Response 1"),
    ("Prompt 2", "Response 2"),
    ("Prompt 3", "Response 3")
]
results = cache.batch_put(batch_data)

# Batch retrieval
prompts = ["Prompt 1", "Prompt 2"]
responses = cache.batch_get(prompts)
```

### 5. Unified Multimodal Caching
Cross-modal search across text, video, and audio with unified vector indexing.

```python
from gpucache import MultimodalCacheManager

manager = MultimodalCacheManager()

# Cache different modalities
text_id = manager.cache_text("Explain AI", "AI is...")
frame_id = manager.cache_video_frame(frame, "Person walking")
audio_id = manager.cache_audio_segment(waveform, 16000, "Speech")

# Cross-modal search
results = manager.cross_modal_search("person", modality="all")
```

## üèóÔ∏è Architecture

### Core Components

```
gpucache/
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ token_prefill_cache.py    # Cross-session KV cache reuse
‚îÇ   ‚îú‚îÄ‚îÄ adaptive_ttl_cache.py     # ML-based TTL prediction
‚îÇ   ‚îú‚îÄ‚îÄ vector_index.py           # FAISS-based similarity search
‚îÇ   ‚îî‚îÄ‚îÄ kv_store.py              # Multi-layer KV store
‚îú‚îÄ‚îÄ normalizer/
‚îÇ   ‚îú‚îÄ‚îÄ semantic_input_cache.py   # Enhanced prompt normalization
‚îÇ   ‚îî‚îÄ‚îÄ heavy_input_cache.py      # High-throughput optimization
‚îú‚îÄ‚îÄ multimodal_cache_manager.py   # Unified cache coordination
‚îú‚îÄ‚îÄ advanced_multimodal_cache.py  # All features combined
‚îú‚îÄ‚îÄ video/video_cache.py         # Video frame caching
‚îú‚îÄ‚îÄ audio/audio_cache.py         # Audio segment caching
‚îî‚îÄ‚îÄ examples/
    ‚îî‚îÄ‚îÄ multimodal_caching_demo.py # Comprehensive demos
```

### Multi-Layer Caching

![GPUCache Architecture](prarabdha_architecture.png)

- **GPU Cache**: FAISS + KV Store for fastest access
- **RAM Cache**: LRU/LFU with TTL for high-speed operations
- **Redis Cache**: Distributed + Auto-sharding for scalability
- **Disk Cache**: Persistent + Compression for long-term storage

## üì¶ Installation

```bash
pip install gpucache
```

## üöÄ Quick Start

### Basic Multimodal Caching

```python
from gpucache import MultimodalCacheManager
import numpy as np

# Initialize cache manager
cache = MultimodalCacheManager(similarity_threshold=0.85)

# Cache text content
text_result = cache.cache_text(
    "Explain quantum computing",
    "Quantum computing uses quantum mechanical phenomena...",
    {"topic": "quantum", "difficulty": "intermediate"}
)

# Cache video frame
frame = np.random.rand(224, 224, 3)  # Simulated video frame
video_result = cache.cache_video_frame(
    frame,
    "A person walking in a park",
    {"scene": "outdoor", "action": "walking"}
)

# Cache audio segment
waveform = np.random.rand(16000)  # 1 second audio at 16kHz
audio_result = cache.cache_audio_segment(
    waveform,
    16000,
    "Speech about artificial intelligence",
    {"content": "speech", "topic": "AI"}
)

# Retrieve content
text_response = cache.get_text("Explain quantum computing")
video_response = cache.get_video_frame(frame)
audio_response = cache.get_audio_segment(waveform, 16000)

# Cross-modal search
results = cache.cross_modal_search("person", modality='all')
```

### Advanced Features Demo

```python
from gpucache import AdvancedMultimodalCache
import numpy as np

# Initialize with all advanced features
advanced_cache = AdvancedMultimodalCache(
    max_memory_gb=32.0,
    similarity_threshold=0.85,
    enable_prefill_reuse=True,
    enable_adaptive_ttl=True,
    enable_heavy_input=True
)

# Token prefill reuse
tokens = [1000, 1001, 1002, 1003, 1004]
kv_cache = {
    'layer_0': np.random.rand(32, 768).astype(np.float32),
    'layer_1': np.random.rand(32, 768).astype(np.float32)
}

# Cache with prefill
prefill_result = advanced_cache.cache_text_with_prefill(
    prompt="Explain quantum computing",
    response="Quantum computing uses quantum mechanical phenomena...",
    tokens=tokens,
    kv_cache=kv_cache,
    user_id="user_123",
    session_id="session_456",
    user_priority=2,
    metadata={"topic": "quantum", "complexity": "high"}
)

# Retrieve with prefill reuse
retrieved = advanced_cache.get_text_with_prefill(
    prompt="Explain quantum computing",
    tokens=tokens,
    user_id="user_123"
)

if retrieved:
    print(f"Response: {retrieved['response']}")
    print(f"Cache type: {retrieved['cache_type']}")
    print(f"Prefix tokens: {len(retrieved['prefix_tokens'])}")
    print(f"KV cache layers: {len(retrieved['kv_cache'])}")
```

## üìä Performance Monitoring

```python
# Get comprehensive statistics
stats = advanced_cache.get_stats()
print(f"Hit rate: {stats['hit_rate']:.2%}")
print(f"Prefill hits: {stats['prefill_hits']}")
print(f"Adaptive TTL hits: {stats['adaptive_ttl_hits']}")
print(f"Heavy input hits: {stats['heavy_input_hits']}")
print(f"Multimodal hits: {stats['multimodal_hits']}")

# Export cache data for analysis
export_data = advanced_cache.export_cache_data()
```

## üîß Configuration

### Similarity Thresholds
```python
# Adjust thresholds for different use cases
semantic_threshold = 0.85    # Text similarity
video_threshold = 0.85       # Video similarity  
audio_threshold = 0.85       # Audio similarity
```

### Memory Management
```python
# Configure memory limits
max_memory_gb = 32.0         # Maximum memory usage
max_entries = 10000          # Maximum cache entries
batch_size = 1000            # Batch operation size
```

### Feature Toggles
```python
# Enable/disable features
enable_prefill_reuse = True   # Token prefill reuse
enable_adaptive_ttl = True    # ML-based TTL prediction
enable_heavy_input = True     # High-throughput optimization
enable_cross_modal = True     # Cross-modal search
```

## üéØ Use Cases

### Long Context LLM Applications
- **Token prefill reuse** for repeated context processing
- **Adaptive TTL** for optimal cache duration
- **Semantic clustering** for similar contexts

### RAG (Retrieval Augmented Generation) Systems
- **Cross-modal search** for text, video, and audio
- **Heavy input optimization** for high-throughput retrieval
- **Unified vector indexing** for efficient similarity search

### Multi-Server Deployments
- **Redis-backed distributed caching**
- **User isolation** with cross-user memory sharing
- **Real-time monitoring** and performance analytics

### Cost-Optimized AI Infrastructure
- **GPU cycle reduction** through intelligent caching
- **Memory optimization** with semantic-aware eviction
- **Batch operations** for high-throughput processing

## üöÄ Key Innovations

### 1. Token Prefill Reuse (Industry First)
- **Cross-session KV cache reuse** with trie-based matching
- **Multi-tenant support** with user isolation
- **Intelligent memory management** with semantic truncation
- **Efficient storage** with estimated memory usage tracking

### 2. Adaptive TTL with ML Prediction
- **Machine learning-based TTL prediction** using access patterns
- **Semantic decay** eviction based on content similarity
- **Multi-factor scoring** for optimal cache management
- **Context-aware caching** with user priority consideration

### 3. Enhanced Semantic Input Caching
- **Advanced prompt normalization** with domain-specific rules
- **Paraphrase detection engine** using synonym groups
- **Semantic clustering** for efficient retrieval
- **Threshold-based retrieval** with configurable similarity

### 4. Heavy Input Optimization
- **High-throughput caching** optimized for heavy input systems
- **Batch operations** for efficient processing
- **Performance tracking** with response time monitoring
- **Memory optimization** with intelligent eviction strategies

### 5. Unified Multimodal Management
- **Cross-modal search** across text, video, and audio
- **Unified vector indexing** with FAISS-based similarity search
- **Rich metadata storage** with export capabilities
- **Comprehensive statistics** and performance monitoring

## üìà Performance Metrics

### Cache Statistics
- **Hit rate tracking** per modality and feature
- **Request counting** and analysis
- **Performance metrics** export
- **Real-time monitoring** dashboard

### Scalability Features
- **FAISS** for efficient similarity search
- **Redis** for distributed caching
- **Vector indexing** for fast retrieval
- **Batch operations** for high throughput

### Production Ready
- **Configurable similarity thresholds**
- **Error handling** and recovery
- **Cache persistence** and backup
- **Cross-modal metadata storage**

## üîÆ Future Enhancements

### Planned Features
- **Real-time streaming** support for live video/audio caching
- **Distributed caching** across multiple nodes
- **Advanced embedding models** integration
- **Cache compression** for storage optimization
- **API integration** for RESTful cache management

### Model Integration
- **CLIP** for video/image embeddings
- **Wav2Vec2** for audio embeddings
- **Sentence Transformers** for text embeddings
- **Custom models** for user-defined embedding functions

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Submit a pull request

## üìÑ License

MIT License - see LICENSE file for details

## üìö Citation

If you use this system in your research, please cite:

```bibtex
@software{gpucache_multimodal_2025,
  title={GPUCache: Advanced Multimodal AI Cache System},
  author={Prarabdha Soni},
  year={2025},
  url={https://github.com/prarabdha-soni/gpucache}
}
```

## üîó Links

- **PyPI**: https://pypi.org/project/gpucache/
- **GitHub**: https://github.com/prarabdha-soni/gpucache
- **Documentation**: Coming soon
- **Issues**: https://github.com/prarabdha-soni/gpucache/issues

---

**GPUCache** - The fastest multimodal AI cache system for production LLM deployments! üöÄ 
