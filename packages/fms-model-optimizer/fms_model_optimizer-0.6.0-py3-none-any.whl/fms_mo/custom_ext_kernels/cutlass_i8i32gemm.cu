// ------ For Linear ------
// This file is modified from base code automatically generated by the CUTLASS 3.2.1 Python
// interface (https://github.com/nvidia/cutlass/python)

#include <cuda_runtime.h>
#include <torch/extension.h>
#include <ATen/ATen.h>
#include <ATen/cuda/CUDAContext.h>
#include "cutlass/cutlass.h"
#include "cutlass/util/device_memory.h"

// helper function allocating the memory
void* device_memory_allocation(size_t size, int device_id=0) {
    if (size > 0) {
        torch::Device device(torch::kCUDA, device_id);
        cudaStream_t stream = at::cuda::getCurrentCUDAStream();
        torch::TensorOptions options = torch::TensorOptions().dtype(torch::kI8).device(device);
        at::Tensor device_tensor = torch::empty({(long)size,}, options);
        return reinterpret_cast<void*>(device_tensor.data_ptr());
    } else {
        return nullptr;
    }
}


#include "cutlass/gemm/device/gemm_universal.h"


// Gemm operator cutlass_tensorop_s8_i16832gemm_s8_256x128_128x3_tn_align16
using DeviceKernel =
    typename cutlass::gemm::device::GemmUniversal<
        // Data type and layout of operand A
        int8_t, cutlass::layout::RowMajor,
        // Data type and layout of operand B
        int8_t, cutlass::layout::ColumnMajor,
        // Data type and layout of operand C
        int32_t, cutlass::layout::RowMajor,
        // Data type of accumulator
        int32_t,
        // Class of operation
        cutlass::arch::OpClassTensorOp,
        // Compute capability of the target kernel
        cutlass::arch::Sm80,
        // Threadblock tile shape
        cutlass::gemm::GemmShape<128, 256, 64>,
        // Warp tile shape
        cutlass::gemm::GemmShape<64, 64, 64>,
        // Instruction shape
        cutlass::gemm::GemmShape<16, 8, 32>, // was 8, 8, 16, better to use 16, 8, 32
        // Epilogue functor
        cutlass::epilogue::thread::LinearCombination<int32_t, 4, int32_t, int32_t>,
        // Swizzling function
        cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>,
        // Number of pipeline stages
        3, // better to be >=3
        // Alignment of operands A and B
        16, 16,
        // Type of math operation
        cutlass::arch::OpMultiplyAddSaturate,
        // Complex transform types of operands A and B
        cutlass::ComplexTransform::kNone, cutlass::ComplexTransform::kNone
    >;


using ElementCompute = typename DeviceKernel::EpilogueOutputOp::ElementCompute;

cutlass::Status batched_gemm_kernel_run(int M, int N, int K,
                        const DeviceKernel::ElementA* A, const DeviceKernel::ElementB* B, const DeviceKernel::ElementC* C, DeviceKernel::ElementC* D,
                        ElementCompute alpha, ElementCompute beta) {

  typename DeviceKernel::Arguments arguments {
      cutlass::gemm::GemmUniversalMode::kGemm,
      {M, N, K},                                        // problem size
      1,
      {alpha, beta},
      A, B, C, D,
      0, 0, 0, 0,                                       // batch strides
      DeviceKernel::LayoutA::packed({M, K}).stride(0),  // lda
      DeviceKernel::LayoutB::packed({K, N}).stride(0),  // ldb,
      DeviceKernel::LayoutC::packed({M, N}).stride(0),  // ldc
      DeviceKernel::LayoutC::packed({M, N}).stride(0)   // ldd
  };

  size_t workspace_size = DeviceKernel::get_workspace_size(arguments);
  cutlass::device_memory::allocation<uint8_t> workspace(workspace_size);

  DeviceKernel gemm_op;
  cutlass::Status status = gemm_op.initialize(arguments,
                                              workspace.get(),
                                              nullptr);     // CUDA stream

  if (status != cutlass::Status::kSuccess) {
    return status;
  }

  status = gemm_op();
  return status;
}

at::Tensor batched_gemm_kernel(const at::Tensor& A, const at::Tensor& B, at::optional<const at::Tensor> C, int32_t alpha, int32_t beta) {
    int M = A.size(0);
    int N = B.size(1);
    int K = A.size(1);

    typename DeviceKernel::ElementC* ptrC = (C == at::nullopt) ?
                                            nullptr :
                                            reinterpret_cast<typename DeviceKernel::ElementC*>(C->contiguous().data_ptr());
    at::Tensor D = B.new_empty({M, N}, torch::kI32);
    // std::cout<<M<<N<<K<<std::endl;

    cutlass::Status status = batched_gemm_kernel_run(M, N, K,
                                                reinterpret_cast<typename DeviceKernel::ElementA*>(A.contiguous().data_ptr()),
                                                reinterpret_cast<typename DeviceKernel::ElementB*>(B.data_ptr()), //.contiguous()
                                                ptrC,
                                                reinterpret_cast<typename DeviceKernel::ElementC*>(D.contiguous().data_ptr()),
                                                ElementCompute(alpha), ElementCompute(beta));

    TORCH_CHECK(status == cutlass::Status::kSuccess, "CUTLASS kernel failed");
    return D;
}


// ------ For Conv2d ------
// This file was automatically generated by the CUTLASS 3.3.0 Python interface (https://github.com/nvidia/cutlass/python)

#include "cutlass/conv/kernel/default_conv2d_fprop.h"
#include "cutlass/conv/kernel/default_conv2d_dgrad.h"
#include "cutlass/conv/kernel/default_conv2d_wgrad.h"
#include "cutlass/conv/device/implicit_gemm_convolution.h"



// Conv2d operation cutlass_sm80_tensorop_s8_i16x8x32fprop_analytic_s8_256x128_128x3_nhwc_align16
// Reference:
//    cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm80.cu

using Conv2dFpropKernel = typename cutlass::conv::kernel::DefaultConv2dFprop<
  int8_t, cutlass::layout::TensorNHWC,
  int8_t, cutlass::layout::TensorNHWC,
  int32_t, cutlass::layout::TensorNHWC, // was int8_t
  int32_t,
  cutlass::arch::OpClassTensorOp,
  cutlass::arch::Sm80,
  cutlass::gemm::GemmShape<128, 128, 64>, // was 256, 128,128
  cutlass::gemm::GemmShape<64, 64, 64 >, // was 64, 64, 128
  cutlass::gemm::GemmShape<16, 8, 32>,
  cutlass::epilogue::thread::LinearCombination<
    int32_t, // was int8_t
    128 / cutlass::sizeof_bits<int32_t>::value, // was 16
    int32_t, // was int32_t
    float>, // was int32_t
  cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>,
  3,
  cutlass::arch::OpMultiplyAddSaturate,
  cutlass::conv::IteratorAlgorithm::kOptimized // was 4 lines below
  // cutlass::conv::IteratorAlgorithm::kAnalytic,
  // cutlass::conv::StrideSupport::kStrided,
  // 16, // what's this for?
  // 16
>::Kernel;

using DeviceKernel_conv =
    typename cutlass::conv::device::ImplicitGemmConvolution<Conv2dFpropKernel>;



using UnderlyingKernel = typename DeviceKernel_conv::UnderlyingKernel;
namespace {
using TensorRefA = typename UnderlyingKernel::TensorRefA;
using TensorRefB = typename UnderlyingKernel::TensorRefB;
using TensorRefC = typename UnderlyingKernel::TensorRefC;
using ElementCompute_conv = typename UnderlyingKernel::EpilogueOutputOp::ElementCompute;
}

template<typename TensorRef, typename Element>
TensorRef get_tensor_ref(cutlass::Tensor4DCoord tensor_coord, Element* ptr){
  cutlass::layout::TensorNHWC layout = cutlass::layout::TensorNHWC::packed(tensor_coord);
  TensorRef tensor_ref(ptr, layout);
  return tensor_ref;
}

cutlass::Status conv_int8_kernel_run(cutlass::conv::Conv2dProblemSize* problem_size,
                        UnderlyingKernel::ElementA* A, UnderlyingKernel::ElementB* B,
                        UnderlyingKernel::ElementC* C, UnderlyingKernel::ElementC* D,
                        ElementCompute_conv alpha, ElementCompute_conv beta, std::string split_k_mode,
                        cudaStream_t stream, int device_id=0) {
  // create the tensor references
  cutlass::Tensor4DCoord tensor_coord_A = cutlass::conv::implicit_gemm_tensor_a_extent(
    cutlass::conv::Operator::kFprop, *problem_size
  );
  cutlass::Tensor4DCoord tensor_coord_B = cutlass::conv::implicit_gemm_tensor_b_extent(
    cutlass::conv::Operator::kFprop, *problem_size
  );
  cutlass::Tensor4DCoord tensor_coord_C = cutlass::conv::implicit_gemm_tensor_c_extent(
    cutlass::conv::Operator::kFprop, *problem_size
  );

  TensorRefA tensor_ref_A = get_tensor_ref<TensorRefA, UnderlyingKernel::ElementA>(tensor_coord_A, A);
  TensorRefB tensor_ref_B = get_tensor_ref<TensorRefB, UnderlyingKernel::ElementB>(tensor_coord_B, B);
  TensorRefC tensor_ref_C = get_tensor_ref<TensorRefC, UnderlyingKernel::ElementC>(tensor_coord_C, C);
  TensorRefC tensor_ref_D = get_tensor_ref<TensorRefC, UnderlyingKernel::ElementC>(tensor_coord_C, D);

  cutlass::conv::SplitKMode mode;
  if (split_k_mode == "serial") {
    mode = cutlass::conv::SplitKMode::kSerial;
  } else if (split_k_mode == "parallel") {
    mode = cutlass::conv::SplitKMode::kParallel;
  } else {
    throw std::runtime_error("Invalid split_k_mode: " + split_k_mode);
  }

  typename DeviceKernel_conv::Arguments arguments{
    *problem_size,
    tensor_ref_A,
    tensor_ref_B,
    tensor_ref_C,
    tensor_ref_D,
    {alpha, beta},
    mode
  };

  DeviceKernel_conv implicit_gemm_op;

  size_t workspace_size = implicit_gemm_op.get_workspace_size(arguments);

  void* workspace_ptr = device_memory_allocation(workspace_size, device_id);

  cutlass::Status status = implicit_gemm_op.can_implement(arguments);
  if (status != cutlass::Status::kSuccess) {
    return status;
  }

  status = implicit_gemm_op.initialize(arguments, workspace_ptr, stream);
  if (status != cutlass::Status::kSuccess) {
    return status;
  }

  //
  // Launch initialized CUTLASS kernel
  //
  status = implicit_gemm_op(stream);

  return status;
}

at::Tensor conv_int8_kernel(const at::Tensor& A, const at::Tensor& B, at::optional<const at::Tensor> C=at::nullopt,
    std::tuple<int, int> stride={1, 1}, std::tuple<int, int> padding={0, 0}, std::tuple<int, int> dilation={1, 1},
    float alpha=1.f, float beta=0.f, std::string split_k_mode="serial", int split_k_slices=1,
    bool output_nchw_chlast=true) {
    int N, H, W, C_, K, R, S, P, Q;
    N = A.size(0);
    C_ = A.size(1);
    H = A.size(2);
    W = A.size(3);

    K = B.size(0);
    R = B.size(2);
    S = B.size(3);

    cutlass::conv::Conv2dProblemSize problem_size(
        cutlass::Tensor4DCoord(N, H, W, C_),
        cutlass::Tensor4DCoord(K, R, S, C_),
        cutlass::Tensor4DCoord(std::get<0>(padding), std::get<0>(padding), std::get<1>(padding), std::get<1>(padding)),
        cutlass::MatrixCoord(std::get<0>(stride), std::get<1>(stride)),
        cutlass::MatrixCoord(std::get<0>(dilation), std::get<1>(dilation)),
        cutlass::conv::Mode::kCrossCorrelation,
        split_k_slices
    );

    P = problem_size.P;
    Q = problem_size.Q;

    typename UnderlyingKernel::ElementC* ptrC = (C == at::nullopt) ?
                                            nullptr :
                                            reinterpret_cast<typename UnderlyingKernel::ElementC*>(C->data_ptr());

    torch::TensorOptions options = torch::TensorOptions()
                                              .dtype(torch::kI32)
                                              .device(B.device())
                                              .requires_grad(false)
                                              .memory_format(at::MemoryFormat::ChannelsLast);
                                              // https://github.com/pytorch/pytorch/blob/main/c10/core/MemoryFormat.h
    at::Tensor D = (output_nchw_chlast) ? torch::zeros({N, P, Q, K}, options) :
                                          torch::zeros({N, K, P, Q}, options); // was N, K, P, Q
                                          // If we want NCHW+ChLast, create NHWC first then permute
    // at::Tensor D = torch::zeros({N, K, P, Q}, options);

    cudaStream_t stream = at::cuda::getCurrentCUDAStream();

    cutlass::Status status = conv_int8_kernel_run(
        &problem_size,
        reinterpret_cast<typename UnderlyingKernel::ElementA*>(A.data_ptr()),
        reinterpret_cast<typename UnderlyingKernel::ElementB*>(B.data_ptr()),
        ptrC,
        reinterpret_cast<typename UnderlyingKernel::ElementC*>(D.data_ptr()),
        alpha, beta,
        split_k_mode, stream, B.device().index());

    TORCH_CHECK(status == cutlass::Status::kSuccess, "CUTLASS kernel failed");
    // seems like this i8 kernel will output nhwc content + contiguous memory,
    //  hence, if we want nchw+ch last memory layout, we need to permute
    return (output_nchw_chlast) ? D.permute({0, 3, 1, 2}): D;
}
