from __future__ import annotations

from typing import Dict, List, Optional

import requests
from bs4 import BeautifulSoup
from langchain.tools import StructuredTool
from pydantic import BaseModel, Field

from agentfoundry.utils.logger import get_logger

logger = get_logger(__name__)


class WebPageScraperInput(BaseModel):
    """
    Input schema for the scrape_webpage tool.
    """
    url: str = Field(..., description="The URL of the webpage to scrape (e.g., 'https://example.com').")
    headers: Optional[Dict[str, str]] = Field(
        None, description="Optional HTTP headers to include in the request."
    )
    timeout: int = Field(10, description="Timeout in seconds for the HTTP request (default: 10).")
    render_js: bool = Field(
        False,
        description=(
            "Whether to render JavaScript before scraping. "
            "Requires 'requests-html' and a Chromium backend."
        ),
    )


def scrape_webpage(
    url: str,
    headers: Optional[Dict[str, str]] = None,
    timeout: int = 10,
    render_js: bool = False,
) -> Dict:
    """
    Fetches a webpage and returns a structured JSON of its key components
    (title, metadata, headings, paragraphs, links, images, lists, tables)
    without the raw HTML markup.

    Args:
        url (str): The webpage URL to scrape.
        headers (dict, optional): HTTP headers for the request.
        timeout (int, optional): Request timeout in seconds.
        render_js (bool, optional): Whether to render JavaScript content using requests-html.

    Returns:
        dict: A dictionary containing structured page content.
    """
    logger.info(f"Scraping webpage: {url} (render_js={render_js})")
    # Use a real browser User-Agent by default to avoid basic bot detection
    if not headers:
        headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/115.0.0.0 Safari/537.36"
            )
        }

    soup = None
    # Optionally render JavaScript for dynamic pages
    if render_js:
        try:
            from requests_html import HTMLSession

            logger.debug("Initializing JS rendering session")
            session = HTMLSession()
            resp_js = session.get(url, headers=headers, timeout=timeout)
            logger.debug(f"JS GET {url} returned status {resp_js.status_code} ({len(resp_js.content)} bytes)")
            resp_js.html.render(timeout=timeout)
            soup = BeautifulSoup(resp_js.html.html, "html.parser")
            logger.debug("Parsed JS-rendered HTML into BeautifulSoup")
        except Exception as exc_js:
            logger.error(f"JS rendering failed for {url}: {exc_js}", exc_info=True)
            # Fallback to static fetch below

    # Fallback to static fetch if not rendering JS or JS fetch failed
    if soup is None:
        try:
            response = requests.get(url, headers=headers, timeout=timeout)
            logger.debug(f"HTTP GET {url} returned status {response.status_code} ({len(response.content)} bytes)")
            # Treat any non-200 status as error to avoid stale anti-bot pages
            response.raise_for_status()
        except Exception as exc:
            logger.error(f"Failed to fetch {url}: {exc}", exc_info=True)
            return {"error": str(exc)}

        soup = BeautifulSoup(response.text, "html.parser")
        logger.debug("Parsed HTML content into BeautifulSoup")

    # Title
    title = soup.title.string.strip() if soup.title and soup.title.string else None
    logger.debug(f"Extracted title: {title}")

    # Meta description and keywords
    meta_desc = None
    meta_keywords = None
    if soup.head:
        desc_tag = soup.head.find("meta", attrs={"name": "description"})
        meta_desc = desc_tag.get("content").strip() if desc_tag and desc_tag.get("content") else None
        kw_tag = soup.head.find("meta", attrs={"name": "keywords"})
        meta_keywords = kw_tag.get("content").strip() if kw_tag and kw_tag.get("content") else None
    logger.debug(f"Extracted meta_description: {meta_desc}, meta_keywords: {meta_keywords}")

    # Headings H1-H6
    headings: List[Dict[str, str]] = []
    for tag in soup.find_all(["h1", "h2", "h3", "h4", "h5", "h6"]):
        text = tag.get_text(strip=True)
        if text:
            headings.append({"level": tag.name, "text": text})
    logger.debug(f"Found {len(headings)} heading elements")

    # Paragraphs
    paragraphs: List[str] = []
    for p in soup.find_all("p"):
        text = p.get_text(strip=True)
        if text:
            paragraphs.append(text)
    logger.debug(f"Found {len(paragraphs)} paragraphs")

    # Links
    links: List[Dict[str, str]] = []
    for a in soup.find_all("a", href=True):
        text = a.get_text(strip=True)
        links.append({"href": a["href"], "text": text})
    logger.debug(f"Found {len(links)} links")

    # Images
    images: List[Dict[str, Optional[str]]] = []
    for img in soup.find_all("img", src=True):
        images.append({"src": img.get("src"), "alt": img.get("alt")})
    logger.debug(f"Found {len(images)} images")

    # Lists (ordered and unordered)
    lists: List[Dict[str, List[str]]] = []
    for lst in soup.find_all(["ul", "ol"]):
        items = [li.get_text(strip=True) for li in lst.find_all("li") if li.get_text(strip=True)]
        lists.append({"type": lst.name, "items": items})
    logger.debug(f"Found {len(lists)} lists")

    # Tables
    tables: List[List[List[str]]] = []
    for table in soup.find_all("table"):
        rows: List[List[str]] = []
        for tr in table.find_all("tr"):
            cells = [cell.get_text(strip=True) for cell in tr.find_all(["th", "td"])]
            if cells:
                rows.append(cells)
        if rows:
            tables.append(rows)
    logger.debug(f"Found {len(tables)} tables")

    logger.info(f"Completed scraping and structuring content for {url}")

    return {
        "url": url,
        "title": title,
        "meta_description": meta_desc,
        "meta_keywords": meta_keywords,
        "headings": headings,
        "paragraphs": paragraphs,
        "links": links,
        "images": images,
        "lists": lists,
        "tables": tables,
    }


# Register as a LangChain structured tool
webpage_scraper_tool = StructuredTool.from_function(
    func=scrape_webpage,
    name="webpage_scraper",
    description=(
        "Scrape a webpage and return its structured content (title, headings, paragraphs, links, "
        "images, lists, tables) in JSON format. Supports JavaScript rendering when needed via the "
        "render_js flag."
    ),
    args_schema=WebPageScraperInput,
)