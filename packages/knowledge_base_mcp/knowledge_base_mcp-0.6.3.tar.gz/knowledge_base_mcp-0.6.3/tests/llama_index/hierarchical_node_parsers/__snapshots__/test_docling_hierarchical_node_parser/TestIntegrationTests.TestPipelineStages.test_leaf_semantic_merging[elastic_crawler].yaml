- node_id: '0'
  node_depth: 0
  node_type: Node
  content_length: 10648
  content: "# How to contribute elastic-crawler\n\nThe `elastic-crawler` repository\
    \ is a free and open project, and we love to receive contributions from our community\
    \ \u2014 you! There are many ways to contribute, from writing tutorials or blog\
    \ posts, improving the documentation, submitting bug reports and feature requests\
    \ or writing code which can be incorporated into `elastic-crawler` itself.\n\n\
    If you want to be rewarded for your contributions, sign up for the Elastic Contributor\
    \ Program . Each time you make a valid contribution, you\u2019ll earn points that\
    \ increase your chances of winning prizes and being recognized as a top contributor.\n\
    \n- Reporting issues\n- Getting help\nTypes of contribution\n    - Enhancements\n\
    Contribution Checklist\n    - Acceptance criteria\n    - Correct code organization\n\
    \    - Log verbosity\n    - Linting\n    - Testing\n    - Backport labels\nPull\
    \ request etiquette\n    - Why do we use a pull request workflow?\n    What constitutes\
    \ a good PR?\n        - Ensure there is a solid title and sum..."
  metadata:
    docling_label: title
    docling_ref: '#/texts/0'
    headings:
    - '# How to contribute elastic-crawler'
  relationships:
    children:
    - '1'
    - '2'
    - '3'
    - '4'
    - '14'
    - '27'
- node_id: '1'
  node_depth: 1
  node_type: Node
  content_length: 625
  content: "# How to contribute elastic-crawler\n\nThe `elastic-crawler` repository\
    \ is a free and open project, and we love to receive contributions from our community\
    \ \u2014 you! There are many ways to contribute, from writing tutorials or blog\
    \ posts, improving the documentation, submitting bug reports and feature requests\
    \ or writing code which can be incorporated into `elastic-crawler` itself.\n\n\
    If you want to be rewarded for your contributions, sign up for the Elastic Contributor\
    \ Program . Each time you make a valid contribution, you\u2019ll earn points that\
    \ increase your chances of winning prizes and being recognized as a top contributor."
  metadata:
    docling_label: title
    docling_ref: '#/texts/0'
    headings:
    - '# How to contribute elastic-crawler'
  relationships:
    next: '2'
    parent: '0'
- node_id: '2'
  node_depth: 1
  node_type: Node
  content_length: 1029
  content: |-
    - Reporting issues
    - Getting help
    Types of contribution
        - Enhancements
    Contribution Checklist
        - Acceptance criteria
        - Correct code organization
        - Log verbosity
        - Linting
        - Testing
        - Backport labels
    Pull request etiquette
        - Why do we use a pull request workflow?
        What constitutes a good PR?
            - Ensure there is a solid title and summary
            - Be explicit about the PR status
            - Keep your branch up-to-date
            - Keep it small
    Reviewing Pull Requests
        - Keep the flow going
        - We are all reviewers
        - Don't add to the PR as a reviewer

    ## Reporting issues

    If something is not working as expected, please open an issue .

    ## Getting help

    The Ingestion team at Elastic maintains this repository and is happy to help. Try posting your question to the Elastic discuss forums . Be sure to mention that you're using the Elastic Crawler and any errors/issues you are encountering. You can also find us in the `#search-enterprise` channel of ...
  metadata:
    docling_label: list
    docling_ref: '#/groups/2'
    headings:
    - '# How to contribute elastic-crawler'
  relationships:
    next: '3'
    parent: '0'
    previous: '1'
- node_id: '3'
  node_depth: 1
  node_type: Node
  content_length: 437
  content: "## Types of contribution\n\n### Enhancements\n\nEnhancements that can\
    \ be done after your initial contribution:\n\n1. Ensure the backend meets performance\
    \ requirements we might request (memory usage, how fast it syncs 10k docs, etc.)\n\
    2. Update the README for the Elastic Crawler\n3. Small functional improvements\
    \ for Elastic Crawler\n\n\u2139\uFE0F Use-case specific customizations (as opposed\
    \ to generic enhancements) will not be accepted as contributions."
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/48'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Types of contribution'
  relationships:
    next: '4'
    parent: '0'
    previous: '2'
- node_id: '4'
  node_depth: 1
  node_type: Node
  content_length: 2646
  content: |-
    ## Contribution Checklist

    ### Acceptance criteria

    All patch changes should have a corresponding GitHub issue filed within the repository. If you need changes that are complex, or you are not sure about how to do something, reach out to the Ingestion team and/or file an issue.

    ### Correct code/file organization

    Any contribution should follow established patterns of code organization within the repository. For example, a new concrete extension of `CrawlResult` should live in lib/crawler/data/crawl_result , and its tests should live in lib/crawler/data/crawl_result . If that new data source is named `FooResult` the file and spec file should be `foo_result.rb` and `foo_result_spec.rb` , respectively. It should also inherit any `Base` class within the module.

    If you are unsure of where a file/class should go - ask.

    ### Log verbosity

    Logging is important to get insights on what's happening in the service. However, we should be careful not to pile up logs that are not adding value in o...
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/55'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
  relationships:
    children:
    - '5'
    - '6'
    - '8'
    - '10'
    - '12'
    next: '14'
    parent: '0'
    previous: '3'
- node_id: '5'
  node_depth: 2
  node_type: Node
  content_length: 407
  content: |-
    ## Contribution Checklist

    ### Acceptance criteria

    All patch changes should have a corresponding GitHub issue filed within the repository. If you need changes that are complex, or you are not sure about how to do something, reach out to the Ingestion team and/or file an issue.

    ### Backport labels

    Make sure to include the appropriate backport labels, if your PR needs to be backported to a past version.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/55'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
  relationships:
    next: '6'
    parent: '4'
- node_id: '6'
  node_depth: 2
  node_type: Node
  content_length: 546
  content: |-
    ### Correct code/file organization

    Any contribution should follow established patterns of code organization within the repository. For example, a new concrete extension of `CrawlResult` should live in lib/crawler/data/crawl_result , and its tests should live in lib/crawler/data/crawl_result . If that new data source is named `FooResult` the file and spec file should be `foo_result.rb` and `foo_result_spec.rb` , respectively. It should also inherit any `Base` class within the module.

    If you are unsure of where a file/class should go - ask.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/61'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Correct code/file organization'
  relationships:
    children:
    - '7'
    next: '8'
    parent: '4'
    previous: '5'
- node_id: '7'
  node_depth: 3
  node_type: Node
  content_length: 546
  content: |-
    ### Correct code/file organization

    Any contribution should follow established patterns of code organization within the repository. For example, a new concrete extension of `CrawlResult` should live in lib/crawler/data/crawl_result , and its tests should live in lib/crawler/data/crawl_result . If that new data source is named `FooResult` the file and spec file should be `foo_result.rb` and `foo_result_spec.rb` , respectively. It should also inherit any `Base` class within the module.

    If you are unsure of where a file/class should go - ask.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/61'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Correct code/file organization'
  relationships:
    parent: '6'
- node_id: '8'
  node_depth: 2
  node_type: Node
  content_length: 753
  content: |-
    ### Log verbosity

    Logging is important to get insights on what's happening in the service. However, we should be careful not to pile up logs that are not adding value in our systems.

    A few tips per log level:

    - CRITICAL (50) -- anything that stops the service, if you want to add more details on why.
    - ERROR (40) -- all Python exceptions will use that level, but you can call it specifically to add details
    - WARNING (30) -- any unexpected behavior that the system knows how to handle, but that should be notified (network retries, deprecation)
    - INFO (20) -- normal operations feedback. These should not be verbose so we don't pile logs overtime for nothing.
    - DEBUG (10) -- like info but with as many details as possible
    - NOTSET (0) -- never used
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/81'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Log verbosity'
  relationships:
    children:
    - '9'
    next: '10'
    parent: '4'
    previous: '6'
- node_id: '9'
  node_depth: 3
  node_type: Node
  content_length: 753
  content: |-
    ### Log verbosity

    Logging is important to get insights on what's happening in the service. However, we should be careful not to pile up logs that are not adding value in our systems.

    A few tips per log level:

    - CRITICAL (50) -- anything that stops the service, if you want to add more details on why.
    - ERROR (40) -- all Python exceptions will use that level, but you can call it specifically to add details
    - WARNING (30) -- any unexpected behavior that the system knows how to handle, but that should be notified (network retries, deprecation)
    - INFO (20) -- normal operations feedback. These should not be verbose so we don't pile logs overtime for nothing.
    - DEBUG (10) -- like info but with as many details as possible
    - NOTSET (0) -- never used
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/81'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Log verbosity'
  relationships:
    parent: '8'
- node_id: '10'
  node_depth: 2
  node_type: Node
  content_length: 407
  content: |-
    ### Linting

    Code style is important in shared codebases, as it helps ensure that everyone can read and understand code that they didn't write. In order to enforce code style, our CI jobs apply a linter (rubocop), and will fail to build (and block merging of) non-compliant changes.

    You can run the linter locally with `./script/bundle exec rubocop` to ensure that your changes do not introduce any issues.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/91'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Linting'
  relationships:
    children:
    - '11'
    next: '12'
    parent: '4'
    previous: '8'
- node_id: '11'
  node_depth: 3
  node_type: Node
  content_length: 407
  content: |-
    ### Linting

    Code style is important in shared codebases, as it helps ensure that everyone can read and understand code that they didn't write. In order to enforce code style, our CI jobs apply a linter (rubocop), and will fail to build (and block merging of) non-compliant changes.

    You can run the linter locally with `./script/bundle exec rubocop` to ensure that your changes do not introduce any issues.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/91'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Linting'
  relationships:
    parent: '10'
- node_id: '12'
  node_depth: 2
  node_type: Node
  content_length: 525
  content: |-
    ### Testing

    Tests not only verify and demonstrate that a new feature does what it is supposed to, but they also protect the codebase from unintentional future regressions. For this reason, it is important to both add tests when contributing new code, and to ensure that all tests (old and new) are passing.

    Our goal is to maintain 92% test coverage for the Elastic Crawler.

    You can run the tests locally with `./script/bundle rspec <file-path>` .

    Be sure to read about our unit tests and integration tests . # TODO: check
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/97'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Testing'
  relationships:
    children:
    - '13'
    parent: '4'
    previous: '10'
- node_id: '13'
  node_depth: 3
  node_type: Node
  content_length: 525
  content: |-
    ### Testing

    Tests not only verify and demonstrate that a new feature does what it is supposed to, but they also protect the codebase from unintentional future regressions. For this reason, it is important to both add tests when contributing new code, and to ensure that all tests (old and new) are passing.

    Our goal is to maintain 92% test coverage for the Elastic Crawler.

    You can run the tests locally with `./script/bundle rspec <file-path>` .

    Be sure to read about our unit tests and integration tests . # TODO: check
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/97'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Testing'
  relationships:
    parent: '12'
- node_id: '14'
  node_depth: 1
  node_type: Node
  content_length: 3451
  content: |-
    ## Pull Request Etiquette

    *this is copied and adapted from https://gist.github.com/mikepea/863f63d6e37281e329f8*

    ### Why do we use a Pull Request workflow?

    PRs are a great way of sharing information, and can help us be aware of the changes that are occurring in our codebase. They are also an excellent way of getting peer review on the work that we do, without the cost of working in direct pairs.

    **Ultimately though, the primary reason we use PRs is to encourage quality in**

    **the commits that are made to our code repositories**

    Done well, the commits (and their attached messages) contained within tell a story to people examining the code at a later date. If we are not careful to ensure the quality of these commits, we silently lose this ability.

    **Poor quality code can be refactored. A terrible commit lasts forever.**

    ### What constitutes a good PR?

    A good quality PR will have the following characteristics:

    - It will be a complete piece of work that adds value in some way.
    - ...
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/111'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
  relationships:
    children:
    - '15'
    - '16'
    - '18'
    next: '27'
    parent: '0'
    previous: '4'
- node_id: '15'
  node_depth: 2
  node_type: Node
  content_length: 113
  content: |-
    ## Pull Request Etiquette

    *this is copied and adapted from https://gist.github.com/mikepea/863f63d6e37281e329f8*
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/111'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
  relationships:
    next: '16'
    parent: '14'
- node_id: '16'
  node_depth: 2
  node_type: Node
  content_length: 721
  content: |-
    ### Why do we use a Pull Request workflow?

    PRs are a great way of sharing information, and can help us be aware of the changes that are occurring in our codebase. They are also an excellent way of getting peer review on the work that we do, without the cost of working in direct pairs.

    **Ultimately though, the primary reason we use PRs is to encourage quality in**

    **the commits that are made to our code repositories**

    Done well, the commits (and their attached messages) contained within tell a story to people examining the code at a later date. If we are not careful to ensure the quality of these commits, we silently lose this ability.

    **Poor quality code can be refactored. A terrible commit lasts forever.**
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/113'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### Why do we use a Pull Request workflow?'
  relationships:
    children:
    - '17'
    next: '18'
    parent: '14'
    previous: '15'
- node_id: '17'
  node_depth: 3
  node_type: Node
  content_length: 721
  content: |-
    ### Why do we use a Pull Request workflow?

    PRs are a great way of sharing information, and can help us be aware of the changes that are occurring in our codebase. They are also an excellent way of getting peer review on the work that we do, without the cost of working in direct pairs.

    **Ultimately though, the primary reason we use PRs is to encourage quality in**

    **the commits that are made to our code repositories**

    Done well, the commits (and their attached messages) contained within tell a story to people examining the code at a later date. If we are not careful to ensure the quality of these commits, we silently lose this ability.

    **Poor quality code can be refactored. A terrible commit lasts forever.**
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/113'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### Why do we use a Pull Request workflow?'
  relationships:
    parent: '16'
- node_id: '18'
  node_depth: 2
  node_type: Node
  content_length: 2613
  content: |-
    ### What constitutes a good PR?

    A good quality PR will have the following characteristics:

    - It will be a complete piece of work that adds value in some way.
    - It will have a title that reflects the work within, and a summary that helps to understand the context of the change.
    - There will be well written commit messages, with well crafted commits that tell the story of the development of this work.
    - Ideally it will be small and easy to understand. Single commit PRs are usually easy to submit, review, and merge.
    - The code contained within will meet the best practises set by the team wherever possible.

    A PR does not end at submission though. A code change is not made until it is merged and used in production.

    A good PR should be able to flow through a peer review system easily and quickly.

    #### Ensure there is a solid title and summary

    PRs are a Github workflow tool, so it's important to understand that the PR title, summary and eventual discussion are not as trackable as the th...
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/124'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
  relationships:
    children:
    - '19'
    - '20'
    - '22'
    - '24'
    - '25'
    parent: '14'
    previous: '16'
- node_id: '19'
  node_depth: 3
  node_type: Node
  content_length: 805
  content: |-
    ### What constitutes a good PR?

    A good quality PR will have the following characteristics:

    - It will be a complete piece of work that adds value in some way.
    - It will have a title that reflects the work within, and a summary that helps to understand the context of the change.
    - There will be well written commit messages, with well crafted commits that tell the story of the development of this work.
    - Ideally it will be small and easy to understand. Single commit PRs are usually easy to submit, review, and merge.
    - The code contained within will meet the best practises set by the team wherever possible.

    A PR does not end at submission though. A code change is not made until it is merged and used in production.

    A good PR should be able to flow through a peer review system easily and quickly.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/124'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
  relationships:
    next: '20'
    parent: '18'
- node_id: '20'
  node_depth: 3
  node_type: Node
  content_length: 712
  content: |-
    #### Ensure there is a solid title and summary

    PRs are a Github workflow tool, so it's important to understand that the PR title, summary and eventual discussion are not as trackable as the the commit history. If we ever move away from Github, we'll likely lose this information.

    That said however, they are a very useful aid in ensuring that PRs are handled quickly and effectively.

    Ensure that your PR title is scannable. People will read through the list of PRs attached to a repo, and must be able to distinguish between them based on title. Include a story/issue reference if possible, so the reviewer can get any extra context. Include a reference to the subsystem affected, if this is a large codebase.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/133'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
    - '#### Ensure there is a solid title and summary'
  relationships:
    children:
    - '21'
    next: '22'
    parent: '18'
    previous: '19'
- node_id: '21'
  node_depth: 4
  node_type: Node
  content_length: 712
  content: |-
    #### Ensure there is a solid title and summary

    PRs are a Github workflow tool, so it's important to understand that the PR title, summary and eventual discussion are not as trackable as the the commit history. If we ever move away from Github, we'll likely lose this information.

    That said however, they are a very useful aid in ensuring that PRs are handled quickly and effectively.

    Ensure that your PR title is scannable. People will read through the list of PRs attached to a repo, and must be able to distinguish between them based on title. Include a story/issue reference if possible, so the reviewer can get any extra context. Include a reference to the subsystem affected, if this is a large codebase.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/133'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
    - '#### Ensure there is a solid title and summary'
  relationships:
    parent: '20'
- node_id: '22'
  node_depth: 3
  node_type: Node
  content_length: 292
  content: |-
    #### Be explicit about the PR status

    If your PR is not fully ready yet for reviews, convert it to a `draft` so people don't waste time reviewing unfinished code, and don't assign anyone as a reviewer.

    Use the proper labels to help people understand your intention with the PR and its scope.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/144'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
    - '#### Be explicit about the PR status'
  relationships:
    children:
    - '23'
    next: '24'
    parent: '18'
    previous: '20'
- node_id: '23'
  node_depth: 4
  node_type: Node
  content_length: 292
  content: |-
    #### Be explicit about the PR status

    If your PR is not fully ready yet for reviews, convert it to a `draft` so people don't waste time reviewing unfinished code, and don't assign anyone as a reviewer.

    Use the proper labels to help people understand your intention with the PR and its scope.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/144'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
    - '#### Be explicit about the PR status'
  relationships:
    parent: '22'
- node_id: '24'
  node_depth: 3
  node_type: Node
  content_length: 244
  content: |-
    #### Keep your branch up-to-date

    Unless there is a good reason not to rebase - typically because more than one person has been working on the branch - it is often a good idea to rebase your branch with the latest `main` to make reviews easier.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/151'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
    - '#### Keep your branch up-to-date'
  relationships:
    next: '25'
    parent: '18'
    previous: '22'
- node_id: '25'
  node_depth: 3
  node_type: Node
  content_length: 552
  content: |-
    #### Keep it small

    Try to only fix one issue or add one feature within the pull request. The larger it is, the more complex it is to review and the more likely it will be delayed. Remember that reviewing PRs is taking time from someone else's day.

    If you must submit a large PR, try to at least make someone else aware of this fact, and arrange for their time to review and get the PR merged. It's not fair to the team to dump large pieces of work on their laps without warning.

    If you can rebase up a large PR into multiple smaller PRs, then do so.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/157'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
    - '#### Keep it small'
  relationships:
    children:
    - '26'
    parent: '18'
    previous: '24'
- node_id: '26'
  node_depth: 4
  node_type: Node
  content_length: 552
  content: |-
    #### Keep it small

    Try to only fix one issue or add one feature within the pull request. The larger it is, the more complex it is to review and the more likely it will be delayed. Remember that reviewing PRs is taking time from someone else's day.

    If you must submit a large PR, try to at least make someone else aware of this fact, and arrange for their time to review and get the PR merged. It's not fair to the team to dump large pieces of work on their laps without warning.

    If you can rebase up a large PR into multiple smaller PRs, then do so.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/157'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
    - '#### Keep it small'
  relationships:
    parent: '25'
- node_id: '27'
  node_depth: 1
  node_type: Node
  content_length: 2450
  content: |-
    ## Reviewing Pull Requests

    It's a reviewers responsibility to ensure:

    - Commit history is excellent
    - Good changes are propagated quickly
    - Code review is performed
    - They understand what is being changed, from the perspective of someone examining the code in the future.

    ### Keep the flow going

    Pull Requests are the fundamental unit of how we progress change. If PRs are getting clogged up in the system, either unreviewed or unmanaged, they are preventing a piece of work from being completed.

    As PRs clog up in the system, merges become more difficult, as other features and fixes are applied to the same codebase. This in turn slows them down further, and often completely blocks progress on a given codebase.

    There is a balance between flow and ensuring the quality of our PRs. As a reviewer you should make a call as to whether a code quality issue is sufficient enough to block the PR whilst the code is improved. Possibly it is more prudent to simply flag that the code needs rework, a...
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/165'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
  relationships:
    children:
    - '28'
    - '29'
    - '31'
    - '33'
    parent: '0'
    previous: '14'
- node_id: '28'
  node_depth: 2
  node_type: Node
  content_length: 273
  content: |-
    ## Reviewing Pull Requests

    It's a reviewers responsibility to ensure:

    - Commit history is excellent
    - Good changes are propagated quickly
    - Code review is performed
    - They understand what is being changed, from the perspective of someone examining the code in the future.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/165'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
  relationships:
    next: '29'
    parent: '27'
- node_id: '29'
  node_depth: 2
  node_type: Node
  content_length: 815
  content: |-
    ### Keep the flow going

    Pull Requests are the fundamental unit of how we progress change. If PRs are getting clogged up in the system, either unreviewed or unmanaged, they are preventing a piece of work from being completed.

    As PRs clog up in the system, merges become more difficult, as other features and fixes are applied to the same codebase. This in turn slows them down further, and often completely blocks progress on a given codebase.

    There is a balance between flow and ensuring the quality of our PRs. As a reviewer you should make a call as to whether a code quality issue is sufficient enough to block the PR whilst the code is improved. Possibly it is more prudent to simply flag that the code needs rework, and raise an issue.

    Any quality issue that will obviously result in a bug should be fixed.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/171'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
    - '### Keep the flow going'
  relationships:
    children:
    - '30'
    next: '31'
    parent: '27'
    previous: '28'
- node_id: '30'
  node_depth: 3
  node_type: Node
  content_length: 815
  content: |-
    ### Keep the flow going

    Pull Requests are the fundamental unit of how we progress change. If PRs are getting clogged up in the system, either unreviewed or unmanaged, they are preventing a piece of work from being completed.

    As PRs clog up in the system, merges become more difficult, as other features and fixes are applied to the same codebase. This in turn slows them down further, and often completely blocks progress on a given codebase.

    There is a balance between flow and ensuring the quality of our PRs. As a reviewer you should make a call as to whether a code quality issue is sufficient enough to block the PR whilst the code is improved. Possibly it is more prudent to simply flag that the code needs rework, and raise an issue.

    Any quality issue that will obviously result in a bug should be fixed.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/171'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
    - '### Keep the flow going'
  relationships:
    parent: '29'
- node_id: '31'
  node_depth: 2
  node_type: Node
  content_length: 753
  content: |-
    ### We are all reviewers

    To make sure PRs flow through the system speedily, we must scale the PR review process. It is not sufficient (or fair!) to expect one or two people to review all PRs to our code. For starters, it creates a blocker every time those people are busy.

    Hopefully with the above guidelines, we can all start sharing the responsibility of being a reviewer.

    NB: With this in mind - if you are the first to comment on a PR, you are that PRs reviewer. If you feel that you can no longer be responsible for the subsequent merge or closure of the PR, then flag this up in the PR conversation, so someone else can take up the role.

    There's no reason why multiple people cannot comment on a PR and review it, and this is to be encouraged.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/183'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
    - '### We are all reviewers'
  relationships:
    children:
    - '32'
    next: '33'
    parent: '27'
    previous: '29'
- node_id: '32'
  node_depth: 3
  node_type: Node
  content_length: 753
  content: |-
    ### We are all reviewers

    To make sure PRs flow through the system speedily, we must scale the PR review process. It is not sufficient (or fair!) to expect one or two people to review all PRs to our code. For starters, it creates a blocker every time those people are busy.

    Hopefully with the above guidelines, we can all start sharing the responsibility of being a reviewer.

    NB: With this in mind - if you are the first to comment on a PR, you are that PRs reviewer. If you feel that you can no longer be responsible for the subsequent merge or closure of the PR, then flag this up in the PR conversation, so someone else can take up the role.

    There's no reason why multiple people cannot comment on a PR and review it, and this is to be encouraged.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/183'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
    - '### We are all reviewers'
  relationships:
    parent: '31'
- node_id: '33'
  node_depth: 2
  node_type: Node
  content_length: 603
  content: |-
    ### Don't add to the PR as a reviewer

    It's sometimes tempting to fix a bug in a PR yourself, or to rework a section to meet coding standards, or just to make a feature better fit your needs.

    If you do this, you are no longer the reviewer of the PR. You are a collaborator, and so should not merge the PR.

    It is of course possible to find a new reviewer, but generally change will be speedier if you require the original submitter to fix the code themselves. Alternatively, if the original PR is 'good enough', raise the changes you'd like to see as separate stories/issues, and rework in your own PR.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/195'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
    - '### Don''t add to the PR as a reviewer'
  relationships:
    children:
    - '34'
    parent: '27'
    previous: '31'
- node_id: '34'
  node_depth: 3
  node_type: Node
  content_length: 603
  content: |-
    ### Don't add to the PR as a reviewer

    It's sometimes tempting to fix a bug in a PR yourself, or to rework a section to meet coding standards, or just to make a feature better fit your needs.

    If you do this, you are no longer the reviewer of the PR. You are a collaborator, and so should not merge the PR.

    It is of course possible to find a new reviewer, but generally change will be speedier if you require the original submitter to fix the code themselves. Alternatively, if the original PR is 'good enough', raise the changes you'd like to see as separate stories/issues, and rework in your own PR.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/195'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
    - '### Don''t add to the PR as a reviewer'
  relationships:
    parent: '33'
