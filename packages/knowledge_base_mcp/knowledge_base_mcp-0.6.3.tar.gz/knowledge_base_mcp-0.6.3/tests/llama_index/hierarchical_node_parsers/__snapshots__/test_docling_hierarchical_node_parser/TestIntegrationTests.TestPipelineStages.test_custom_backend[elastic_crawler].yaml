- node_id: '0'
  node_depth: 0
  node_type: Node
  content_length: 10648
  content: "# How to contribute elastic-crawler\n\nThe `elastic-crawler` repository\
    \ is a free and open project, and we love to receive contributions from our community\
    \ \u2014 you! There are many ways to contribute, from writing tutorials or blog\
    \ posts, improving the documentation, submitting bug reports and feature requests\
    \ or writing code which can be incorporated into `elastic-crawler` itself.\n\n\
    If you want to be rewarded for your contributions, sign up for the Elastic Contributor\
    \ Program . Each time you make a valid contribution, you\u2019ll earn points that\
    \ increase your chances of winning prizes and being recognized as a top contributor.\n\
    \n- Reporting issues\n- Getting help\nTypes of contribution\n    - Enhancements\n\
    Contribution Checklist\n    - Acceptance criteria\n    - Correct code organization\n\
    \    - Log verbosity\n    - Linting\n    - Testing\n    - Backport labels\nPull\
    \ request etiquette\n    - Why do we use a pull request workflow?\n    What constitutes\
    \ a good PR?\n        - Ensure there is a solid title and sum..."
  metadata:
    docling_label: title
    docling_ref: '#/texts/0'
    headings:
    - '# How to contribute elastic-crawler'
  relationships:
    children:
    - '1'
    - '2'
    - '3'
    - '4'
    - '5'
    - '6'
    - '7'
    - '8'
    - '31'
    - '62'
- node_id: '1'
  node_depth: 1
  node_type: Node
  content_length: 35
  content: '# How to contribute elastic-crawler'
  metadata:
    docling_label: title
    docling_ref: '#/texts/0'
    headings:
    - '# How to contribute elastic-crawler'
  relationships:
    next: '2'
    parent: '0'
- node_id: '2'
  node_depth: 1
  node_type: Node
  content_length: 342
  content: "The `elastic-crawler` repository is a free and open project, and we love\
    \ to receive contributions from our community \u2014 you! There are many ways\
    \ to contribute, from writing tutorials or blog posts, improving the documentation,\
    \ submitting bug reports and feature requests or writing code which can be incorporated\
    \ into `elastic-crawler` itself."
  metadata:
    docling_label: inline
    docling_ref: '#/groups/0'
    headings:
    - '# How to contribute elastic-crawler'
  relationships:
    next: '3'
    parent: '0'
    previous: '1'
- node_id: '3'
  node_depth: 1
  node_type: Node
  content_length: 244
  content: "If you want to be rewarded for your contributions, sign up for the Elastic\
    \ Contributor Program . Each time you make a valid contribution, you\u2019ll earn\
    \ points that increase your chances of winning prizes and being recognized as\
    \ a top contributor."
  metadata:
    docling_label: inline
    docling_ref: '#/groups/1'
    headings:
    - '# How to contribute elastic-crawler'
  relationships:
    next: '4'
    parent: '0'
    previous: '2'
- node_id: '4'
  node_depth: 1
  node_type: Node
  content_length: 598
  content: |-
    - Reporting issues
    - Getting help
    Types of contribution
        - Enhancements
    Contribution Checklist
        - Acceptance criteria
        - Correct code organization
        - Log verbosity
        - Linting
        - Testing
        - Backport labels
    Pull request etiquette
        - Why do we use a pull request workflow?
        What constitutes a good PR?
            - Ensure there is a solid title and summary
            - Be explicit about the PR status
            - Keep your branch up-to-date
            - Keep it small
    Reviewing Pull Requests
        - Keep the flow going
        - We are all reviewers
        - Don't add to the PR as a reviewer
  metadata:
    docling_label: list
    docling_ref: '#/groups/2'
    headings:
    - '# How to contribute elastic-crawler'
  relationships:
    next: '5'
    parent: '0'
    previous: '3'
- node_id: '5'
  node_depth: 1
  node_type: Node
  content_length: 84
  content: |-
    ## Reporting issues

    If something is not working as expected, please open an issue .
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/33'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reporting issues'
  relationships:
    next: '6'
    parent: '0'
    previous: '4'
- node_id: '6'
  node_depth: 1
  node_type: Node
  content_length: 343
  content: |-
    ## Getting help

    The Ingestion team at Elastic maintains this repository and is happy to help. Try posting your question to the Elastic discuss forums . Be sure to mention that you're using the Elastic Crawler and any errors/issues you are encountering. You can also find us in the `#search-enterprise` channel of the Elastic Community Slack .
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/37'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Getting help'
  relationships:
    next: '7'
    parent: '0'
    previous: '5'
- node_id: '7'
  node_depth: 1
  node_type: Node
  content_length: 437
  content: "## Types of contribution\n\n### Enhancements\n\nEnhancements that can\
    \ be done after your initial contribution:\n\n1. Ensure the backend meets performance\
    \ requirements we might request (memory usage, how fast it syncs 10k docs, etc.)\n\
    2. Update the README for the Elastic Crawler\n3. Small functional improvements\
    \ for Elastic Crawler\n\n\u2139\uFE0F Use-case specific customizations (as opposed\
    \ to generic enhancements) will not be accepted as contributions."
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/48'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Types of contribution'
  relationships:
    next: '8'
    parent: '0'
    previous: '6'
- node_id: '8'
  node_depth: 1
  node_type: Node
  content_length: 2646
  content: |-
    ## Contribution Checklist

    ### Acceptance criteria

    All patch changes should have a corresponding GitHub issue filed within the repository. If you need changes that are complex, or you are not sure about how to do something, reach out to the Ingestion team and/or file an issue.

    ### Correct code/file organization

    Any contribution should follow established patterns of code organization within the repository. For example, a new concrete extension of `CrawlResult` should live in lib/crawler/data/crawl_result , and its tests should live in lib/crawler/data/crawl_result . If that new data source is named `FooResult` the file and spec file should be `foo_result.rb` and `foo_result_spec.rb` , respectively. It should also inherit any `Base` class within the module.

    If you are unsure of where a file/class should go - ask.

    ### Log verbosity

    Logging is important to get insights on what's happening in the service. However, we should be careful not to pile up logs that are not adding value in o...
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/55'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
  relationships:
    children:
    - '9'
    - '10'
    - '11'
    - '15'
    - '20'
    - '24'
    - '30'
    next: '31'
    parent: '0'
    previous: '7'
- node_id: '9'
  node_depth: 2
  node_type: Node
  content_length: 25
  content: '## Contribution Checklist'
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/55'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
  relationships:
    next: '10'
    parent: '8'
- node_id: '10'
  node_depth: 2
  node_type: Node
  content_length: 251
  content: |-
    ### Acceptance criteria

    All patch changes should have a corresponding GitHub issue filed within the repository. If you need changes that are complex, or you are not sure about how to do something, reach out to the Ingestion team and/or file an issue.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/56'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Acceptance criteria'
  relationships:
    next: '11'
    parent: '8'
    previous: '9'
- node_id: '11'
  node_depth: 2
  node_type: Node
  content_length: 546
  content: |-
    ### Correct code/file organization

    Any contribution should follow established patterns of code organization within the repository. For example, a new concrete extension of `CrawlResult` should live in lib/crawler/data/crawl_result , and its tests should live in lib/crawler/data/crawl_result . If that new data source is named `FooResult` the file and spec file should be `foo_result.rb` and `foo_result_spec.rb` , respectively. It should also inherit any `Base` class within the module.

    If you are unsure of where a file/class should go - ask.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/61'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Correct code/file organization'
  relationships:
    children:
    - '12'
    - '13'
    - '14'
    next: '15'
    parent: '8'
    previous: '10'
- node_id: '12'
  node_depth: 3
  node_type: Node
  content_length: 34
  content: '### Correct code/file organization'
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/61'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Correct code/file organization'
  relationships:
    next: '13'
    parent: '11'
- node_id: '13'
  node_depth: 3
  node_type: Node
  content_length: 452
  content: Any contribution should follow established patterns of code organization
    within the repository. For example, a new concrete extension of `CrawlResult`
    should live in lib/crawler/data/crawl_result , and its tests should live in lib/crawler/data/crawl_result
    . If that new data source is named `FooResult` the file and spec file should be
    `foo_result.rb` and `foo_result_spec.rb` , respectively. It should also inherit
    any `Base` class within the module.
  metadata:
    docling_label: inline
    docling_ref: '#/groups/12'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Correct code/file organization'
  relationships:
    next: '14'
    parent: '11'
    previous: '12'
- node_id: '14'
  node_depth: 3
  node_type: Node
  content_length: 56
  content: If you are unsure of where a file/class should go - ask.
  metadata:
    docling_label: text
    docling_ref: '#/texts/80'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Correct code/file organization'
  relationships:
    parent: '11'
    previous: '13'
- node_id: '15'
  node_depth: 2
  node_type: Node
  content_length: 753
  content: |-
    ### Log verbosity

    Logging is important to get insights on what's happening in the service. However, we should be careful not to pile up logs that are not adding value in our systems.

    A few tips per log level:

    - CRITICAL (50) -- anything that stops the service, if you want to add more details on why.
    - ERROR (40) -- all Python exceptions will use that level, but you can call it specifically to add details
    - WARNING (30) -- any unexpected behavior that the system knows how to handle, but that should be notified (network retries, deprecation)
    - INFO (20) -- normal operations feedback. These should not be verbose so we don't pile logs overtime for nothing.
    - DEBUG (10) -- like info but with as many details as possible
    - NOTSET (0) -- never used
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/81'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Log verbosity'
  relationships:
    children:
    - '16'
    - '17'
    - '18'
    - '19'
    next: '20'
    parent: '8'
    previous: '11'
- node_id: '16'
  node_depth: 3
  node_type: Node
  content_length: 17
  content: '### Log verbosity'
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/81'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Log verbosity'
  relationships:
    next: '17'
    parent: '15'
- node_id: '17'
  node_depth: 3
  node_type: Node
  content_length: 164
  content: Logging is important to get insights on what's happening in the service.
    However, we should be careful not to pile up logs that are not adding value in
    our systems.
  metadata:
    docling_label: inline
    docling_ref: '#/groups/13'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Log verbosity'
  relationships:
    next: '18'
    parent: '15'
    previous: '16'
- node_id: '18'
  node_depth: 3
  node_type: Node
  content_length: 25
  content: 'A few tips per log level:'
  metadata:
    docling_label: text
    docling_ref: '#/texts/84'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Log verbosity'
  relationships:
    next: '19'
    parent: '15'
    previous: '17'
- node_id: '19'
  node_depth: 3
  node_type: Node
  content_length: 541
  content: |-
    - CRITICAL (50) -- anything that stops the service, if you want to add more details on why.
    - ERROR (40) -- all Python exceptions will use that level, but you can call it specifically to add details
    - WARNING (30) -- any unexpected behavior that the system knows how to handle, but that should be notified (network retries, deprecation)
    - INFO (20) -- normal operations feedback. These should not be verbose so we don't pile logs overtime for nothing.
    - DEBUG (10) -- like info but with as many details as possible
    - NOTSET (0) -- never used
  metadata:
    docling_label: list
    docling_ref: '#/groups/14'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Log verbosity'
  relationships:
    parent: '15'
    previous: '18'
- node_id: '20'
  node_depth: 2
  node_type: Node
  content_length: 407
  content: |-
    ### Linting

    Code style is important in shared codebases, as it helps ensure that everyone can read and understand code that they didn't write. In order to enforce code style, our CI jobs apply a linter (rubocop), and will fail to build (and block merging of) non-compliant changes.

    You can run the linter locally with `./script/bundle exec rubocop` to ensure that your changes do not introduce any issues.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/91'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Linting'
  relationships:
    children:
    - '21'
    - '22'
    - '23'
    next: '24'
    parent: '8'
    previous: '15'
- node_id: '21'
  node_depth: 3
  node_type: Node
  content_length: 11
  content: '### Linting'
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/91'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Linting'
  relationships:
    next: '22'
    parent: '20'
- node_id: '22'
  node_depth: 3
  node_type: Node
  content_length: 269
  content: Code style is important in shared codebases, as it helps ensure that everyone
    can read and understand code that they didn't write. In order to enforce code
    style, our CI jobs apply a linter (rubocop), and will fail to build (and block
    merging of) non-compliant changes.
  metadata:
    docling_label: inline
    docling_ref: '#/groups/15'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Linting'
  relationships:
    next: '23'
    parent: '20'
    previous: '21'
- node_id: '23'
  node_depth: 3
  node_type: Node
  content_length: 123
  content: You can run the linter locally with `./script/bundle exec rubocop` to ensure
    that your changes do not introduce any issues.
  metadata:
    docling_label: inline
    docling_ref: '#/groups/16'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Linting'
  relationships:
    parent: '20'
    previous: '22'
- node_id: '24'
  node_depth: 2
  node_type: Node
  content_length: 525
  content: |-
    ### Testing

    Tests not only verify and demonstrate that a new feature does what it is supposed to, but they also protect the codebase from unintentional future regressions. For this reason, it is important to both add tests when contributing new code, and to ensure that all tests (old and new) are passing.

    Our goal is to maintain 92% test coverage for the Elastic Crawler.

    You can run the tests locally with `./script/bundle rspec <file-path>` .

    Be sure to read about our unit tests and integration tests . # TODO: check
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/97'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Testing'
  relationships:
    children:
    - '25'
    - '26'
    - '27'
    - '28'
    - '29'
    next: '30'
    parent: '8'
    previous: '20'
- node_id: '25'
  node_depth: 3
  node_type: Node
  content_length: 11
  content: '### Testing'
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/97'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Testing'
  relationships:
    next: '26'
    parent: '24'
- node_id: '26'
  node_depth: 3
  node_type: Node
  content_length: 294
  content: Tests not only verify and demonstrate that a new feature does what it is
    supposed to, but they also protect the codebase from unintentional future regressions.
    For this reason, it is important to both add tests when contributing new code,
    and to ensure that all tests (old and new) are passing.
  metadata:
    docling_label: inline
    docling_ref: '#/groups/17'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Testing'
  relationships:
    next: '27'
    parent: '24'
    previous: '25'
- node_id: '27'
  node_depth: 3
  node_type: Node
  content_length: 66
  content: Our goal is to maintain 92% test coverage for the Elastic Crawler.
  metadata:
    docling_label: text
    docling_ref: '#/texts/100'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Testing'
  relationships:
    next: '28'
    parent: '24'
    previous: '26'
- node_id: '28'
  node_depth: 3
  node_type: Node
  content_length: 72
  content: You can run the tests locally with `./script/bundle rspec <file-path>`
    .
  metadata:
    docling_label: inline
    docling_ref: '#/groups/18'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Testing'
  relationships:
    next: '29'
    parent: '24'
    previous: '27'
- node_id: '29'
  node_depth: 3
  node_type: Node
  content_length: 74
  content: 'Be sure to read about our unit tests and integration tests . # TODO: check'
  metadata:
    docling_label: inline
    docling_ref: '#/groups/19'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Testing'
  relationships:
    parent: '24'
    previous: '28'
- node_id: '30'
  node_depth: 2
  node_type: Node
  content_length: 127
  content: |-
    ### Backport labels

    Make sure to include the appropriate backport labels, if your PR needs to be backported to a past version.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/109'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Contribution Checklist'
    - '### Backport labels'
  relationships:
    parent: '8'
    previous: '24'
- node_id: '31'
  node_depth: 1
  node_type: Node
  content_length: 3451
  content: |-
    ## Pull Request Etiquette

    *this is copied and adapted from https://gist.github.com/mikepea/863f63d6e37281e329f8*

    ### Why do we use a Pull Request workflow?

    PRs are a great way of sharing information, and can help us be aware of the changes that are occurring in our codebase. They are also an excellent way of getting peer review on the work that we do, without the cost of working in direct pairs.

    **Ultimately though, the primary reason we use PRs is to encourage quality in**

    **the commits that are made to our code repositories**

    Done well, the commits (and their attached messages) contained within tell a story to people examining the code at a later date. If we are not careful to ensure the quality of these commits, we silently lose this ability.

    **Poor quality code can be refactored. A terrible commit lasts forever.**

    ### What constitutes a good PR?

    A good quality PR will have the following characteristics:

    - It will be a complete piece of work that adds value in some way.
    - ...
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/111'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
  relationships:
    children:
    - '32'
    - '33'
    - '34'
    - '41'
    next: '62'
    parent: '0'
    previous: '8'
- node_id: '32'
  node_depth: 2
  node_type: Node
  content_length: 25
  content: '## Pull Request Etiquette'
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/111'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
  relationships:
    next: '33'
    parent: '31'
- node_id: '33'
  node_depth: 2
  node_type: Node
  content_length: 86
  content: '*this is copied and adapted from https://gist.github.com/mikepea/863f63d6e37281e329f8*'
  metadata:
    docling_label: text
    docling_ref: '#/texts/112'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
  relationships:
    next: '34'
    parent: '31'
    previous: '32'
- node_id: '34'
  node_depth: 2
  node_type: Node
  content_length: 721
  content: |-
    ### Why do we use a Pull Request workflow?

    PRs are a great way of sharing information, and can help us be aware of the changes that are occurring in our codebase. They are also an excellent way of getting peer review on the work that we do, without the cost of working in direct pairs.

    **Ultimately though, the primary reason we use PRs is to encourage quality in**

    **the commits that are made to our code repositories**

    Done well, the commits (and their attached messages) contained within tell a story to people examining the code at a later date. If we are not careful to ensure the quality of these commits, we silently lose this ability.

    **Poor quality code can be refactored. A terrible commit lasts forever.**
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/113'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### Why do we use a Pull Request workflow?'
  relationships:
    children:
    - '35'
    - '36'
    - '37'
    - '38'
    - '39'
    - '40'
    next: '41'
    parent: '31'
    previous: '33'
- node_id: '35'
  node_depth: 3
  node_type: Node
  content_length: 42
  content: '### Why do we use a Pull Request workflow?'
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/113'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### Why do we use a Pull Request workflow?'
  relationships:
    next: '36'
    parent: '34'
- node_id: '36'
  node_depth: 3
  node_type: Node
  content_length: 242
  content: PRs are a great way of sharing information, and can help us be aware of
    the changes that are occurring in our codebase. They are also an excellent way
    of getting peer review on the work that we do, without the cost of working in
    direct pairs.
  metadata:
    docling_label: inline
    docling_ref: '#/groups/20'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### Why do we use a Pull Request workflow?'
  relationships:
    next: '37'
    parent: '34'
    previous: '35'
- node_id: '37'
  node_depth: 3
  node_type: Node
  content_length: 79
  content: '**Ultimately though, the primary reason we use PRs is to encourage quality
    in**'
  metadata:
    docling_label: text
    docling_ref: '#/texts/118'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### Why do we use a Pull Request workflow?'
  relationships:
    next: '38'
    parent: '34'
    previous: '36'
- node_id: '38'
  node_depth: 3
  node_type: Node
  content_length: 54
  content: '**the commits that are made to our code repositories**'
  metadata:
    docling_label: text
    docling_ref: '#/texts/119'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### Why do we use a Pull Request workflow?'
  relationships:
    next: '39'
    parent: '34'
    previous: '37'
- node_id: '39'
  node_depth: 3
  node_type: Node
  content_length: 221
  content: Done well, the commits (and their attached messages) contained within tell
    a story to people examining the code at a later date. If we are not careful to
    ensure the quality of these commits, we silently lose this ability.
  metadata:
    docling_label: inline
    docling_ref: '#/groups/21'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### Why do we use a Pull Request workflow?'
  relationships:
    next: '40'
    parent: '34'
    previous: '38'
- node_id: '40'
  node_depth: 3
  node_type: Node
  content_length: 73
  content: '**Poor quality code can be refactored. A terrible commit lasts forever.**'
  metadata:
    docling_label: text
    docling_ref: '#/texts/123'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### Why do we use a Pull Request workflow?'
  relationships:
    parent: '34'
    previous: '39'
- node_id: '41'
  node_depth: 2
  node_type: Node
  content_length: 2613
  content: |-
    ### What constitutes a good PR?

    A good quality PR will have the following characteristics:

    - It will be a complete piece of work that adds value in some way.
    - It will have a title that reflects the work within, and a summary that helps to understand the context of the change.
    - There will be well written commit messages, with well crafted commits that tell the story of the development of this work.
    - Ideally it will be small and easy to understand. Single commit PRs are usually easy to submit, review, and merge.
    - The code contained within will meet the best practises set by the team wherever possible.

    A PR does not end at submission though. A code change is not made until it is merged and used in production.

    A good PR should be able to flow through a peer review system easily and quickly.

    #### Ensure there is a solid title and summary

    PRs are a Github workflow tool, so it's important to understand that the PR title, summary and eventual discussion are not as trackable as the th...
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/124'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
  relationships:
    children:
    - '42'
    - '43'
    - '44'
    - '45'
    - '46'
    - '47'
    - '52'
    - '56'
    - '57'
    parent: '31'
    previous: '34'
- node_id: '42'
  node_depth: 3
  node_type: Node
  content_length: 31
  content: '### What constitutes a good PR?'
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/124'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
  relationships:
    next: '43'
    parent: '41'
- node_id: '43'
  node_depth: 3
  node_type: Node
  content_length: 58
  content: 'A good quality PR will have the following characteristics:'
  metadata:
    docling_label: text
    docling_ref: '#/texts/125'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
  relationships:
    next: '44'
    parent: '41'
    previous: '42'
- node_id: '44'
  node_depth: 3
  node_type: Node
  content_length: 519
  content: |-
    - It will be a complete piece of work that adds value in some way.
    - It will have a title that reflects the work within, and a summary that helps to understand the context of the change.
    - There will be well written commit messages, with well crafted commits that tell the story of the development of this work.
    - Ideally it will be small and easy to understand. Single commit PRs are usually easy to submit, review, and merge.
    - The code contained within will meet the best practises set by the team wherever possible.
  metadata:
    docling_label: list
    docling_ref: '#/groups/22'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
  relationships:
    next: '45'
    parent: '41'
    previous: '43'
- node_id: '45'
  node_depth: 3
  node_type: Node
  content_length: 108
  content: A PR does not end at submission though. A code change is not made until
    it is merged and used in production.
  metadata:
    docling_label: text
    docling_ref: '#/texts/131'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
  relationships:
    next: '46'
    parent: '41'
    previous: '44'
- node_id: '46'
  node_depth: 3
  node_type: Node
  content_length: 81
  content: A good PR should be able to flow through a peer review system easily and
    quickly.
  metadata:
    docling_label: text
    docling_ref: '#/texts/132'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
  relationships:
    next: '47'
    parent: '41'
    previous: '45'
- node_id: '47'
  node_depth: 3
  node_type: Node
  content_length: 712
  content: |-
    #### Ensure there is a solid title and summary

    PRs are a Github workflow tool, so it's important to understand that the PR title, summary and eventual discussion are not as trackable as the the commit history. If we ever move away from Github, we'll likely lose this information.

    That said however, they are a very useful aid in ensuring that PRs are handled quickly and effectively.

    Ensure that your PR title is scannable. People will read through the list of PRs attached to a repo, and must be able to distinguish between them based on title. Include a story/issue reference if possible, so the reviewer can get any extra context. Include a reference to the subsystem affected, if this is a large codebase.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/133'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
    - '#### Ensure there is a solid title and summary'
  relationships:
    children:
    - '48'
    - '49'
    - '50'
    - '51'
    next: '52'
    parent: '41'
    previous: '46'
- node_id: '48'
  node_depth: 4
  node_type: Node
  content_length: 46
  content: '#### Ensure there is a solid title and summary'
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/133'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
    - '#### Ensure there is a solid title and summary'
  relationships:
    next: '49'
    parent: '47'
- node_id: '49'
  node_depth: 4
  node_type: Node
  content_length: 232
  content: PRs are a Github workflow tool, so it's important to understand that the
    PR title, summary and eventual discussion are not as trackable as the the commit
    history. If we ever move away from Github, we'll likely lose this information.
  metadata:
    docling_label: inline
    docling_ref: '#/groups/23'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
    - '#### Ensure there is a solid title and summary'
  relationships:
    next: '50'
    parent: '47'
    previous: '48'
- node_id: '50'
  node_depth: 4
  node_type: Node
  content_length: 103
  content: That said however, they are a very useful aid in ensuring that PRs are
    handled quickly and effectively.
  metadata:
    docling_label: inline
    docling_ref: '#/groups/24'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
    - '#### Ensure there is a solid title and summary'
  relationships:
    next: '51'
    parent: '47'
    previous: '49'
- node_id: '51'
  node_depth: 4
  node_type: Node
  content_length: 325
  content: Ensure that your PR title is scannable. People will read through the list
    of PRs attached to a repo, and must be able to distinguish between them based
    on title. Include a story/issue reference if possible, so the reviewer can get
    any extra context. Include a reference to the subsystem affected, if this is a
    large codebase.
  metadata:
    docling_label: inline
    docling_ref: '#/groups/25'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
    - '#### Ensure there is a solid title and summary'
  relationships:
    parent: '47'
    previous: '50'
- node_id: '52'
  node_depth: 3
  node_type: Node
  content_length: 292
  content: |-
    #### Be explicit about the PR status

    If your PR is not fully ready yet for reviews, convert it to a `draft` so people don't waste time reviewing unfinished code, and don't assign anyone as a reviewer.

    Use the proper labels to help people understand your intention with the PR and its scope.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/144'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
    - '#### Be explicit about the PR status'
  relationships:
    children:
    - '53'
    - '54'
    - '55'
    next: '56'
    parent: '41'
    previous: '47'
- node_id: '53'
  node_depth: 4
  node_type: Node
  content_length: 36
  content: '#### Be explicit about the PR status'
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/144'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
    - '#### Be explicit about the PR status'
  relationships:
    next: '54'
    parent: '52'
- node_id: '54'
  node_depth: 4
  node_type: Node
  content_length: 163
  content: If your PR is not fully ready yet for reviews, convert it to a `draft`
    so people don't waste time reviewing unfinished code, and don't assign anyone
    as a reviewer.
  metadata:
    docling_label: inline
    docling_ref: '#/groups/26'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
    - '#### Be explicit about the PR status'
  relationships:
    next: '55'
    parent: '52'
    previous: '53'
- node_id: '55'
  node_depth: 4
  node_type: Node
  content_length: 89
  content: Use the proper labels to help people understand your intention with the
    PR and its scope.
  metadata:
    docling_label: inline
    docling_ref: '#/groups/27'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
    - '#### Be explicit about the PR status'
  relationships:
    parent: '52'
    previous: '54'
- node_id: '56'
  node_depth: 3
  node_type: Node
  content_length: 244
  content: |-
    #### Keep your branch up-to-date

    Unless there is a good reason not to rebase - typically because more than one person has been working on the branch - it is often a good idea to rebase your branch with the latest `main` to make reviews easier.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/151'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
    - '#### Keep your branch up-to-date'
  relationships:
    next: '57'
    parent: '41'
    previous: '52'
- node_id: '57'
  node_depth: 3
  node_type: Node
  content_length: 552
  content: |-
    #### Keep it small

    Try to only fix one issue or add one feature within the pull request. The larger it is, the more complex it is to review and the more likely it will be delayed. Remember that reviewing PRs is taking time from someone else's day.

    If you must submit a large PR, try to at least make someone else aware of this fact, and arrange for their time to review and get the PR merged. It's not fair to the team to dump large pieces of work on their laps without warning.

    If you can rebase up a large PR into multiple smaller PRs, then do so.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/157'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
    - '#### Keep it small'
  relationships:
    children:
    - '58'
    - '59'
    - '60'
    - '61'
    parent: '41'
    previous: '56'
- node_id: '58'
  node_depth: 4
  node_type: Node
  content_length: 18
  content: '#### Keep it small'
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/157'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
    - '#### Keep it small'
  relationships:
    next: '59'
    parent: '57'
- node_id: '59'
  node_depth: 4
  node_type: Node
  content_length: 228
  content: Try to only fix one issue or add one feature within the pull request. The
    larger it is, the more complex it is to review and the more likely it will be
    delayed. Remember that reviewing PRs is taking time from someone else's day.
  metadata:
    docling_label: inline
    docling_ref: '#/groups/29'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
    - '#### Keep it small'
  relationships:
    next: '60'
    parent: '57'
    previous: '58'
- node_id: '60'
  node_depth: 4
  node_type: Node
  content_length: 230
  content: If you must submit a large PR, try to at least make someone else aware
    of this fact, and arrange for their time to review and get the PR merged. It's
    not fair to the team to dump large pieces of work on their laps without warning.
  metadata:
    docling_label: inline
    docling_ref: '#/groups/30'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
    - '#### Keep it small'
  relationships:
    next: '61'
    parent: '57'
    previous: '59'
- node_id: '61'
  node_depth: 4
  node_type: Node
  content_length: 70
  content: If you can rebase up a large PR into multiple smaller PRs, then do so.
  metadata:
    docling_label: text
    docling_ref: '#/texts/164'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Pull Request Etiquette'
    - '### What constitutes a good PR?'
    - '#### Keep it small'
  relationships:
    parent: '57'
    previous: '60'
- node_id: '62'
  node_depth: 1
  node_type: Node
  content_length: 2450
  content: |-
    ## Reviewing Pull Requests

    It's a reviewers responsibility to ensure:

    - Commit history is excellent
    - Good changes are propagated quickly
    - Code review is performed
    - They understand what is being changed, from the perspective of someone examining the code in the future.

    ### Keep the flow going

    Pull Requests are the fundamental unit of how we progress change. If PRs are getting clogged up in the system, either unreviewed or unmanaged, they are preventing a piece of work from being completed.

    As PRs clog up in the system, merges become more difficult, as other features and fixes are applied to the same codebase. This in turn slows them down further, and often completely blocks progress on a given codebase.

    There is a balance between flow and ensuring the quality of our PRs. As a reviewer you should make a call as to whether a code quality issue is sufficient enough to block the PR whilst the code is improved. Possibly it is more prudent to simply flag that the code needs rework, a...
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/165'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
  relationships:
    children:
    - '63'
    - '64'
    - '65'
    - '66'
    - '72'
    - '78'
    parent: '0'
    previous: '31'
- node_id: '63'
  node_depth: 2
  node_type: Node
  content_length: 26
  content: '## Reviewing Pull Requests'
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/165'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
  relationships:
    next: '64'
    parent: '62'
- node_id: '64'
  node_depth: 2
  node_type: Node
  content_length: 42
  content: 'It''s a reviewers responsibility to ensure:'
  metadata:
    docling_label: text
    docling_ref: '#/texts/166'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
  relationships:
    next: '65'
    parent: '62'
    previous: '63'
- node_id: '65'
  node_depth: 2
  node_type: Node
  content_length: 201
  content: |-
    - Commit history is excellent
    - Good changes are propagated quickly
    - Code review is performed
    - They understand what is being changed, from the perspective of someone examining the code in the future.
  metadata:
    docling_label: list
    docling_ref: '#/groups/31'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
  relationships:
    next: '66'
    parent: '62'
    previous: '64'
- node_id: '66'
  node_depth: 2
  node_type: Node
  content_length: 815
  content: |-
    ### Keep the flow going

    Pull Requests are the fundamental unit of how we progress change. If PRs are getting clogged up in the system, either unreviewed or unmanaged, they are preventing a piece of work from being completed.

    As PRs clog up in the system, merges become more difficult, as other features and fixes are applied to the same codebase. This in turn slows them down further, and often completely blocks progress on a given codebase.

    There is a balance between flow and ensuring the quality of our PRs. As a reviewer you should make a call as to whether a code quality issue is sufficient enough to block the PR whilst the code is improved. Possibly it is more prudent to simply flag that the code needs rework, and raise an issue.

    Any quality issue that will obviously result in a bug should be fixed.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/171'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
    - '### Keep the flow going'
  relationships:
    children:
    - '67'
    - '68'
    - '69'
    - '70'
    - '71'
    next: '72'
    parent: '62'
    previous: '65'
- node_id: '67'
  node_depth: 3
  node_type: Node
  content_length: 23
  content: '### Keep the flow going'
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/171'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
    - '### Keep the flow going'
  relationships:
    next: '68'
    parent: '66'
- node_id: '68'
  node_depth: 3
  node_type: Node
  content_length: 200
  content: Pull Requests are the fundamental unit of how we progress change. If PRs
    are getting clogged up in the system, either unreviewed or unmanaged, they are
    preventing a piece of work from being completed.
  metadata:
    docling_label: inline
    docling_ref: '#/groups/32'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
    - '### Keep the flow going'
  relationships:
    next: '69'
    parent: '66'
    previous: '67'
- node_id: '69'
  node_depth: 3
  node_type: Node
  content_length: 217
  content: As PRs clog up in the system, merges become more difficult, as other features
    and fixes are applied to the same codebase. This in turn slows them down further,
    and often completely blocks progress on a given codebase.
  metadata:
    docling_label: inline
    docling_ref: '#/groups/33'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
    - '### Keep the flow going'
  relationships:
    next: '70'
    parent: '66'
    previous: '68'
- node_id: '70'
  node_depth: 3
  node_type: Node
  content_length: 297
  content: There is a balance between flow and ensuring the quality of our PRs. As
    a reviewer you should make a call as to whether a code quality issue is sufficient
    enough to block the PR whilst the code is improved. Possibly it is more prudent
    to simply flag that the code needs rework, and raise an issue.
  metadata:
    docling_label: inline
    docling_ref: '#/groups/34'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
    - '### Keep the flow going'
  relationships:
    next: '71'
    parent: '66'
    previous: '69'
- node_id: '71'
  node_depth: 3
  node_type: Node
  content_length: 70
  content: Any quality issue that will obviously result in a bug should be fixed.
  metadata:
    docling_label: text
    docling_ref: '#/texts/182'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
    - '### Keep the flow going'
  relationships:
    parent: '66'
    previous: '70'
- node_id: '72'
  node_depth: 2
  node_type: Node
  content_length: 753
  content: |-
    ### We are all reviewers

    To make sure PRs flow through the system speedily, we must scale the PR review process. It is not sufficient (or fair!) to expect one or two people to review all PRs to our code. For starters, it creates a blocker every time those people are busy.

    Hopefully with the above guidelines, we can all start sharing the responsibility of being a reviewer.

    NB: With this in mind - if you are the first to comment on a PR, you are that PRs reviewer. If you feel that you can no longer be responsible for the subsequent merge or closure of the PR, then flag this up in the PR conversation, so someone else can take up the role.

    There's no reason why multiple people cannot comment on a PR and review it, and this is to be encouraged.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/183'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
    - '### We are all reviewers'
  relationships:
    children:
    - '73'
    - '74'
    - '75'
    - '76'
    - '77'
    next: '78'
    parent: '62'
    previous: '66'
- node_id: '73'
  node_depth: 3
  node_type: Node
  content_length: 24
  content: '### We are all reviewers'
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/183'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
    - '### We are all reviewers'
  relationships:
    next: '74'
    parent: '72'
- node_id: '74'
  node_depth: 3
  node_type: Node
  content_length: 247
  content: To make sure PRs flow through the system speedily, we must scale the PR
    review process. It is not sufficient (or fair!) to expect one or two people to
    review all PRs to our code. For starters, it creates a blocker every time those
    people are busy.
  metadata:
    docling_label: inline
    docling_ref: '#/groups/35'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
    - '### We are all reviewers'
  relationships:
    next: '75'
    parent: '72'
    previous: '73'
- node_id: '75'
  node_depth: 3
  node_type: Node
  content_length: 101
  content: Hopefully with the above guidelines, we can all start sharing the responsibility
    of being a reviewer.
  metadata:
    docling_label: text
    docling_ref: '#/texts/188'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
    - '### We are all reviewers'
  relationships:
    next: '76'
    parent: '72'
    previous: '74'
- node_id: '76'
  node_depth: 3
  node_type: Node
  content_length: 268
  content: 'NB: With this in mind - if you are the first to comment on a PR, you are
    that PRs reviewer. If you feel that you can no longer be responsible for the subsequent
    merge or closure of the PR, then flag this up in the PR conversation, so someone
    else can take up the role.'
  metadata:
    docling_label: inline
    docling_ref: '#/groups/36'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
    - '### We are all reviewers'
  relationships:
    next: '77'
    parent: '72'
    previous: '75'
- node_id: '77'
  node_depth: 3
  node_type: Node
  content_length: 105
  content: There's no reason why multiple people cannot comment on a PR and review
    it, and this is to be encouraged.
  metadata:
    docling_label: inline
    docling_ref: '#/groups/37'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
    - '### We are all reviewers'
  relationships:
    parent: '72'
    previous: '76'
- node_id: '78'
  node_depth: 2
  node_type: Node
  content_length: 603
  content: |-
    ### Don't add to the PR as a reviewer

    It's sometimes tempting to fix a bug in a PR yourself, or to rework a section to meet coding standards, or just to make a feature better fit your needs.

    If you do this, you are no longer the reviewer of the PR. You are a collaborator, and so should not merge the PR.

    It is of course possible to find a new reviewer, but generally change will be speedier if you require the original submitter to fix the code themselves. Alternatively, if the original PR is 'good enough', raise the changes you'd like to see as separate stories/issues, and rework in your own PR.
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/195'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
    - '### Don''t add to the PR as a reviewer'
  relationships:
    children:
    - '79'
    - '80'
    - '81'
    - '82'
    parent: '62'
    previous: '72'
- node_id: '79'
  node_depth: 3
  node_type: Node
  content_length: 37
  content: '### Don''t add to the PR as a reviewer'
  metadata:
    docling_label: section_header
    docling_ref: '#/texts/195'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
    - '### Don''t add to the PR as a reviewer'
  relationships:
    next: '80'
    parent: '78'
- node_id: '80'
  node_depth: 3
  node_type: Node
  content_length: 152
  content: It's sometimes tempting to fix a bug in a PR yourself, or to rework a section
    to meet coding standards, or just to make a feature better fit your needs.
  metadata:
    docling_label: inline
    docling_ref: '#/groups/38'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
    - '### Don''t add to the PR as a reviewer'
  relationships:
    next: '81'
    parent: '78'
    previous: '79'
- node_id: '81'
  node_depth: 3
  node_type: Node
  content_length: 113
  content: If you do this, you are no longer the reviewer of the PR. You are a collaborator,
    and so should not merge the PR.
  metadata:
    docling_label: inline
    docling_ref: '#/groups/39'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
    - '### Don''t add to the PR as a reviewer'
  relationships:
    next: '82'
    parent: '78'
    previous: '80'
- node_id: '82'
  node_depth: 3
  node_type: Node
  content_length: 295
  content: It is of course possible to find a new reviewer, but generally change will
    be speedier if you require the original submitter to fix the code themselves.
    Alternatively, if the original PR is 'good enough', raise the changes you'd like
    to see as separate stories/issues, and rework in your own PR.
  metadata:
    docling_label: inline
    docling_ref: '#/groups/40'
    headings:
    - '# How to contribute elastic-crawler'
    - '## Reviewing Pull Requests'
    - '### Don''t add to the PR as a reviewer'
  relationships:
    parent: '78'
    previous: '81'
