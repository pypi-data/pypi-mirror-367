[
  {
    "category": "Pod Management",
    "prompts": [
      {
        "prompt": "Create a pod named nginx using the nginx image and expose port 80. You can use 'kubectl run' or create a YAML file.",
        "validation_steps": [
          {
            "cmd": "#!/bin/bash\nset -e\nPOD_NAME=$(kubectl get pod nginx -o jsonpath='{.metadata.name}' 2>/dev/null)\nif [ \"$POD_NAME\" != \"nginx\" ]; then\n  echo \"Error: Pod 'nginx' not found.\"\n  exit 1\nfi\nIMAGE=$(kubectl get pod nginx -o jsonpath='{.spec.containers[0].image}')\nif [ \"$IMAGE\" != \"nginx\" ]; then\n  echo \"Error: Image is '$IMAGE', expected 'nginx'.\"\n  exit 1\nfi\nPORT=$(kubectl get pod nginx -o jsonpath='{.spec.containers[0].ports[0].containerPort}')\nif [ \"$PORT\" != \"80\" ]; then\n  echo \"Error: Port is '$PORT', expected '80'.\"\n  exit 1\nfi\necho \"Pod 'nginx' with image 'nginx' and port 80 found.\"",
            "matcher": {"exit_code": 0}
          }
        ],
        "explanation": "You can solve this with 'kubectl run nginx --image=nginx --port=80' or by creating a Pod manifest with the specified image and containerPort."
      },
      {
        "prompt": "Create a pod named busybox that runs the command 'sleep 3600'",
        "validation_steps": [
          {"cmd": "kubectl get pod busybox", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Generate a YAML definition file for a pod named nginx without creating it",
        "validation_steps": [
          {"cmd": "[ -f nginx.yaml ] && grep -q 'kind: Pod' nginx.yaml && grep -q 'name: nginx' nginx.yaml", "matcher": {"exit_code": 0}}
        ],
        "explanation": "The key is using `--dry-run=client -o yaml` and redirecting the output to a file."
      },
      {
        "prompt": "Create a pod that runs the latest nginx image and sets an environment variable DB_URL=postgresql://db",
        "validation_steps": [
          {"cmd": "kubectl get pod nginx -o jsonpath='{.spec.containers[0].env[0].value}' | grep 'postgresql://db'", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Extract the YAML definition of a running pod named 'webapp' in the 'development' namespace",
        "pre_shell_cmds": ["kubectl create ns development", "kubectl run webapp -n development --image=nginx"],
        "validation_steps": [
          {"cmd": "[ -f webapp.yaml ] && grep -q 'kind: Pod' webapp.yaml && grep -q 'name: webapp' webapp.yaml", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Create a pod named nginx using the nginx image and set labels app=web and tier=frontend",
        "validation_steps": [
          {"cmd": "kubectl get pod nginx --show-labels | grep 'app=web,tier=frontend'", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Create an interactive temporary pod with Ubuntu image to troubleshoot cluster issues",
        "explanation": "The command 'kubectl run my-shell --rm -i --tty --image=ubuntu -- bash' launches an interactive shell in a new pod and automatically cleans it up on exit.",
        "validation_steps": [
          {"cmd": "echo 'This question is validated by observing user runs the correct command interactively.'", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Create a pod with resource requests of 100m CPU and 256Mi memory",
        "initial_files": {
          "pod.yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n"
        },
        "validation_steps": [
          {"cmd": "kubectl apply -f pod.yaml && kubectl get pod nginx -o jsonpath='{.spec.containers[0].resources.requests.cpu}' | grep '100m' && kubectl get pod nginx -o jsonpath='{.spec.containers[0].resources.requests.memory}' | grep '256Mi'", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Create a pod that runs curl against an internal service at 10.244.0.4",
        "explanation": "This requires creating a temporary, interactive pod to run a network command within the cluster.",
        "validation_steps": [
          {"cmd": "echo 'This is validated by the user running the command and observing the output.'", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Create a Pod named `config-pod` using the `busybox` image that mounts a ConfigMap named `app-config` as a volume at `/etc/config`",
        "pre_shell_cmds": ["kubectl create configmap app-config --from-literal=key=value"],
        "initial_files": {
          "pod.yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: config-pod\nspec:\n  containers:\n  - name: busybox\n    image: busybox\n    command: ['sleep', '3600']\n"
        },
        "validation_steps": [
          {"cmd": "kubectl apply -f pod.yaml && kubectl get pod config-pod -o jsonpath='{.spec.volumes[0].configMap.name}' | grep 'app-config' && kubectl get pod config-pod -o jsonpath='{.spec.containers[0].volumeMounts[0].mountPath}' | grep '/etc/config'", "matcher": {"exit_code": 0}}
        ]
      }
    ]
  },
  {
    "category": "Deployment Management",
    "prompts": [
      {
        "prompt": "Create a deployment named webapp with image nginx:1.17 and 3 replicas",
        "validation_steps": [
          {"cmd": "kubectl get deployment webapp -o jsonpath='{.spec.replicas}' | grep 3 && kubectl get deployment webapp -o jsonpath='{.spec.template.spec.containers[0].image}' | grep 'nginx:1.17'", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Generate a YAML file for a deployment with nginx image without creating it",
        "validation_steps": [
          {"cmd": "[ -f deploy.yaml ] && grep -q 'kind: Deployment' deploy.yaml && grep -q 'name: nginx' deploy.yaml", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Scale a deployment named 'frontend' to 5 replicas",
        "pre_shell_cmds": ["kubectl create deployment frontend --image=nginx"],
        "validation_steps": [
          {"cmd": "kubectl get deployment frontend -o jsonpath='{.spec.replicas}' | grep 5", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Update the image of a deployment named 'webapp' to nginx:1.18",
        "pre_shell_cmds": ["kubectl create deployment webapp --image=nginx:1.17"],
        "validation_steps": [
          {"cmd": "kubectl get deployment webapp -o jsonpath='{.spec.template.spec.containers[0].image}' | grep 'nginx:1.18'", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Check the rollout status of a deployment named 'frontend'",
        "pre_shell_cmds": ["kubectl create deployment frontend --image=nginx"],
        "validation_steps": [
          {"cmd": "kubectl rollout status deployment/frontend", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Roll back a deployment named 'webapp' to its previous version",
        "pre_shell_cmds": ["kubectl create deployment webapp --image=nginx:1.17", "kubectl set image deployment/webapp webapp=nginx:1.18 --record", "sleep 5"],
        "validation_steps": [
          {"cmd": "kubectl get deployment webapp -o jsonpath='{.spec.template.spec.containers[0].image}' | grep 'nginx:1.17'", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Create a deployment with a record of the change-cause for future reference",
        "validation_steps": [
          {"cmd": "kubectl get deployment nginx -o jsonpath='{.metadata.annotations.kubernetes\\.io/change-cause}' | grep 'kubectl create'", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "View the history of a deployment named 'webapp'",
        "pre_shell_cmds": ["kubectl create deployment webapp --image=nginx"],
        "validation_steps": [
          {"cmd": "kubectl rollout history deployment/webapp", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Pause the rollout of a deployment named 'frontend'",
        "pre_shell_cmds": ["kubectl create deployment frontend --image=nginx"],
        "validation_steps": [
          {"cmd": "kubectl get deployment frontend -o jsonpath='{.spec.paused}' | grep true", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Resume the rollout of a deployment named 'frontend'",
        "pre_shell_cmds": ["kubectl create deployment frontend --image=nginx", "kubectl rollout pause deployment/frontend"],
        "validation_steps": [
          {"cmd": "test $(kubectl get deployment frontend -o jsonpath='{.spec.paused}') != 'true'", "matcher": {"exit_code": 0}}
        ]
      }
    ]
  },
  {
    "category": "Namespace Operations",
    "prompts": [
      {
        "prompt": "Create a new namespace called 'development'",
        "validation_steps": [
          {"cmd": "kubectl get namespace development", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Set the current context to use the 'production' namespace by default",
        "pre_shell_cmds": ["kubectl create ns production"],
        "validation_steps": [
          {"cmd": "kubectl config view --minify | grep 'namespace: production'", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Verify which namespace is currently being used in your context",
        "validation_steps": [
          {"cmd": "kubectl config view --minify | grep namespace:", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Create a new namespace called 'testing' and output its YAML definition",
        "validation_steps": [
          {"cmd": "kubectl get ns testing --dry-run=client -o yaml", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "List all resources across all namespaces",
        "validation_steps": [
          {"cmd": "kubectl get all -A", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "List all pods in the 'kube-system' namespace",
        "validation_steps": [
          {"cmd": "kubectl get pods -n kube-system", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Create a pod in the 'development' namespace",
        "pre_shell_cmds": ["kubectl create ns development"],
        "validation_steps": [
          {"cmd": "kubectl get pod nginx -n development", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Delete a namespace called 'testing' and all its resources",
        "pre_shell_cmds": ["kubectl create ns testing"],
        "validation_steps": [
          {"cmd": "! kubectl get ns testing", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "List all namespaces in the cluster",
        "validation_steps": [
          {"cmd": "kubectl get namespaces", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Create a resource quota for a namespace limiting it to 10 pods",
        "pre_shell_cmds": ["kubectl create ns development"],
        "validation_steps": [
          {"cmd": "kubectl get quota ns-quota -n development", "matcher": {"exit_code": 0}}
        ]
      }
    ]
  },
  {
    "category": "ConfigMap Operations",
    "prompts": [
      {
        "prompt": "Create a ConfigMap named 'app-config' with key-value pairs: APP_COLOR=blue and APP_MODE=prod",
        "validation_steps": [
          {"cmd": "kubectl get cm app-config -o yaml | grep 'APP_COLOR: blue' && kubectl get cm app-config -o yaml | grep 'APP_MODE: prod'", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Create a ConfigMap named 'game-config' from a configuration file located at 'game.properties'",
        "initial_files": {"game.properties": "level=1\nworld=dungeon\n"},
        "validation_steps": [
          {"cmd": "kubectl get cm game-config", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "View the contents of a ConfigMap named 'db-config'",
        "pre_shell_cmds": ["kubectl create cm db-config --from-literal=url=mydb"],
        "validation_steps": [
          {"cmd": "kubectl describe cm db-config", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Create a ConfigMap from all files in the 'configs' directory",
        "initial_files": {"configs/conf1": "val1", "configs/conf2": "val2"},
        "validation_steps": [
          {"cmd": "kubectl get cm app-settings", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Create a pod that uses a ConfigMap as environment variables",
        "pre_shell_cmds": ["kubectl create cm my-cm --from-literal=mykey=myval"],
        "initial_files": {
          "pod.yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: my-container\n    image: busybox\n    command: ['sleep', '3600']\n"
        },
        "validation_steps": [
          {"cmd": "kubectl apply -f pod.yaml && kubectl get pod my-pod -o yaml | grep 'configMapKeyRef'", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Extract a ConfigMap to YAML format",
        "pre_shell_cmds": ["kubectl create cm app-config --from-literal=key=val"],
        "validation_steps": [
          {"cmd": "[ -f config.yaml ] && grep -q 'kind: ConfigMap' config.yaml", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Update a value in an existing ConfigMap named 'app-config'",
        "pre_shell_cmds": ["kubectl create cm app-config --from-literal=key=val"],
        "validation_steps": [
          {"cmd": "kubectl edit cm app-config", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Create a ConfigMap named 'multi-config' with key-value pairs DB_URL=mysql://db, API_KEY=123456, and DEBUG=true from the command line",
        "validation_steps": [
          {"cmd": "kubectl get cm multi-config", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Mount a ConfigMap as a volume in a pod at path /etc/config with specific file permissions",
        "pre_shell_cmds": ["kubectl create cm my-cm --from-literal=mykey=myval"],
        "initial_files": {
          "pod.yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: config-pod\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n"
        },
        "validation_steps": [
          {"cmd": "kubectl apply -f pod.yaml && kubectl get pod config-pod -o jsonpath='{.spec.volumes[0].configMap.name}' | grep 'my-cm'", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Create a ConfigMap from an env file",
        "initial_files": {"config.env": "KEY1=VAL1\nKEY2=VAL2\n"},
        "validation_steps": [
          {"cmd": "kubectl get cm env-config", "matcher": {"exit_code": 0}}
        ]
      }
    ]
  },
  {
    "category": "Secret Management",
    "prompts": [
      {
        "prompt": "Create a Secret named 'db-creds' with username=admin and password=password123",
        "validation_steps": [
          {"cmd": "kubectl get secret db-creds", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Create a Secret named 'tls-cert' from a certificate file",
        "initial_files": {"cert.pem": "my-fake-cert"},
        "validation_steps": [
          {"cmd": "kubectl get secret tls-cert", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "View the encoded values in a Secret named 'api-secrets'",
        "pre_shell_cmds": ["kubectl create secret generic api-secrets --from-literal=key=val"],
        "validation_steps": [
          {"cmd": "kubectl get secret api-secrets -o yaml", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Encode a string for use in a Secret YAML definition",
        "validation_steps": [
          {"cmd": "echo 'This is validated by user running the command.'", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Decode a base64 encoded Secret value",
        "validation_steps": [
          {"cmd": "echo 'This is validated by user running the command.'", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Create a TLS Secret type from certificate and key files",
        "initial_files": {"cert.crt": "fake-cert", "key.key": "fake-key"},
        "validation_steps": [
          {"cmd": "kubectl get secret my-tls --type=kubernetes.io/tls", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Create a pod that uses a Secret value as an environment variable",
        "pre_shell_cmds": ["kubectl create secret generic my-secret --from-literal=API_KEY=123"],
        "initial_files": {"pod.yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: secure-app\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n"},
        "validation_steps": [
          {"cmd": "kubectl apply -f pod.yaml && kubectl get pod secure-app -o yaml | grep 'secretKeyRef'", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Create a Secret from a .env file",
        "initial_files": {".env": "API_KEY=123\n"},
        "validation_steps": [
          {"cmd": "kubectl get secret my-secrets", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Create a pod that mounts a Secret as a volume using the nginx image",
        "pre_shell_cmds": ["kubectl create secret generic my-secret --from-literal=file.txt=content"],
        "initial_files": {"pod.yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-pod\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n"},
        "validation_steps": [
          {"cmd": "kubectl apply -f pod.yaml && kubectl get pod secret-pod -o jsonpath='{.spec.volumes[0].secret.secretName}' | grep 'my-secret'", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "Create a docker-registry type Secret for private registry authentication",
        "validation_steps": [
          {"cmd": "kubectl get secret regcred --type=kubernetes.io/dockerconfigjson", "matcher": {"exit_code": 0}}
        ]
      }
    ]
  },
  {
    "category": "Service Account Operations",
    "prompts": [
      {
        "prompt": "Create a Service Account named 'deployment-sa'",
        "validation_steps": [
          {"cmd": "kubectl get sa deployment-sa", "matcher": {"exit_code": 0}}
        ]
      },
      {
        "prompt": "List all Service Accounts in the current namespace",
        "validation_steps": [
          {"cmd": "kubectl get sa", "matcher": {"exit_code": 0}}
        ]
      }
    ]
  },
  {
    "category": "Additional Commands",
    "prompts": [
      {
        "id": "create-kubectl-alias",
        "prompt": "Create an alias 'k' for 'kubectl'",
        "explanation": "This is typically done by running 'alias k=kubectl'. Note: This command's effect is temporary for the current shell session and cannot be automatically validated here.",
        "validation_steps": [
          {
            "cmd": "echo 'Validation for this step is manual. Please verify the command works in your shell.'",
            "matcher": {
              "exit_code": 0
            }
          }
        ]
      },
      {
        "id": "enable-kubectl-autocompletion-bash",
        "prompt": "Enable kubectl autocompletion for bash",
        "explanation": "This is typically done by running 'source <(kubectl completion bash)'. Note: This command's effect is temporary for the current shell session and cannot be automatically validated here.",
        "validation_steps": [
          {
            "cmd": "echo 'Validation for this step is manual. Please verify the command works in your shell.'",
            "matcher": {
              "exit_code": 0
            }
          }
        ]
      },
      {
        "id": "add-kubectl-completion-to-bashrc",
        "prompt": "Add kubectl autocompletion to ~/.bashrc",
        "validation_steps": [
          {
            "cmd": "grep -q 'source <(kubectl completion bash)' ~/.bashrc",
            "matcher": {
              "exit_code": 0
            }
          }
        ]
      },
      {
        "id": "set-kubeconfig-env-var",
        "prompt": "Set the KUBECONFIG environment variable to /home/user/config",
        "explanation": "This is done with 'export KUBECONFIG=/home/user/config'. The effect is temporary to the shell and cannot be automatically validated.",
        "validation_steps": [
          {
            "cmd": "echo 'Validation for this step is manual. Please verify the command works in your shell.'",
            "matcher": {
              "exit_code": 0
            }
          }
        ]
      },
      {
        "id": "kubectl-version",
        "prompt": "Check the kubectl client and server versions",
        "validation_steps": [
          {
            "cmd": "kubectl version",
            "matcher": {
              "exit_code": 0
            }
          }
        ]
      },
      {
        "id": "create-namespace-dev",
        "prompt": "Create a namespace named dev",
        "validation_steps": [
          {
            "cmd": "kubectl get namespace dev",
            "matcher": {
              "exit_code": 0
            }
          }
        ]
      },
      {
        "id": "list-all-namespaces",
        "prompt": "List all namespaces",
        "validation_steps": [
          {
            "cmd": "kubectl get namespaces",
            "matcher": {
              "exit_code": 0
            }
          }
        ]
      },
      {
        "id": "describe-namespace-dev",
        "prompt": "Describe namespace dev",
        "pre_shell_cmds": [
          "kubectl create namespace dev"
        ],
        "validation_steps": [
          {
            "cmd": "kubectl describe namespace dev",
            "matcher": {
              "exit_code": 0
            }
          }
        ]
      },
      {
        "id": "delete-namespace-dev",
        "prompt": "Delete namespace dev",
        "pre_shell_cmds": [
          "kubectl create namespace dev"
        ],
        "validation_steps": [
          {
            "cmd": "! kubectl get namespace dev",
            "matcher": {
              "exit_code": 0
            }
          }
        ]
      },
      {
        "id": "create-deployment-frontend",
        "prompt": "Create a deployment named frontend using image nginx:1.14",
        "validation_steps": [
          {
            "cmd": "kubectl get deployment frontend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep 'nginx:1.14'",
            "matcher": {
              "exit_code": 0
            }
          }
        ]
      },
      {
        "id": "list-all-deployments",
        "prompt": "List all deployments",
        "validation_steps": [
          {
            "cmd": "kubectl get deployments",
            "matcher": {
              "exit_code": 0
            }
          }
        ]
      },
      {
        "id": "describe-deployment-frontend",
        "prompt": "Describe deployment frontend",
        "pre_shell_cmds": [
          "kubectl create deployment frontend --image=nginx"
        ],
        "validation_steps": [
          {
            "cmd": "kubectl describe deployment frontend",
            "matcher": {
              "exit_code": 0
            }
          }
        ]
      },
      {
        "id": "scale-deployment-frontend",
        "prompt": "Scale deployment frontend to 3 replicas",
        "pre_shell_cmds": [
          "kubectl create deployment frontend --image=nginx"
        ],
        "validation_steps": [
          {
            "cmd": "kubectl get deployment frontend -o jsonpath='{.spec.replicas}' | grep 3",
            "matcher": {
              "exit_code": 0
            }
          }
        ]
      },
      {
        "id": "rollback-deployment-frontend",
        "prompt": "Roll back the deployment frontend to the previous version",
        "pre_shell_cmds": [
          "kubectl create deployment frontend --image=nginx:1.17 --record",
          "kubectl set image deployment/frontend frontend=nginx:1.18 --record",
          "sleep 5"
        ],
        "validation_steps": [
          {
            "cmd": "kubectl get deployment frontend -o jsonpath='{.spec.template.spec.containers[0].image}' | grep 'nginx:1.17'",
            "matcher": {
              "exit_code": 0
            }
          }
        ]
      },
      {
        "id": "restart-deployment-frontend",
        "prompt": "Restart the deployment frontend",
        "pre_shell_cmds": [
          "kubectl create deployment frontend --image=nginx"
        ],
        "validation_steps": [
          {
            "cmd": "kubectl rollout status deployment/frontend",
            "matcher": {
              "exit_code": 0
            }
          }
        ]
      },
      {
        "id": "delete-deployment-frontend",
        "prompt": "Delete deployment frontend",
        "pre_shell_cmds": [
          "kubectl create deployment frontend --image=nginx"
        ],
        "validation_steps": [
          {
            "cmd": "! kubectl get deployment frontend",
            "matcher": {
              "exit_code": 0
            }
          }
        ]
      },
      {
        "prompt": "Run a pod named busybox with image busybox that runs indefinitely",
        "response": "kubectl run busybox --image=busybox --restart=Never -- /bin/sh -c \"sleep 3600\""
      },
      {
        "prompt": "List all pods",
        "response": "kubectl get pods"
      },
      {
        "prompt": "Describe pod busybox",
        "response": "kubectl describe pod busybox"
      },
      {
        "prompt": "Delete pod busybox",
        "response": "kubectl delete pod busybox"
      },
      {
        "prompt": "Expose deployment frontend on port 80 as a ClusterIP service named frontend-svc",
        "response": "kubectl expose deployment frontend --port=80 --type=ClusterIP --name=frontend-svc"
      },
      {
        "prompt": "List all services",
        "response": "kubectl get services"
      },
      {
        "prompt": "Describe service frontend-svc",
        "response": "kubectl describe service frontend-svc"
      },
      {
        "prompt": "Delete service frontend-svc",
        "response": "kubectl delete service frontend-svc"
      },
      {
        "id": "create-configmap-from-file",
        "prompt": "Create a ConfigMap named app-config from file config.yaml",
        "initial_files": {
          "config.yaml": "key: value"
        },
        "validation_steps": [
          {
            "cmd": "kubectl get configmap app-config -o jsonpath='{.data.config\\.yaml}' | grep 'key: value'",
            "matcher": {
              "exit_code": 0
            }
          }
        ]
      },
      {
        "prompt": "List all ConfigMaps",
        "response": "kubectl get configmaps"
      },
      {
        "prompt": "Describe configmap app-config",
        "response": "kubectl describe configmap app-config"
      },
      {
        "prompt": "Delete configmap app-config",
        "response": "kubectl delete configmap app-config"
      },
      {
        "prompt": "Create a generic secret named db-secret with literal username and password",
        "response": "kubectl create secret generic db-secret --from-literal=username=admin --from-literal=password=secret"
      },
      {
        "prompt": "List all secrets",
        "response": "kubectl get secrets"
      },
      {
        "prompt": "Describe secret db-secret",
        "response": "kubectl describe secret db-secret"
      },
      {
        "prompt": "Delete secret db-secret",
        "response": "kubectl delete secret db-secret"
      },
      {
        "prompt": "Create a ServiceAccount named default-sa",
        "response": "kubectl create serviceaccount default-sa"
      },
      {
        "prompt": "List all ServiceAccounts",
        "response": "kubectl get serviceaccounts"
      },
      {
        "prompt": "Describe serviceaccount default-sa",
        "response": "kubectl describe serviceaccount default-sa"
      },
      {
        "prompt": "Delete serviceaccount default-sa",
        "response": "kubectl delete serviceaccount default-sa"
      },
      {
        "prompt": "Label pod busybox with env=prod",
        "response": "kubectl label pod busybox env=prod"
      },
      {
        "prompt": "Remove the label env from pod busybox",
        "response": "kubectl label pod busybox env-"
      },
      {
        "prompt": "Annotate deployment frontend with description=test",
        "response": "kubectl annotate deployment frontend description=test"
      },
      {
        "prompt": "Remove the annotation description from deployment frontend",
        "response": "kubectl annotate deployment frontend description-"
      },
      {
        "prompt": "Apply the configuration in deployment.yaml",
        "response": "kubectl apply -f deployment.yaml"
      },
      {
        "prompt": "Replace configmap using file configmap.yaml",
        "response": "kubectl replace -f configmap.yaml"
      },
      {
        "prompt": "Open the file app.yaml in Vim",
        "response": "vim app.yaml"
      },
      {
        "prompt": "Use kubectl to edit deployment frontend",
        "response": "kubectl edit deployment frontend"
      },
      {
        "prompt": "Open service.yaml in Vim",
        "response": "vim service.yaml"
      },
      {
        "prompt": "View logs of pod busybox",
        "response": "kubectl logs busybox"
      },
      {
        "prompt": "Follow logs of pod busybox",
        "response": "kubectl logs busybox -f"
      },
      {
        "prompt": "View logs of container nginx in pod frontend-pod",
        "response": "kubectl logs frontend-pod -c nginx"
      },
      {
        "prompt": "Execute a shell in pod busybox",
        "response": "kubectl exec -it busybox -- /bin/sh"
      },
      {
        "prompt": "Port-forward local port 8080 to port 80 on service frontend-svc",
        "response": "kubectl port-forward service/frontend-svc 8080:80"
      },
      {
        "prompt": "Describe pod busybox for troubleshooting",
        "response": "kubectl describe pod busybox"
      },
      {
        "prompt": "List all events",
        "response": "kubectl get events"
      },
      {
        "prompt": "Get pod busybox in YAML format",
        "response": "kubectl get pods busybox -o yaml"
      },
      {
        "prompt": "Get pod busybox in JSON format",
        "response": "kubectl get pods busybox -o json"
      },
      {
        "prompt": "Copy file /tmp/data from pod busybox to current directory",
        "response": "kubectl cp busybox:/tmp/data ./"
      },
      {
        "prompt": "Show resource usage of pods",
        "response": "kubectl top pods"
      },
      {
        "prompt": "Show resource usage of nodes",
        "response": "kubectl top nodes"
      },
      {
        "prompt": "Describe node worker-node-1",
        "response": "kubectl describe node worker-node-1"
      },
      {
        "prompt": "View the current kubectl context",
        "response": "kubectl config current-context"
      },
      {
        "prompt": "List all contexts",
        "response": "kubectl config get-contexts"
      },
      {
        "prompt": "Switch kubectl context to minikube",
        "response": "kubectl config use-context minikube"
      },
      {
        "prompt": "Set the namespace to dev in the current context",
        "response": "kubectl config set-context --current --namespace=dev"
      },
      {
        "prompt": "Display cluster info",
        "response": "kubectl cluster-info"
      },
      {
        "prompt": "Show available API resource types",
        "response": "kubectl api-resources"
      },
      {
        "prompt": "Explain the fields of pods",
        "response": "kubectl explain pods"
      },
      {
        "prompt": "Get pods with label app=frontend",
        "response": "kubectl get pods -l app=frontend"
      },
      {
        "prompt": "Get pods with all labels",
        "response": "kubectl get pods --show-labels"
      },
      {
        "prompt": "Apply deployment.yaml without sending to server (dry run)",
        "response": "kubectl apply -f deployment.yaml --dry-run=client"
      },
      {
        "prompt": "Generate YAML for deployment 'test' using the nginx image without creating it",
        "response": "kubectl create deployment test --image=nginx --dry-run=client -o yaml"
      },
      {
        "prompt": "Continuously watch pods",
        "response": "kubectl get pods -w"
      },
      {
        "prompt": "Get all resources in all namespaces",
        "response": "kubectl get all -A"
      },
      {
        "prompt": "List pods with status phase=Running",
        "response": "kubectl get pods --field-selector=status.phase=Running"
      },
      {
        "prompt": "Watch the rollout status of deployment frontend",
        "response": "kubectl rollout status deployment/frontend -w"
      },
      {
        "prompt": "Restart the deployment frontend",
        "response": "kubectl rollout restart deployment/frontend"
      },
      {
        "prompt": "Auto-scale deployment frontend to minimum 2 and maximum 5 pods at 80% CPU",
        "response": "kubectl autoscale deployment frontend --min=2 --max=5 --cpu-percent=80"
      },
      {
        "prompt": "Check if you can create pods",
        "response": "kubectl auth can-i create pods"
      },
      {
        "prompt": "Show only context names",
        "response": "kubectl config get-contexts -o name"
      },
      {
        "prompt": "Open or create the file app.yaml in Vim",
        "response": "vim app.yaml"
      },
      {
        "prompt": "Enter insert mode",
        "response": "i"
      },
      {
        "prompt": "Exit insert mode",
        "response": "<Esc>"
      },
      {
        "prompt": "Save and quit Vim",
        "response": ":wq"
      },
      {
        "prompt": "Quit without saving",
        "response": ":q!"
      },
      {
        "prompt": "Delete the current line",
        "response": "dd"
      },
      {
        "prompt": "Yank (copy) the current line",
        "response": "yy"
      },
      {
        "prompt": "Paste after the cursor",
        "response": "p"
      },
      {
        "prompt": "Search forward for 'pattern'",
        "response": "/pattern"
      },
      {
        "prompt": "Go to line N",
        "response": ":N"
      },
      {
        "prompt": "Undo the last change",
        "response": "u"
      },
      {
        "prompt": "Redo the last undone change",
        "response": "<C-r>"
      },
      {
        "prompt": "Enter visual mode",
        "response": "v"
      },
      {
        "prompt": "Enter visual line mode",
        "response": "V"
      },
      {
        "prompt": "Enter visual block mode",
        "response": "<C-v>"
      },
      {
        "prompt": "Enter replace mode",
        "response": "R"
      },
      {
        "prompt": "Move to the next word",
        "response": "w"
      },
      {
        "prompt": "Move to the previous word",
        "response": "b"
      },
      {
        "prompt": "Move to the end of the word",
        "response": "e"
      },
      {
        "prompt": "Move to the first character of the line",
        "response": "0"
      },
      {
        "prompt": "Move to the end of the line",
        "response": "$"
      },
      {
        "prompt": "Jump to matching bracket",
        "response": "%"
      }
    ]
  },
  {
    "category": "Live Kubernetes Questions",
    "prompts": [
      {
        "prompt": "The provided YAML is for a basic pod. Modify it to create a pod named 'nginx-live' that uses the 'nginx:stable' image.",
        "initial_files": {
          "pod.yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: main\n    image: busybox\n    command: [\"sleep\", \"3600\"]"
        },
        "validation_steps": [
          {
            "cmd": "#!/bin/bash\nset -e\nkubectl apply -f pod.yaml\n# Check pod name\nPOD_NAME=$(kubectl get pods -o jsonpath='{.items[0].metadata.name}')\nif [ \"$POD_NAME\" != \"nginx-live\" ]; then\n  echo \"Error: Pod name is '$POD_NAME', expected 'nginx-live'\"\n  exit 1\nfi\n# Check image\nIMAGE=$(kubectl get pod nginx-live -o jsonpath='{.spec.containers[0].image}')\nif [ \"$IMAGE\" != \"nginx:stable\" ]; then\n  echo \"Error: Image is '$IMAGE', expected 'nginx:stable'\"\n  exit 1\nfi\necho \"Pod 'nginx-live' with image 'nginx:stable' found.\"",
            "matcher": {"exit_code": 0}
          }
        ],
        "explanation": "To solve this, you need to change the `metadata.name` to 'nginx-live' and the `spec.containers[0].image` to 'nginx:stable'."
      },
      {
        "prompt": "A 'webapp' deployment is defined in the file. Add a new NodePort service named 'webapp-svc' that exposes the deployment on port 80. The service should target port 8080 on the pods.",
        "initial_files": {
          "service.yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webapp\n  labels:\n    app: webapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: webapp\n  template:\n    metadata:\n      labels:\n        app: webapp\n    spec:\n      containers:\n      - name: webapp\n        image: httpd\n        ports:\n        - containerPort: 8080\n---\n# Add your Service definition below"
        },
        "validation_steps": [
          {
            "cmd": "#!/bin/bash\nset -e\nkubectl apply -f service.yaml\nkubectl wait --for=condition=available deployment/webapp --timeout=60s > /dev/null\n# Check service exists and is NodePort\nTYPE=$(kubectl get svc webapp-svc -o jsonpath='{.spec.type}')\nif [ \"$TYPE\" != \"NodePort\" ]; then\n  echo \"Error: Service 'webapp-svc' is type '$TYPE', expected 'NodePort'\"\n  exit 1\nfi\n# Check service port and targetPort\nPORT=$(kubectl get svc webapp-svc -o jsonpath='{.spec.ports[0].port}')\nTARGET_PORT=$(kubectl get svc webapp-svc -o jsonpath='{.spec.ports[0].targetPort}')\nif [ \"$PORT\" != \"80\" ]; then\n  echo \"Error: Service port is '$PORT', expected '80'\"\n  exit 1\nfi\nif [ \"$TARGET_PORT\" != \"8080\" ]; then\n  echo \"Error: Service targetPort is '$TARGET_PORT', expected '8080'\"\n  exit 1\nfi\necho \"Service 'webapp-svc' correctly configured as a NodePort.\"",
            "matcher": {"exit_code": 0}
          }
        ],
        "explanation": "You need to add a second resource definition for the Service, setting `spec.type` to `NodePort`, `spec.ports[0].port` to 80, `spec.ports[0].targetPort` to 8080, and `spec.selector` to match the deployment's labels (`app: webapp`)."
      },
      {
        "prompt": "Create two resources: 1. A ConfigMap named 'app-config' with data 'index.html: <h1>Hello World</h1>'. 2. A Pod named 'web-server' using the 'nginx' image that mounts the ConfigMap as a volume at '/usr/share/nginx/html', replacing the default nginx page.",
        "initial_files": {
          "resources.yaml": "# Add your ConfigMap and Pod definitions here"
        },
        "validation_steps": [
          {
            "cmd": "#!/bin/bash\nset -e\nkubectl apply -f resources.yaml\n# Check ConfigMap data\nDATA=$(kubectl get cm app-config -o jsonpath='{.data.index\\.html}')\nif ! echo \"$DATA\" | grep -q 'Hello World'; then\n  echo \"Error: ConfigMap 'app-config' does not have the correct data.\"\n  exit 1\nfi\n# Wait for pod to be ready\nkubectl wait --for=condition=Ready pod/web-server --timeout=120s > /dev/null\n# Check that the mounted file contains the correct content\nCONTENT=$(kubectl exec web-server -- curl -s localhost)\nif ! echo \"$CONTENT\" | grep -q 'Hello World'; then\n  echo \"Error: The web server did not return the expected content from the ConfigMap.\"\n  echo \"Received: $CONTENT\"\n  exit 1\nfi\necho \"Pod is serving content from the mounted ConfigMap correctly.\"",
            "matcher": {"exit_code": 0}
          }
        ],
        "explanation": "You need to define a ConfigMap with the specified data. Then, define a Pod that references this ConfigMap in `spec.volumes` and mounts it into the container at the correct path using `spec.containers[0].volumeMounts`."
      }
    ]
  },
  {
    "category": "Advanced Scenarios",
    "prompts": [
      {
        "prompt": "Solve this question on instance: ssh ckad5601\nCreate a single Pod of image httpd:2.4.41-alpine in Namespace default. The Pod should be named pod1 and the container should be named pod1-container.\nYour manager would like to run a command manually on occasion to output the status of that exact Pod. Please write a command that does this into /opt/course/2/pod1-status-command.sh on ckad5601. The command should use kubectl.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad7326\nTeam Neptune needs a Job template located at /opt/course/3/job.yaml. This Job should run image busybox:1.31.0 and execute sleep 2 && echo done. It should be in namespace neptune, run a total of 3 times and should execute 2 runs in parallel.\nStart the Job and check its history. Each pod created by the Job should have the label id: awesome-job. The job should be named neb-new-job and the container neb-new-job-container.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad7326\nTeam Mercury asked you to perform some operations using Helm, all in Namespace mercury:\nDelete release internal-issue-report-apiv1\nUpgrade release internal-issue-report-apiv2 to any newer version of chart bitnami/nginx available\nInstall a new release internal-issue-report-apache of chart bitnami/apache. The Deployment should have two replicas, set these via Helm-values during install\nThere seems to be a broken release, stuck in pending-install state. Find it and delete it",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad7326\nTeam Neptune has its own ServiceAccount named neptune-sa-v2 in Namespace neptune. A coworker needs the token from the Secret that belongs to that ServiceAccount. Write the base64 decoded token to file /opt/course/5/token on ckad7326.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad5601\nCreate a single Pod named pod6 in Namespace default of image busybox:1.31.0. The Pod should have a readiness-probe executing cat /tmp/ready. It should initially wait 5 and periodically wait 10 seconds. This will set the container ready only if the file /tmp/ready exists.\nThe Pod should run the command touch /tmp/ready && sleep 1d, which will create the necessary file to be ready and then idles. Create the Pod and confirm it starts.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad7326\nThe board of Team Neptune decided to take over control of one e-commerce webserver from Team Saturn. The administrator who once setup this webserver is not part of the organisation any longer. All information you could get was that the e-commerce system is called my-happy-shop.\nSearch for the correct Pod in Namespace saturn and move it to Namespace neptune. It doesn't matter if you shut it down and spin it up again, it probably hasn't any customers anyways.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad7326\nThere is an existing Deployment named api-new-c32 in Namespace neptune. A developer did make an update to the Deployment but the updated version never came online. Check the Deployment history and find a revision that works, then rollback to it. Could you tell Team Neptune what the error was so it doesn't happen again?",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad9043\nIn Namespace pluto there is single Pod named holy-api. It has been working okay for a while now but Team Pluto needs it to be more reliable.\nConvert the Pod into a Deployment named holy-api with 3 replicas and delete the single Pod once done. The raw Pod template file is available at /opt/course/9/holy-api-pod.yaml.\nIn addition, the new Deployment should set allowPrivilegeEscalation: false and privileged: false for the security context on container level.\nPlease create the Deployment and save its yaml under /opt/course/9/holy-api-deployment.yaml on ckad9043.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad9043\nTeam Pluto needs a new cluster internal Service. Create a ClusterIP Service named project-plt-6cc-svc in Namespace pluto. This Service should expose a single Pod named project-plt-6cc-api of image nginx:1.17.3-alpine, create that Pod as well. The Pod should be identified by label project: plt-6cc-api. The Service should use tcp port redirection of 3333:80.\nFinally use for example curl from a temporary nginx:alpine Pod to get the response from the Service. Write the response into /opt/course/10/service_test.html on ckad9043. Also check if the logs of Pod project-plt-6cc-api show the request and write those into /opt/course/10/service_test.log on ckad9043.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad9043\nThere are files to build a container image located at /opt/course/11/image on ckad9043. The container will run a Golang application which outputs information to stdout. You're asked to perform the following tasks:\n\u2139\ufe0f Run all Docker and Podman commands as user root. Use sudo docker and sudo podman or become root with sudo -i\nChange the Dockerfile: set ENV variable SUN_CIPHER_ID to hardcoded value 5b9c1065-e39d-4a43-a04a-e59bcea3e03f\nBuild the image using sudo docker, tag it registry.killer.sh:5000/sun-cipher:v1-docker and push it to the registry\nBuild the image using sudo podman, tag it registry.killer.sh:5000/sun-cipher:v1-podman and push it to the registry\nRun a container using sudo podman, which keeps running detached in the background, named sun-cipher using image registry.killer.sh:5000/sun-cipher:v1-podman\nWrite the logs your container sun-cipher produces into /opt/course/11/logs on ckad9043",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad5601\nCreate a new PersistentVolume named earth-project-earthflower-pv. It should have a capacity of 2Gi, accessMode ReadWriteOnce, hostPath /Volumes/Data and no storageClassName defined.\nNext create a new PersistentVolumeClaim in Namespace earth named earth-project-earthflower-pvc . It should request 2Gi storage, accessMode ReadWriteOnce and should not define a storageClassName. The PVC should bound to the PV correctly.\nFinally create a new Deployment project-earthflower in Namespace earth which mounts that volume at /tmp/project-data. The Pods of that Deployment should be of image httpd:2.4.41-alpine.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad9043\nTeam Moonpie, which has the Namespace moon, needs more storage. Create a new PersistentVolumeClaim named moon-pvc-126 in that namespace. This claim should use a new StorageClass moon-retain with the provisioner set to moon-retainer and the reclaimPolicy set to Retain. The claim should request storage of 3Gi, an accessMode of ReadWriteOnce and should use the new StorageClass.\nThe provisioner moon-retainer will be created by another team, so it's expected that the PVC will not boot yet. Confirm this by writing the event message from the PVC into file /opt/course/13/pvc-126-reason on ckad9043.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad9043\nYou need to make changes on an existing Pod in Namespace moon called secret-handler. Create a new Secret secret1 which contains user=test and pass=pwd. The Secret's content should be available in Pod secret-handler as environment variables SECRET1_USER and SECRET1_PASS. The yaml for Pod secret-handler is available at /opt/course/14/secret-handler.yaml.\nThere is existing yaml for another Secret at /opt/course/14/secret2.yaml, create this Secret and mount it inside the same Pod at /tmp/secret2. Your changes should be saved under /opt/course/14/secret-handler-new.yaml on ckad9043. Both Secrets should only be available in Namespace moon.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad9043\nTeam Moonpie has a nginx server Deployment called web-moon in Namespace moon. Someone started configuring it but it was never completed. To complete please create a ConfigMap called configmap-web-moon-html containing the content of file /opt/course/15/web-moon.html under the data key-name index.html.\nThe Deployment web-moon is already configured to work with this ConfigMap and serve its content. Test the nginx configuration for example using curl from a temporary nginx:alpine Pod.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad7326\nThe Tech Lead of Mercury2D decided it's time for more logging, to finally fight all these missing data incidents. There is an existing container named cleaner-con in Deployment cleaner in Namespace mercury. This container mounts a volume and writes logs into a file called cleaner.log.\nThe yaml for the existing Deployment is available at /opt/course/16/cleaner.yaml. Persist your changes at /opt/course/16/cleaner-new.yaml on ckad7326 but also make sure the Deployment is running.\nCreate a sidecar container named logger-con, image busybox:1.31.0 , which mounts the same volume and writes the content of cleaner.log to stdout, you can use the tail -f command for this. This way it can be picked up by kubectl logs.\nCheck if the logs of the new container reveal something about the missing data incidents.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad5601\nLast lunch you told your coworker from department Mars Inc how amazing InitContainers are. Now he would like to see one in action. There is a Deployment yaml at /opt/course/17/test-init-container.yaml. This Deployment spins up a single Pod of image nginx:1.17.3-alpine and serves files from a mounted volume, which is empty right now.\nCreate an InitContainer named init-con which also mounts that volume and creates a file index.html with content check this out! in the root of the mounted volume. For this test we ignore that it doesn't contain valid html.\nThe InitContainer should be using image busybox:1.31.0. Test your implementation for example using curl from a temporary nginx:alpine Pod.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad5601\nThere seems to be an issue in Namespace mars where the ClusterIP service manager-api-svc should make the Pods of Deployment manager-api-deployment available inside the cluster.\nYou can test this with curl manager-api-svc.mars:4444 from a temporary nginx:alpine Pod. Check for the misconfiguration and apply a fix.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad5601\nIn Namespace jupiter you'll find an apache Deployment (with one replica) named jupiter-crew-deploy and a ClusterIP Service called jupiter-crew-svc which exposes it. Change this service to a NodePort one to make it available on all nodes on port 30100.\nTest the NodePort Service using the internal IP of all available nodes and the port 30100 using curl, you can reach the internal node IPs directly from your main terminal. On which nodes is the Service reachable? On which node is the Pod running?",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad7326\nIn Namespace venus you'll find two Deployments named api and frontend. Both Deployments are exposed inside the cluster using Services. Create a NetworkPolicy named np1 which restricts outgoing tcp connections from Deployment frontend and only allows those going to Deployment api. Make sure the NetworkPolicy still allows outgoing traffic on UDP/TCP ports 53 for DNS resolution.\nTest using: wget www.google.com and wget api:2222 from a Pod of Deployment frontend.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad7326\nTeam Neptune needs 3 Pods of image httpd:2.4-alpine, create a Deployment named neptune-10ab for this. The containers should be named neptune-pod-10ab. Each container should have a memory request of 20Mi and a memory limit of 50Mi.\nTeam Neptune has its own ServiceAccount neptune-sa-v2 under which the Pods should run. The Deployment should be in Namespace neptune.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      },
      {
        "prompt": "Solve this question on instance: ssh ckad9043\nTeam Sunny needs to identify some of their Pods in namespace sun. They ask you to add a new label protected: true to all Pods with an existing label type: worker or type: runner. Also add an annotation protected: do not delete this pod to all Pods having the new label protected: true.",
        "response": "This is a complex scenario. Follow the prompt instructions on the specified instance."
      }
    ]
  }
]
