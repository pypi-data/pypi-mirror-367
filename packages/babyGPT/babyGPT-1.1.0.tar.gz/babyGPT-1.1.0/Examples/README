
If you have just started working with the babyGPT module, you would need to run the
scripts in the order shown below:

    1.    run_gatherer.py

            Before you run this script, check the URLs that are specified in the list

                       urls

            near the top of the script.  For the very first run, you might just want
            to have a single URL in that list just to make sure that there exist no
            network issues (caused by, say, firewalls, etc.) that would keep you from
            downloading the articles.

            By default, the downloaded articles are saved in the following directory

                      saved_articles_dir

            The Examples directory also contains the following shellscript

                      total_chars.sh                      

            Execute this script inside the directory "saved_articles_dir" to see the
            total number of chars in all of the text data you downloaded.


    2.    train_tokenizer.py

            If the text corpus you have collected is for a specialized domain (such
            as movies, sports, healthcare, etc.), you are likely to get better
            results from babyGPT if you first train a tokenizer for that domain using
            this script.

            Note that the module comes with a pre-trained tokenizer with a vocab size
            of around 50,000 tokens.  I trained this tokenizer using the babyGPT
            module on the athlete news dataset created by Adrien Dubois. The name of
            the tokenizer JSON in the Examples directory is:

                            109_babygpt_tokenizer_49275.json
                                                                                                                                                                                  
    3.    create_base_model_with_buffered_context.py

            This trains an unsupervised model for your corpus on the basis of next
            token prediction.

            This script hangs to the last few tokens in each instance from the
            previous batch to provide a context for the first token in the
            corresponding instance in the new batch.

            When you train model using the "create_" script, a checkpoint is
            automatically saved every 10,000 training iterations.  The checkpoints
            are saved in the directory

                        checkpoint_dir


    4.    interact_with_prompts.py

            This is the script for interacting with a trained babyGPT model through
            prompts.  The idea is that you supply a small number of words (as, say,
            the beginning of a new thought) as a prompt and the model supplies the
            rest of the words to complete the thought.  At this time, the model
            extends your prompt until it reaches a period (or the end dictated by the
            size of the "max_seq_length" parameter.


    5.    apply_tokenizer.py

            If you have created a new JSON file for the tokenizer, this script is
            just to test the tokenizer on a small txt file

