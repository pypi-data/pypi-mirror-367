Metadata-Version: 2.4
Name: autofeatureselect
Version: 0.1.0
Summary: A Python package for automated feature selection in ML workflows.
Home-page: https://github.com/yourusername/autofeatureselect
Author: Shreenidhi T H
Author-email: your.email@example.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.7
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy
Requires-Dist: pandas
Requires-Dist: scikit-learn
Requires-Dist: xgboost
Requires-Dist: shap
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: license-file
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary


# ğŸ” AutoFeatureSelect

A lightweight, modular Python package for **automatic feature selection** in machine learning workflows.  
It helps data scientists and ML engineers **remove irrelevant, redundant, or noisy features** to improve model performance, reduce overfitting, and simplify datasets â€” without manual guesswork.

---

## âœ¨ Why AutoFeatureSelect?

Most automated ML tools focus on **feature generation**, not feature elimination.

But **throwing more features** at a model often hurts performance. What if you want to **cut the noise**?

That's where `AutoFeatureSelect` shines:
- âœ… Selects only the most **relevant features**
- âœ… Works on any **tabular dataset**
- âœ… No need to generate new features
- âœ… Transparent, modular, and customizable
- âœ… Compatible with any ML framework (`scikit-learn`, `XGBoost`, `LightGBM`, etc.)

---

## ğŸš€ Installation

```bash
pip install autofeatureselect
```

---

## ğŸ“¦ What This Package Does

`AutoFeatureSelect` provides a growing set of **feature selection tools**, including:

| Method                    | Type     | Description                                                        |
|---------------------------|----------|--------------------------------------------------------------------|
| `low_variance_filter`     | Filter   | Removes features with very low variance (near-constant columns)   |
| `correlation_filter`      | Filter   | Drops features that are highly correlated with others              |
| `mutual_info_filter`      | Filter   | Selects features with high mutual information with target          |
| `vif_filter`              | Filter   | Removes multicollinear features using Variance Inflation Factor    |
| `rfe_filter`              | Wrapper  | Uses model-based recursive elimination of weakest features         |
| `tree_importance_filter`  | Embedded | Uses tree model importance scores for feature selection            |
| `shap_filter`             | Embedded | Uses SHAP values to rank and select top features                   |


---

## ğŸ”§ Example Usage

```python
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from autofeatureselect.filters import (
    low_variance_filter,
    correlation_filter,
    mutual_info_filter,
    vif_filter
)
from autofeatureselect.model_wrappers import (
    tree_importance_filter,
    rfe_filter,
    shap_filter
)

# Load your dataset
df = pd.read_csv("data.csv")
target = "target_column"

# 1. Apply Low Variance Filter
df = low_variance_filter(df, threshold=0.01)

# 2. Apply Correlation Filter
df = correlation_filter(df, target=target, threshold=0.9)

# 3. Apply Mutual Information Filter
df = mutual_info_filter(df, target=target, problem_type="classification", threshold=0.01)

# 4. Apply VIF Filter
df = vif_filter(df, threshold=5.0)

# Separate features and target for model-based methods
X = df.drop(columns=[target])
y = df[target]

# 5. Apply Tree-Based Feature Importance
X = tree_importance_filter(X, y, model=RandomForestClassifier(), threshold=0.01)

# 6. Apply RFE (Recursive Feature Elimination)
X = rfe_filter(X, y, model=RandomForestClassifier(), threshold=0.01)

# 7. Apply SHAP-Based Selection
X = shap_filter(X, y, model=RandomForestClassifier(), threshold=0.01)

# Combine X and y back
df_selected = pd.concat([X, y], axis=1)

# Final selected features
print(df_selected.head())

```

---

## ğŸ’¡ When to Use Which Method?

| Method                  | Best For                                | Notes                                     |
|-------------------------|------------------------------------------|-------------------------------------------|
| `low_variance_filter`   | Removing constant or near-constant columns | Fast and simple                           |
| `correlation_filter`    | Redundant features (highly correlated)   | Pairwise comparison                        |
| `mutual_info_filter`    | Non-linear relevance to target           | Requires `problem_type` parameter         |
| `vif_filter`            | Multicollinearity                        | Good before linear models                 |
| `rfe_filter`            | Model-driven selection                   | Slower but accurate                       |
| `tree_importance_filter`| Tree models (e.g. XGBoost, RandomForest) | Uses feature importances from trees       |
| `shap_filter`           | Global interpretability, model-agnostic | Uses SHAP values, slower but insightful   |

---

## ğŸ” How It's Different from Other Tools

| Feature | AutoFeatureSelect | Featuretools | tsfresh | feature-engine | scikit-learn |
|--------|--------------------|--------------|---------|----------------|--------------|
| Focus | Feature **Selection** | Feature **Generation** | Feature **Extraction** | Manual Engineering | General ML |
| Auto-generate new features | âŒ | âœ… | âœ… | âš ï¸ | âš ï¸ |
| Remove irrelevant features | âœ… | âŒ | âš ï¸ Partial | âœ… | âœ… |
| Targeted data | Tabular | Relational | Time-series | Tabular | All |
| Easy to use | âœ… | âŒ | âŒ | âš ï¸ | âœ… |
| Beginner-friendly | âœ… | âŒ | âŒ | âš ï¸ | âš ï¸ |

---

## ğŸ§ª Testing the Package

```bash
# Run from the root of the project directory
pytest test/
```

Ensure `pytest` is installed:
```bash
pip install pytest
```

---

## ğŸ“ Project Structure

```
autofeatureselect/
â”‚
â”œâ”€â”€ filters.py # Statistical methods
â”‚ â”œâ”€â”€ low_variance_filter()
â”‚ â”œâ”€â”€ correlation_filter()
â”‚ â””â”€â”€ mutual_info_filter()
â”‚
â”œâ”€â”€ model_wrappers.py # Model-based methods
â”‚ â”œâ”€â”€ tree_importance_filter()
â”‚ â”œâ”€â”€ rfe_filter()
â”‚ â””â”€â”€ shap_filter()
â”‚
â””â”€â”€ init.py # Exposes selected functions
```

---

## ğŸ“˜ Documentation

Every function is self-contained and comes with:

- Docstrings
- Clear parameters
- Defaults set to typical values
- Returns filtered `DataFrame` (with target column)

You can use Pythonâ€™s built-in help:

```python
from autofeatureselect.filters import mutual_info_filter
help(mutual_info_filter)
```

Or read the source â€” itâ€™s clean and readable!

---

## ğŸ¤” What It Does *Not* Do

- âŒ It does not create new features (use `Featuretools`, `tsfresh`, or `scikit-learn` for that)
- âŒ It does not automate the entire pipeline (no one-size-fits-all â€” feature selection must be task-specific!)
- âŒ It does not hide logic in â€œblack boxesâ€ â€” everything is transparent and user-controllable

---

## ğŸ™Œ Contributing

Want to add your own feature selection method?  
PRs and discussions welcome! Just follow the modular style, and document everything clearly.

---

## ğŸ‘¤ Author

**Shreenidhi TH**  
Developer passionate about building tools for applied ML and automation.

---
## License

This project is licensed under the [MIT License](./LICENSE).

----
