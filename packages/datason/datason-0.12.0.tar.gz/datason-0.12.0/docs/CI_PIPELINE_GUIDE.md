# CI/CD Pipeline Guide

datason uses a **modern, multi-pipeline CI/CD architecture** designed for speed, clarity, and parallel execution. This guide explains our complete CI/CD setup.

## ğŸ—ï¸ **Architecture Overview**

```mermaid
graph TD
    A[Git Push/PR] --> B{Change Type?}
    B -->|Any Code| C[ğŸ” Quality Pipeline]
    B -->|Non-docs| D[ğŸ§ª Main CI Pipeline]
    B -->|Docs Only| E[ğŸ“š Docs Pipeline]
    B -->|Release Tag| F[ğŸ“¦ Publish Pipeline]

    C --> G[âœ… Code Quality & Security]
    D --> H[âœ… Tests & Build]
    E --> I[âœ… Documentation]
    F --> J[âœ… PyPI Release]
```

## ğŸ”„ **Pipeline Details**

### **ğŸ§ª Main CI Pipeline** (`ci.yml`)

**Triggers:**
- Push to `main`, `develop`
- Pull requests to `main`
- **Excludes**: docs-only changes

**What it does:**
```yaml
jobs:
  test:
    - ğŸ“¥ Checkout code
    - ğŸ Setup Python 3.11
    - ğŸ’¾ Cache pip dependencies
    - ğŸ“¦ Install dev dependencies
    - ğŸ§ª Run core tests with plugin matrix:
        â€¢ minimal: tests/unit/ (no optional deps)
â€¢ with-numpy: tests/unit/ + ML features (numpy)
â€¢ with-pandas: tests/unit/ + data features (pandas)
â€¢ with-ml-deps: tests/unit/ + tests/integration/ (full ML stack)
â€¢ full: tests/unit/ + tests/edge_cases/ + tests/integration/
    - ğŸ“Š Upload coverage to Codecov
    - ğŸ”’ Security scan (bandit)
    - ğŸ“¤ Upload security report

  build:
    - ğŸ—ï¸ Build package (wheel + sdist)
    - âœ… Check package integrity
    - ğŸ“¤ Upload build artifacts
```

**Performance:** ~2-3 minutes with caching

### **ğŸ” Code Quality & Security Pipeline** (`ruff.yml`)

**Triggers:**
- **All** pushes to `main`, `develop`
- **All** pull requests to `main`, `develop`

**What it does:**
```yaml
jobs:
  quality-and-security:
    - ğŸ“¥ Checkout code
    - ğŸ Setup Python 3.11
    - ğŸ’¾ Cache pip dependencies (quality)
    - ğŸ› ï¸ Install ruff + bandit
    - ğŸ§¹ Run ruff linter
    - ğŸ¨ Run ruff formatter check
    - ğŸ›¡ï¸ Run bandit security scan
    - ğŸ“Š Generate quality report
```

**Performance:** ~30-60 seconds (15-30s with cache)

### **ğŸ“š Documentation Pipeline** (`docs.yml`)

**Triggers:**
- Changes to `docs/**`, `mkdocs.yml`, `README.md`

**What it does:**
```yaml
jobs:
  build-docs:
    - ğŸ“¥ Checkout code
    - ğŸ Setup Python 3.11
    - ğŸ’¾ Cache pip dependencies (docs)
    - ğŸ’¾ Cache MkDocs build
    - ğŸ“¦ Install docs dependencies
    - ğŸ—ï¸ Build documentation
    - ğŸ“¤ Upload docs artifact

  deploy-github-pages:
    - ğŸš€ Deploy to GitHub Pages (main only)
```

**Performance:** ~1-2 minutes (very fast with cache)

### **ğŸ“¦ Publish Pipeline** (`publish.yml`)

**Triggers:**
- GitHub releases (automatic)
- Manual workflow dispatch

**What it does:**
```yaml
jobs:
  build:
    - ğŸ—ï¸ Build package
    - âœ… Verify package integrity
    - ğŸ“¤ Upload build artifacts

  test-pypi:
    - ğŸ§ª Publish to TestPyPI
    - âœ… Verify upload

  pypi:
    - ğŸš€ Publish to PyPI (releases only)
    - ğŸ“¢ Create release summary
```

**Performance:** ~2-3 minutes

## âš¡ **Performance Optimizations**

### **Intelligent Caching Strategy**

Each pipeline has optimized caching:

```yaml
# Main CI - General development cache
key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}

# Quality - Specialized for linting tools
key: ${{ runner.os }}-quality-pip-${{ hashFiles('**/pyproject.toml') }}

# Docs - Documentation-specific cache + MkDocs
key: ${{ runner.os }}-docs-pip-${{ hashFiles('**/pyproject.toml') }}
key: ${{ runner.os }}-mkdocs-${{ hashFiles('mkdocs.yml') }}-${{ hashFiles('docs/**') }}
```

**Cache Benefits:**
- **First run:** Full dependency installation
- **Subsequent runs:** 2-5x faster execution
- **Cross-pipeline sharing:** Quality cache falls back to main cache

### **Smart Triggering**

**Path-based triggers** prevent unnecessary runs:

```yaml
# Main CI skips docs-only changes
paths-ignore:
  - 'docs/**'
  - '*.md'
  - 'mkdocs.yml'

# Docs pipeline only runs for docs changes
paths: ['docs/**', 'mkdocs.yml', 'README.md']
```

**Result:** Docs changes don't trigger expensive test runs

## ğŸ¯ **Pipeline Responsibilities**

| Pipeline | Purpose | Speed | When |
|----------|---------|-------|------|
| **Main CI** | Core functionality validation | ~2-3 min | Code changes |
| **Quality** | Code quality & security | ~30-60s | All changes |
| **Docs** | Documentation generation | ~1-2 min | Docs changes |
| **Performance** | Performance regression tracking (informational) | ~3-5 min | Performance changes, weekly |
| **Publish** | Package distribution | ~2-3 min | Releases |

## ğŸ—ï¸ **Test Structure & CI Matrix Alignment**

### **New Organized Test Structure**

Our test suite is organized into logical directories for optimal CI performance:

```
tests/
â”œâ”€â”€ core/           # Fast core functionality tests (~7 seconds)
â”‚   â”œâ”€â”€ test_core.py                    # Basic serialization
â”‚   â”œâ”€â”€ test_security.py                # Security features  
â”‚   â”œâ”€â”€ test_circular_references.py     # Circular reference handling
â”‚   â”œâ”€â”€ test_edge_cases.py              # Edge cases and error handling
â”‚   â”œâ”€â”€ test_converters.py              # Type converters
â”‚   â”œâ”€â”€ test_deserializers.py           # Deserialization functionality
â”‚   â””â”€â”€ test_dataframe_orientation_regression.py
â”‚
â”œâ”€â”€ features/       # Feature-specific tests (~10-20 seconds)
â”‚   â”œâ”€â”€ test_ml_serializers.py          # ML library integrations
â”‚   â”œâ”€â”€ test_chunked_streaming.py       # Streaming/chunking features
â”‚   â”œâ”€â”€ test_auto_detection_and_metadata.py  # Auto-detection
â”‚   â””â”€â”€ test_template_deserialization.py     # Template deserialization
â”‚
â”œâ”€â”€ integration/    # Integration tests (~5-15 seconds)
â”‚   â”œâ”€â”€ test_config_and_type_handlers.py     # Configuration integration
â”‚   â”œâ”€â”€ test_optional_dependencies.py        # Dependency integrations
â”‚   â””â”€â”€ test_pickle_bridge.py                # Pickle bridge functionality
â”‚
â”œâ”€â”€ coverage/       # Coverage boost tests (~10-30 seconds)
â”‚   â””â”€â”€ test_*_coverage_boost.py             # Targeted coverage improvement
â”‚
â””â”€â”€ benchmarks/     # Performance tests (separate pipeline, ~60-120 seconds)
    â””â”€â”€ test_*_benchmarks.py                 # Performance measurements
```

### **CI Test Matrix Mapping**

| **CI Job** | **Test Directories** | **Dependencies** | **Purpose** | **Speed** |
|------------|----------------------|------------------|-------------|-----------|
| **minimal** | `tests/unit/` | None | Core functionality only | ~7s |
| **with-numpy** | `tests/unit/` + ML features | numpy | Basic array support | ~15s |
| **with-pandas** | `tests/unit/` + data features | pandas | DataFrame support | ~25s |
| **with-ml-deps** | `tests/unit/` + `tests/integration/` | numpy, pandas, sklearn | Full ML stack | ~45s |
| **full** | `tests/unit/` + `tests/edge_cases/` + `tests/integration/` | All dependencies | Complete test suite | ~60s |
| **Performance** | `tests/benchmarks/` | All dependencies | Performance tracking | ~120s (separate) |

### **Plugin Testing Strategy**

Our CI implements **plugin-style testing** where the core package has zero required dependencies but gains functionality when optional dependencies are available:

- **Core Tests** (`tests/unit/`): Always run, no optional dependencies
- **Feature Tests** (`tests/features/`): Run only when relevant dependencies are available
- **Integration Tests** (`tests/integration/`): Test cross-component functionality
- **Coverage Tests** (`tests/coverage/`): Improve coverage metrics for edge cases

This ensures:
- âœ… **Zero dependency install works**: `pip install datason`
- âœ… **Enhanced features work**: `pip install datason[ml]`
- âœ… **Cross-platform compatibility**: Tests run on multiple Python versions
- âœ… **Fast feedback**: Core tests complete in ~7 seconds

## ğŸ” **Quality Gates**

### **Automatic Checks**

Every commit is validated by:

```yaml
âœ… Ruff linting (1000+ rules)
âœ… Ruff formatting (consistent style)
âœ… Bandit security scanning
âœ… Test suite with coverage
âœ… Type checking (mypy)
âœ… Package build integrity
```

### **Required Checks**

For PR merging, these must pass:
- [ ] **Code Quality & Security** pipeline
- [ ] **Main CI** pipeline (if code changed)
- [ ] **Documentation** pipeline (if docs changed)

### **Optional Checks**

Non-blocking but monitored:
- Coverage reports (Codecov)
- Security reports (GitHub Security tab)
- Performance benchmarks

## ğŸ“Š **Monitoring & Reports**

### **GitHub Actions Dashboard**
- âœ… **Green**: All checks passing
- âŒ **Red**: Issues found, PR blocked
- ğŸŸ¡ **Yellow**: In progress

### **Detailed Reports**

**Quality Pipeline** generates rich reports:
```yaml
## Code Quality & Security Report

### ğŸ” Ruff Linting
âœ… No issues found

### ğŸ¨ Code Formatting  
âœ… All files properly formatted

### ğŸ›¡ï¸ Security Scan Results
**Scanned**: 2,547 lines of code
**High Severity**: 0
**Medium Severity**: 0  
**Low Severity**: 1
âœ… **Security Status**: PASSED
```

### **Artifacts Available**

- ğŸ“Š **Coverage reports** (HTML, XML)
- ğŸ›¡ï¸ **Security reports** (JSON)
- ğŸ“¦ **Build artifacts** (wheels, sdist)
- ğŸ“š **Documentation** (static site)

## ğŸš€ **Developer Workflow**

### **Local Development**
```bash
# Pre-commit handles local quality
git commit â†’ pre-commit runs â†’ quality checks pass â†’ commit succeeds

# Manual quality check
ruff check --fix .
ruff format .
pytest --cov=datason
```

### **Push to GitHub**
```bash
git push origin feature-branch
```

**What happens:**
1. **Quality Pipeline** runs immediately (~30s)
2. **Main CI** runs in parallel (~2-3min)
3. **PR Status Checks** update in real-time

### **Documentation Changes**
```bash
# Edit docs/something.md
git push origin docs-update
```

**What happens:**
1. **Docs Pipeline** runs (~1-2min)
2. **Quality Pipeline** runs (for any .md files)
3. **Main CI** skipped (docs-only change)

### **Release Process**
```bash
# Create GitHub release
git tag v1.0.0
git push origin v1.0.0
# Create release in GitHub UI
```

**What happens:**
1. **Publish Pipeline** triggers automatically
2. **Package built** and verified
3. **Published to PyPI** with OIDC (secure, no tokens)

## ğŸ› ï¸ **Configuration Files**

### **Workflow Locations**
```
.github/workflows/
â”œâ”€â”€ ci.yml              # Main CI pipeline
â”œâ”€â”€ ruff.yml            # Code quality & security
â”œâ”€â”€ docs.yml            # Documentation
â””â”€â”€ publish.yml         # PyPI publishing
```

### **Key Configuration**
```yaml
# pyproject.toml - Tool configuration
[tool.ruff]           # Linting rules
[tool.pytest]         # Test configuration  
[tool.coverage]       # Coverage requirements
[tool.bandit]         # Security scanning

# .pre-commit-config.yaml - Local hooks
repos:
  - ruff              # Local quality checks
  - bandit            # Local security
```

## ğŸ”§ **Customization**

### **Adding New Checks**

**To Quality Pipeline:**
```yaml
- name: New Quality Check
  run: |
    new-tool check datason/
```

**To Main CI:**
```yaml
- name: New Test Type
  run: |
    pytest tests/test_new_feature.py
```

### **Performance Tuning**

**Cache Optimization:**
```yaml
# Add new cache for expensive tool
- name: Cache expensive-tool
  uses: actions/cache@v4
  with:
    path: ~/.cache/expensive-tool
    key: ${{ runner.os }}-expensive-${{ hashFiles('config') }}
```

**Parallel Jobs:**
```yaml
strategy:
  matrix:
    python-version: [3.8, 3.9, 3.10, 3.11, 3.12, 3.13]
```

## ğŸ¯ **Best Practices**

### **Pipeline Design**
- âœ… **Fast feedback** - Quality checks run first (30s)
- âœ… **Parallel execution** - Independent pipelines don't block each other
- âœ… **Intelligent triggers** - Only run what's needed
- âœ… **Comprehensive caching** - 2-5x speedup on repeated runs

### **Security**
- âœ… **OIDC Publishing** - No API tokens needed
- âœ… **Multi-tool scanning** - Bandit + Safety + pip-audit
- âœ… **Artifact signing** - GPG-signed commits
- âœ… **Dependency monitoring** - Dependabot + security advisories

### **Developer Experience**
- âœ… **Clear status** - GitHub status checks show exactly what failed
- âœ… **Rich reports** - Detailed summaries in GitHub UI
- âœ… **Local consistency** - pre-commit matches CI exactly
- âœ… **Fast iteration** - Quick feedback for common issues

## ğŸ“ˆ **Metrics & Performance**

### **Pipeline Performance**
| Pipeline | Cached | Uncached | Frequency |
|----------|--------|----------|-----------|
| Quality | ~30s | ~60s | Every push/PR |
| Main CI | ~2min | ~3min | Most pushes/PRs |
| Docs | ~1min | ~2min | Docs changes only |
| Publish | ~3min | ~3min | Releases only |

### **Success Rates**
- **Quality Pipeline**: 95%+ pass rate (fast feedback catches most issues)
- **Main CI**: 90%+ pass rate (comprehensive testing)
- **Docs**: 99%+ pass rate (simple build process)

### **ğŸ“Š Performance Pipeline Strategy**

**Why Performance Tests Don't Block CI:**

The performance pipeline is **informational only** and doesn't block CI for these important reasons:

1. **Environment Variability**: GitHub Actions runners have inconsistent performance
2. **Micro-benchmark Noise**: Tests measuring <1ms are highly sensitive to environment
3. **Hardware Differences**: Local vs CI environments produce different baseline measurements
4. **False Positive Prevention**: Avoid blocking legitimate code changes due to infrastructure noise

**Environment-Aware Thresholds:**

| Environment | Threshold | Reasoning |
|-------------|-----------|-----------|
| **Local Development** | 5% | Stable environment, consistent hardware |
| **CI (Same Environment)** | 25% | Account for runner variability |
| **CI (Cross Environment)** | 25%+ | Local baseline vs CI execution |

**How to Interpret Performance Results:**

âœ… **Safe to Ignore:**
- Changes <50% (likely environment noise)
- First run after environment change
- Micro-benchmarks showing high variance

âš ï¸ **Worth Investigating:**
- Consistent patterns across multiple tests
- Changes >100% without code explanation
- Memory usage regressions

ğŸ”¥ **Action Required:**
- Changes >200% with clear code correlation
- New algorithmic complexity introduced
- Memory leaks or resource issues

**Performance Baseline Management:**

```bash
# Create new CI baseline (if needed)
gh workflow run performance.yml -f save_baseline=true

# Local performance testing
cd benchmarks
python ci_performance_tracker.py

# Test with different threshold
PERFORMANCE_REGRESSION_THRESHOLD=15 python ci_performance_tracker.py
```

## ğŸ” **Troubleshooting**

### **Common Issues**

**Quality Pipeline Fails:**
```bash
# Local fix
ruff check --fix .
ruff format .
git commit --amend
```

**Main CI Test Failures:**
```bash
# Local debugging
pytest tests/test_failing.py -v
pytest --cov=datason --cov-report=html
# Open htmlcov/index.html
```

**Cache Issues:**
```bash
# Clear cache via GitHub Actions UI
# Or update cache key in workflow
```

### **Performance Issues**

**Slow Pipeline:**
- Check cache hit rates in Actions logs
- Verify cache keys are stable
- Consider splitting large jobs

**Resource Limits:**
- Use `timeout-minutes` for runaway processes
- Monitor memory usage in logs
- Consider matrix builds for heavy testing

---

**This architecture provides:**
- ğŸš€ **Fast feedback** (30s for quality issues)
- ğŸ”„ **Parallel execution** (quality + tests simultaneously)  
- ğŸ’¾ **Intelligent caching** (2-5x speedup)
- ğŸ¯ **Smart triggering** (only run what's needed)
- ğŸ“Š **Rich reporting** (detailed GitHub summaries)
- ğŸ”’ **Strong security** (multi-tool scanning + OIDC)

The result is a **production-ready CI/CD pipeline** that scales with your team while maintaining developer velocity! ğŸ‰
