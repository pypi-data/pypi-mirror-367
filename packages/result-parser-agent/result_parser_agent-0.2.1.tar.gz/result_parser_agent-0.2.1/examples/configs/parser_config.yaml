# Simplified configuration for Results Parser Agent
# Focus on core requirements: input, metrics, output

agent:
  # LLM configuration
  llm:
    provider: "groq"                    # LLM provider (groq, openai, anthropic, ollama, google)
    model: "llama3.1-8b-instant"        # Fast and efficient for parsing tasks
    api_key: null                       # Set to null to use GROQ_API_KEY environment variable
    temperature: 0.1                    # Temperature for LLM responses
    max_tokens: 4000                    # Maximum tokens for responses
  
  # Agent behavior
  max_retries: 3
  chunk_size: 2000
  timeout: 300

parsing:
  # List of metrics to extract from result files
  # The agent will intelligently discover patterns for these metrics
  metrics:
    - "RPS"
    - "latency"
    - "throughput"
    - "accuracy"
    - "precision"
    - "recall"
    - "f1_score"
  
  # Whether pattern matching is case sensitive
  case_sensitive: false
  
  # Whether to use fuzzy matching for pattern recognition
  fuzzy_match: true
  
  # Minimum confidence threshold for metric extraction (0.0 to 1.0)
  min_confidence: 0.7

output:
  # Output format: json, csv, yaml
  format: "json"
  
  # Whether to pretty print the output
  pretty_print: true
  
  # Whether to include metadata in the output
  include_metadata: true

logging:
  # Logging level: DEBUG, INFO, WARNING, ERROR
  level: "INFO"
  
  # Log format string
  format: "{time} | {level} | {message}"
  
  # Log file path (null for console only)
  file: null 